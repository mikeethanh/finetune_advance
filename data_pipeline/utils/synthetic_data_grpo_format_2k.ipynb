{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "db196208",
      "metadata": {
        "id": "db196208"
      },
      "source": [
        "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "485c1e63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "485c1e63",
        "outputId": "737488ce-5581-4d7d-bcad-fc82033e891b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index llama-index-llms-openai llama-index-embeddings-openai llama-index-vector-stores-faiss\n",
        "!pip install -q faiss-cpu beautifulsoup4 requests tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7204d2",
      "metadata": {
        "id": "dc7204d2"
      },
      "source": [
        "## 2. Import libraries v√† setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "95b2ffc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95b2ffc7",
        "outputId": "ba99f3bd-b39d-4ff5-9572-d241932c28d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setup completed!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "# LlamaIndex\n",
        "from llama_index.core import Document, VectorStoreIndex, Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "import faiss\n",
        "\n",
        "# Setup OpenAI API key t·ª´ Colab Secrets\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "print(\"‚úÖ Setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2803dbab",
      "metadata": {
        "id": "2803dbab"
      },
      "source": [
        "## 3. C·∫•u h√¨nh LlamaIndex v√† GRPO Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f9878703",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9878703",
        "outputId": "76f5de67-ee4a-4374-c535-1a695cdb9bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LlamaIndex configured with GPT-4o-mini\n",
            "‚úÖ GRPO format markers configured:\n",
            "   Reasoning: <start_working_out> ... <end_working_out>\n",
            "   Solution: <SOLUTION> ... </SOLUTION>\n"
          ]
        }
      ],
      "source": [
        "# C·∫•u h√¨nh LLM v√† Embedding\n",
        "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Set global settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 50\n",
        "\n",
        "# Define GRPO reasoning format markers\n",
        "reasoning_start = \"<start_working_out>\"\n",
        "reasoning_end = \"<end_working_out>\"\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "print(\"‚úÖ LlamaIndex configured with GPT-4o-mini\")\n",
        "print(f\"‚úÖ GRPO format markers configured:\")\n",
        "print(f\"   Reasoning: {reasoning_start} ... {reasoning_end}\")\n",
        "print(f\"   Solution: {solution_start} ... {solution_end}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4fe0769",
      "metadata": {
        "id": "b4fe0769"
      },
      "source": [
        "## 4. Crawl vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ VBPL (T√°i s·ª≠ d·ª•ng function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2748bd77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2748bd77",
        "outputId": "82ea0637-d9e4-4044-9d2b-1a34df5f4e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Crawling document from: https://www.moj.gov.vn/vbpq/lists/vn%20bn%20php%20lut/view_detail.aspx?itemid=23311\n",
            "\n",
            "‚úÖ Crawled 62,877 characters\n",
            "üìÑ Number of lines: 602\n",
            "\n",
            "Preview (first 1000 chars):\n",
            "Turn on more accessible mode\n",
            "Turn off more accessible mode\n",
            "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ B·ªô T∆∞ ph√°p - Ministry of Justice‚Äôs portal\n",
            "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠\n",
            "CSDLQG v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t\n",
            "Ch∆∞∆°ng VIII\n",
            "C·ª•c C√¥ng ngh·ªá th√¥ng tin, B·ªô T∆∞ ph√°p tr√¢n tr·ªçng c·∫£m ∆°n Qu√Ω ƒë·ªôc gi·∫£ trong th·ªùi gian qua ƒë√£ s·ª≠ d·ª•ng h·ªá th·ªëng vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t t·∫°i ƒë·ªãa ch·ªâ\n",
            "http://www.moj.gov.vn/pages/vbpq.aspx\n",
            "ƒê·∫øn nay, nh·∫±m ph·ª•c v·ª• t·ªët h∆°n nhu c·∫ßu khai th√°c, tra c·ª©u vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t t·ª´ Trung ∆∞∆°ng ƒë·∫øn ƒë·ªãa ph∆∞∆°ng, C·ª•c C√¥ng ngh·ªá th√¥ng tin ƒë√£ ƒë∆∞a C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t v√†o s·ª≠ d·ª•ng t·∫°i ƒë·ªãa ch·ªâ\n",
            "http://vbpl.vn/Pages/portal.aspx\n",
            "ƒë·ªÉ thay th·∫ø cho h·ªá th·ªëng c≈© n√≥i tr√™n.\n",
            "C·ª•c C√¥ng ngh·ªá th√¥ng tin tr√¢n tr·ªçng th√¥ng b√°o t·ªõi Qu√Ω ƒë·ªôc gi·∫£ ƒë∆∞·ª£c bi·∫øt v√† mong r·∫±ng C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t s·∫Ω ti·∫øp t·ª•c l√† ƒë·ªãa ch·ªâ tin c·∫≠y ƒë·ªÉ khai th√°c, tra c·ª©u vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t.\n",
            "Trong qu√° tr√¨nh s·ª≠ d·ª•ng, ch√∫ng t√¥i lu√¥n hoan ngh√™nh m·ªçi √Ω ki·∫øn g√≥p √Ω c·ªßa Qu√Ω ƒë·ªôc gi·∫£ ƒë·ªÉ C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c ...\n"
          ]
        }
      ],
      "source": [
        "def crawl_legal_document(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Crawl n·ªôi dung vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ VBPL.vn\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # T√¨m n·ªôi dung ch√≠nh - VBPL th∆∞·ªùng d√πng c√°c selectors n√†y\n",
        "        content_text = \"\"\n",
        "\n",
        "        # C√°ch 1: T√¨m div c√≥ id ho·∫∑c class ch·ª©a \"content\", \"fulltext\", \"noidung\"\n",
        "        selectors = [\n",
        "            {'id': 'divContent'},\n",
        "            {'id': 'ctl00_Content'},\n",
        "            {'class': 'content1'},\n",
        "            {'class': 'fulltext'},\n",
        "            {'class': 'noidung'},\n",
        "            {'id': 'content'},\n",
        "        ]\n",
        "\n",
        "        for selector in selectors:\n",
        "            content_div = soup.find('div', selector)\n",
        "            if content_div:\n",
        "                content_text = content_div.get_text(separator='\\n', strip=True)\n",
        "                if len(content_text) > 500:  # N·ªôi dung ƒë·ªß d√†i\n",
        "                    break\n",
        "\n",
        "        # C√°ch 2: N·∫øu kh√¥ng t√¨m th·∫•y, t√¨m trong body v√† lo·∫°i b·ªè script, style\n",
        "        if len(content_text) < 500:\n",
        "            # X√≥a c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt\n",
        "            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n",
        "                tag.decompose()\n",
        "\n",
        "            # L·∫•y text t·ª´ body\n",
        "            body = soup.find('body')\n",
        "            if body:\n",
        "                content_text = body.get_text(separator='\\n', strip=True)\n",
        "\n",
        "        # L√†m s·∫°ch text\n",
        "        content_text = re.sub(r'\\n\\s*\\n', '\\n\\n', content_text)\n",
        "        lines = content_text.split('\\n')\n",
        "        cleaned_lines = [line for line in lines if len(line.strip()) > 10]\n",
        "        content_text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "        return content_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error crawling {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Crawl vƒÉn b·∫£n (s·ª≠ d·ª•ng c√πng URL)\n",
        "document_url = \"https://www.moj.gov.vn/vbpq/lists/vn%20bn%20php%20lut/view_detail.aspx?itemid=23311\"\n",
        "print(f\"üì• Crawling document from: {document_url}\")\n",
        "legal_text = crawl_legal_document(document_url)\n",
        "\n",
        "print(f\"\\n‚úÖ Crawled {len(legal_text):,} characters\")\n",
        "print(f\"üìÑ Number of lines: {len(legal_text.splitlines())}\")\n",
        "print(f\"\\nPreview (first 1000 chars):\\n{legal_text[:1000]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9732c07a",
      "metadata": {
        "id": "9732c07a"
      },
      "source": [
        "## 5. Chunking v√† t·∫°o Documents (C√πng c√°ch v·ªõi file g·ªëc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "297eaf31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "297eaf31",
        "outputId": "b4294f53-8bea-4d4c-c938-79afdf51cb3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created 69 chunks\n",
            "\n",
            "Sample chunk:\n",
            "Turn on more accessible mode\n",
            "Turn off more accessible mode\n",
            "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ B·ªô T∆∞ ph√°p - Ministry of Justice‚Äôs portal\n",
            "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠\n",
            "CSDLQG v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t\n",
            "Ch∆∞∆°ng VIII\n",
            "C·ª•c C√¥ng ngh·ªá th√¥ng tin, B·ªô T∆∞ ph√°p tr√¢n tr·ªçng c·∫£m ∆°n Qu√Ω ƒë·ªôc gi·∫£ trong th·ªùi gian qua ƒë√£ s·ª≠ d·ª•ng h·ªá th·ªëng vƒÉn b·∫£n...\n"
          ]
        }
      ],
      "source": [
        "# T·∫°o Document t·ª´ text crawled\n",
        "document = Document(text=legal_text)\n",
        "\n",
        "# Chunking v·ªõi SentenceSplitter (c√πng settings)\n",
        "splitter = SentenceSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=50,\n",
        "    separator=\"\\n\"\n",
        ")\n",
        "\n",
        "nodes = splitter.get_nodes_from_documents([document])\n",
        "\n",
        "print(f\"‚úÖ Created {len(nodes)} chunks\")\n",
        "print(f\"\\nSample chunk:\\n{nodes[0].text[:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a303d866",
      "metadata": {
        "id": "a303d866"
      },
      "source": [
        "## 6. T·∫°o FAISS Vector Store v√† merge similar chunks (C√πng logic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ba63cf18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "01953596ae9a445cb09a724a87851df0",
            "f7463396c9004303bc8044d9b51e2fab",
            "aaf11151f9d0489fabc09e97a6083f82",
            "09498438f4fa4584b7b9369476efb77b",
            "3b59eef6af4c4f81bd6121f2bf7974e6",
            "b2f4c62404534b018c899b0308530a1d",
            "2573209d56fc465099bc3e5f6740c746",
            "7c42a3ebbbea462abb698ede5fccfd21",
            "0155bfbbecfc411babab8573800dc593",
            "007912efae324b12ac2bea5532d1826d",
            "74e1c1de6d474e5f823136a0f0b62cea"
          ]
        },
        "id": "ba63cf18",
        "outputId": "ef1c1f1e-6702-499c-94a9-3f2c0c05039e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Creating vector index...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/69 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01953596ae9a445cb09a724a87851df0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Vector index created with 69 nodes\n"
          ]
        }
      ],
      "source": [
        "# T·∫°o FAISS index\n",
        "dimension = 1536  # text-embedding-3-small dimension\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# T·∫°o vector store\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "\n",
        "# T·∫°o index t·ª´ nodes\n",
        "print(\"üîÑ Creating vector index...\")\n",
        "index = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    vector_store=vector_store,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Vector index created with {len(nodes)} nodes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2c5be45c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5be45c",
        "outputId": "1afe794f-1f76-4228-a5e8-da91b334f83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Merging similar chunks (threshold=0.75)...\n",
            "üìä Input: 69 chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Getting embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 69/69 [00:21<00:00,  3.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Merged 69 chunks into 43 contexts\n",
            "üìà Avg chunks per context: 1.6\n",
            "\n",
            "üìä Number of merged contexts: 43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def merge_similar_chunks(nodes: List, similarity_threshold: float = 0.75) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Merge c√°c chunks c√≥ cosine similarity cao ƒë·ªÉ t·∫°o contexts phong ph√∫ h∆°n\n",
        "    \"\"\"\n",
        "    print(f\"üîÑ Merging similar chunks (threshold={similarity_threshold})...\")\n",
        "    print(f\"üìä Input: {len(nodes)} chunks\")\n",
        "\n",
        "    # Get embeddings cho t·∫•t c·∫£ nodes\n",
        "    embeddings = []\n",
        "    for node in tqdm(nodes, desc=\"Getting embeddings\"):\n",
        "        emb = embed_model.get_text_embedding(node.text)\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    # T√≠nh cosine similarity matrix\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    # Merge chunks\n",
        "    merged_contexts = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(nodes)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "\n",
        "        # T√¨m c√°c chunks similar\n",
        "        similar_indices = np.where(similarity_matrix[i] > similarity_threshold)[0]\n",
        "        similar_indices = [idx for idx in similar_indices if idx not in used_indices]\n",
        "\n",
        "        # Merge text\n",
        "        merged_text = \"\\n\\n\".join([nodes[idx].text for idx in similar_indices])\n",
        "\n",
        "        merged_contexts.append({\n",
        "            'context': merged_text,\n",
        "            'num_chunks': len(similar_indices),\n",
        "            'chunk_indices': similar_indices\n",
        "        })\n",
        "\n",
        "        used_indices.update(similar_indices)\n",
        "\n",
        "    print(f\"‚úÖ Merged {len(nodes)} chunks into {len(merged_contexts)} contexts\")\n",
        "    print(f\"üìà Avg chunks per context: {len(nodes)/len(merged_contexts):.1f}\")\n",
        "    return merged_contexts\n",
        "\n",
        "# Merge chunks v·ªõi c√πng threshold\n",
        "merged_contexts = merge_similar_chunks(nodes, similarity_threshold=0.75)\n",
        "print(f\"\\nüìä Number of merged contexts: {len(merged_contexts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c3aaf0",
      "metadata": {
        "id": "e2c3aaf0"
      },
      "source": [
        "## 7. Generate Questions t·ª´ Contexts (C√πng logic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2283a78d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2283a78d",
        "outputId": "d6a9cdb9-de3e-4c92-9153-08307dbbc0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Sample generated questions:\n",
            "1. C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t l√† g√¨?\n",
            "2. T√¥i c√≥ th·ªÉ t√¨m th·∫•y c√°c vƒÉn b·∫£n ph√°p lu·∫≠t n√†o tr√™n c·ªïng th√¥ng tin ƒëi·ªán t·ª≠ c·ªßa B·ªô T∆∞ ph√°p?\n",
            "3. C√≥ nh·ªØng ƒëi·ªÅu ki·ªán n√†o ƒë·ªÉ s·ª≠ d·ª•ng C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t?\n",
            "4. Th·ªß t·ª•c ƒë·ªÉ tra c·ª©u vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t tr√™n c·ªïng th√¥ng tin n√†y nh∆∞ th·∫ø n√†o?\n",
            "5. Quy·ªÅn l·ª£i c·ªßa t√¥i khi s·ª≠ d·ª•ng C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t l√† g√¨?\n",
            "6. T√¥i c√≥ th·ªÉ g·ª≠i √Ω ki·∫øn g√≥p √Ω v·ªÅ C∆° s·ªü d·ªØ li·ªáu n√†y ·ªü ƒë√¢u?\n"
          ]
        }
      ],
      "source": [
        "def generate_questions_from_context(context: str, num_questions: int = 6) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate c√¢u h·ªèi t·ª´ context s·ª≠ d·ª•ng GPT-4o-mini\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"B·∫°n l√† chuy√™n gia ph√°p lu·∫≠t Vi·ªát Nam. D·ª±a tr√™n ƒëo·∫°n vƒÉn b·∫£n ph√°p lu·∫≠t d∆∞·ªõi ƒë√¢y, h√£y t·∫°o {num_questions} c√¢u h·ªèi hay v√† ƒëa d·∫°ng m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ h·ªèi.\n",
        "\n",
        "VƒÉn b·∫£n ph√°p lu·∫≠t:\n",
        "{context}\n",
        "\n",
        "Y√™u c·∫ßu:\n",
        "- C√¢u h·ªèi ph·∫£i t·ª± nhi√™n, gi·ªëng nh∆∞ ng∆∞·ªùi d√πng th·∫≠t s·∫Ω h·ªèi\n",
        "- ƒêa d·∫°ng v·ªÅ lo·∫°i c√¢u h·ªèi: h·ªèi ƒë·ªãnh nghƒ©a, ƒëi·ªÅu ki·ªán, th·ªß t·ª•c, quy·ªÅn l·ª£i, tr√°ch nhi·ªám, v.v.\n",
        "- C√¢u h·ªèi ph·∫£i c√≥ th·ªÉ tr·∫£ l·ªùi ƒë∆∞·ª£c t·ª´ context tr√™n\n",
        "- Tr·∫£ v·ªÅ ONLY danh s√°ch c√°c c√¢u h·ªèi, m·ªói c√¢u m·ªôt d√≤ng, kh√¥ng ƒë√°nh s·ªë\n",
        "\n",
        "C√¢u h·ªèi:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.complete(prompt)\n",
        "        questions_text = response.text.strip()\n",
        "\n",
        "        # Parse questions\n",
        "        questions = [q.strip() for q in questions_text.split('\\n') if q.strip()]\n",
        "        # Lo·∫°i b·ªè numbering n·∫øu c√≥\n",
        "        questions = [re.sub(r'^\\d+\\.\\s*', '', q) for q in questions]\n",
        "\n",
        "        return questions[:num_questions]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating questions: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test v·ªõi 1 context\n",
        "test_context = merged_contexts[0]['context']\n",
        "test_questions = generate_questions_from_context(test_context, num_questions=6)\n",
        "\n",
        "print(\"üìù Sample generated questions:\")\n",
        "for i, q in enumerate(test_questions, 1):\n",
        "    print(f\"{i}. {q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad19c88b",
      "metadata": {
        "id": "ad19c88b"
      },
      "source": [
        "## 8. Generate Structured Answers cho GRPO Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "899d7489",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "899d7489",
        "outputId": "10a58abd-5b1f-4a79-be06-6989e5161aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üí¨ Sample Structured Q&A:\n",
            "Q: C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t l√† g√¨?\n",
            "A: <start_working_out>\n",
            "C√¢u h·ªèi y√™u c·∫ßu ng∆∞·ªùi d√πng t√¨m hi·ªÉu v·ªÅ \"C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t\". ƒêi·ªÅu n√†y cho th·∫•y h·ªç mu·ªën bi·∫øt ƒë·ªãnh nghƒ©a, ch·ª©c nƒÉng v√† m·ª•c ƒë√≠ch c·ªßa h·ªá th·ªëng n√†y. Trong vƒÉn b·∫£n ƒë√£ cung c·∫•p, c√≥ nh·∫Øc ƒë·∫øn \"C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t\" nh∆∞ m·ªôt c√¥ng c·ª• ƒë·ªÉ khai th√°c, tra c·ª©u vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t t·ª´ Trung ∆∞∆°ng ƒë·∫øn ƒë·ªãa ph∆∞∆°ng. B√™n c·∫°nh ƒë√≥, vƒÉn b·∫£n c≈©ng th√¥ng b√°o r·∫±ng h·ªá th·ªëng n√†y ƒë√£ ƒë∆∞·ª£c ƒë∆∞a v√†o s·ª≠ d·ª•ng ƒë·ªÉ thay th·∫ø cho h·ªá th·ªëng c≈©, v·ªõi m·ª•c ti√™u ph·ª•c v·ª• t·ªët h∆°n nhu c·∫ßu c·ªßa ng∆∞·ªùi d√πng. Do ƒë√≥, c√≥ th·ªÉ suy ra r·∫±ng c∆° s·ªü d·ªØ li·ªáu n√†y nh·∫±m cung c·∫•p th√¥ng tin ph√°p l√Ω m·ªôt c√°ch d·ªÖ d√†ng v√† thu·∫≠n ti·ªán cho ng∆∞·ªùi s·ª≠ d·ª•ng.\n",
            "<end_working_out>\n",
            "\n",
            "<SOLUTION>C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t l√† h·ªá th·ªëng th√¥ng tin ƒëi·ªán t·ª≠ nh·∫±m ph·ª•c v·ª• vi·ªác khai th√°c, tra c·ª©u c√°c vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t t·ª´ Trung ∆∞∆°ng ƒë·∫øn ƒë·ªãa ph∆∞∆°ng. H·ªá th·ªëng n√†y gi√∫p ng∆∞·ªùi d√¢n v√† c√°c c∆° quan d·ªÖ d√†ng ti·∫øp c·∫≠n th√¥ng tin ph√°p l√Ω, g√≥p ph·∫ßn n√¢ng cao hi·ªáu qu·∫£ qu·∫£n l√Ω nh√† n∆∞·ªõc v·ªÅ ph√°p lu·∫≠t.</SOLUTION>\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def generate_structured_answer(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate c√¢u tr·∫£ l·ªùi c√≥ c·∫•u tr√∫c GRPO format v·ªõi reasoning v√† solution\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. Khi tr·∫£ l·ªùi c√¢u h·ªèi, h√£y:\n",
        "1. Suy nghƒ© v√† ph√¢n t√≠ch c√¢u h·ªèi trong ph·∫ßn {reasoning_start} {reasoning_end}\n",
        "2. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c trong ph·∫ßn {solution_start}{solution_end}\n",
        "\n",
        "VƒÉn b·∫£n ph√°p lu·∫≠t:\n",
        "{context}\n",
        "\n",
        "C√¢u h·ªèi: {question}\n",
        "\n",
        "Y√™u c·∫ßu cho ph·∫ßn suy nghƒ© (working_out):\n",
        "- Ph√¢n t√≠ch c√¢u h·ªèi: ng∆∞·ªùi d√πng mu·ªën bi·∫øt g√¨?\n",
        "- T√¨m ki·∫øm th√¥ng tin li√™n quan trong vƒÉn b·∫£n ph√°p lu·∫≠t\n",
        "- X√°c ƒë·ªãnh ƒëi·ªÅu lu·∫≠t, kho·∫£n, ƒëi·ªÉm √°p d·ª•ng\n",
        "- Gi·∫£i th√≠ch logic ph√°p l√Ω\n",
        "\n",
        "Y√™u c·∫ßu cho ph·∫ßn solution:\n",
        "- Tr·∫£ l·ªùi tr·ª±c ti·∫øp, r√µ r√†ng\n",
        "- Tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t c·ª• th·ªÉ\n",
        "- ƒê·ªô d√†i: 30-150 t·ª´\n",
        "- D·ªÖ hi·ªÉu v·ªõi ng∆∞·ªùi d√¢n\n",
        "\n",
        "Format b·∫Øt bu·ªôc:\n",
        "{reasoning_start}\n",
        "[Ph·∫ßn ph√¢n t√≠ch v√† suy nghƒ©]\n",
        "{reasoning_end}\n",
        "\n",
        "{solution_start}[C√¢u tr·∫£ l·ªùi ch√≠nh x√°c]{solution_end}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.complete(prompt)\n",
        "        answer = response.text.strip()\n",
        "\n",
        "        # Validate format\n",
        "        if reasoning_start in answer and reasoning_end in answer and solution_start in answer and solution_end in answer:\n",
        "            return answer\n",
        "        else:\n",
        "            # Fallback: create structure if not properly formatted\n",
        "            simple_answer = response.text.strip()\n",
        "            structured = f\"\"\"{reasoning_start}\n",
        "ƒê·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y, t√¥i c·∫ßn ph√¢n t√≠ch vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p v√† t√¨m th√¥ng tin li√™n quan ƒë·∫øn: {question}\n",
        "\n",
        "Sau khi xem x√©t c√°c quy ƒë·ªãnh, t√¥i s·∫Ω ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n ph√°p lu·∫≠t hi·ªán h√†nh.\n",
        "{reasoning_end}\n",
        "\n",
        "{solution_start}{simple_answer}{solution_end}\"\"\"\n",
        "            return structured\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating structured answer: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Test v·ªõi 1 question\n",
        "test_structured_answer = generate_structured_answer(test_questions[0], test_context)\n",
        "\n",
        "print(\"üí¨ Sample Structured Q&A:\")\n",
        "print(f\"Q: {test_questions[0]}\")\n",
        "print(f\"A: {test_structured_answer}\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4bac87c",
      "metadata": {
        "id": "f4bac87c"
      },
      "source": [
        "## 9. Generate to√†n b·ªô GRPO dataset (Target: 800+ samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f3efcc3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3efcc3c",
        "outputId": "d9a1394f-2c4a-4894-d402-a16f06e15b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Generating 2150 GRPO Q&A pairs from 43 contexts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing contexts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [3:34:12<00:00, 298.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Generated 2121 GRPO Q&A pairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_grpo_synthetic_dataset(\n",
        "    contexts: List[Dict],\n",
        "    target_samples: int = 2150,\n",
        "    questions_per_context: int = 50\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate to√†n b·ªô synthetic dataset v·ªõi GRPO structured format\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "\n",
        "    # T√≠nh s·ªë contexts c·∫ßn d√πng\n",
        "    num_contexts_needed = min(len(contexts), (target_samples // questions_per_context) + 1)\n",
        "\n",
        "    print(f\"üöÄ Generating {target_samples} GRPO Q&A pairs from {num_contexts_needed} contexts...\")\n",
        "\n",
        "    for ctx_data in tqdm(contexts[:num_contexts_needed], desc=\"Processing contexts\"):\n",
        "        context = ctx_data['context']\n",
        "\n",
        "        # Skip contexts qu√° ng·∫Øn\n",
        "        if len(context.split()) < 50:\n",
        "            continue\n",
        "\n",
        "        # Generate questions\n",
        "        questions = generate_questions_from_context(context, num_questions=questions_per_context)\n",
        "\n",
        "        # Generate structured answers cho m·ªói question\n",
        "        for question in questions:\n",
        "            if len(dataset) >= target_samples:\n",
        "                break\n",
        "\n",
        "            structured_answer = generate_structured_answer(question, context)\n",
        "\n",
        "            if structured_answer and reasoning_start in structured_answer:  # Validate structure\n",
        "                dataset.append({\n",
        "                    'question': question,\n",
        "                    'answer': structured_answer,\n",
        "                    'format': 'grpo_structured'  # Metadata ƒë·ªÉ ph√¢n bi·ªát\n",
        "                })\n",
        "\n",
        "        if len(dataset) >= target_samples:\n",
        "            break\n",
        "\n",
        "    print(f\"\\n‚úÖ Generated {len(dataset)} GRPO Q&A pairs\")\n",
        "    return dataset\n",
        "\n",
        "# Generate dataset\n",
        "grpo_synthetic_data = generate_grpo_synthetic_dataset(\n",
        "    merged_contexts,\n",
        "    target_samples=2150,\n",
        "    questions_per_context=50\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc922c4",
      "metadata": {
        "id": "9bc922c4"
      },
      "source": [
        "## 10. L∆∞u GRPO dataset d∆∞·ªõi d·∫°ng JSONL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "47835c73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "47835c73",
        "outputId": "9eed8117-1f3a-445a-a83c-5fbf0fd6416c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 2121 GRPO Q&A pairs to synthetic_legal_qa_grpo_format.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1800080e-d7cf-4561-ae81-334e5bf239a8\", \"synthetic_legal_qa_grpo_format.jsonl\", 3119571)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# L∆∞u ra file JSONL v·ªõi t√™n ph√¢n bi·ªát\n",
        "output_path = 'synthetic_legal_qa_grpo_format.jsonl'\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for item in grpo_synthetic_data:\n",
        "        json.dump(item, f, ensure_ascii=False)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(f\"‚úÖ Saved {len(grpo_synthetic_data)} GRPO Q&A pairs to {output_path}\")\n",
        "\n",
        "# Download file (n·∫øu ch·∫°y tr√™n Colab)\n",
        "from google.colab import files\n",
        "files.download(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a573fd50",
      "metadata": {
        "id": "a573fd50"
      },
      "source": [
        "## 11. Th·ªëng k√™ & Preview GRPO dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1792e3ca",
      "metadata": {
        "id": "1792e3ca"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# T·∫°o DataFrame ƒë·ªÉ ph√¢n t√≠ch\n",
        "df = pd.DataFrame(grpo_synthetic_data)\n",
        "\n",
        "# Th·ªëng k√™ ƒë·ªô d√†i\n",
        "df['question_len'] = df['question'].apply(lambda x: len(x.split()))\n",
        "df['answer_len'] = df['answer'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Th·ªëng k√™ c·∫•u tr√∫c GRPO\n",
        "df['has_reasoning'] = df['answer'].apply(lambda x: reasoning_start in x and reasoning_end in x)\n",
        "df['has_solution'] = df['answer'].apply(lambda x: solution_start in x and solution_end in x)\n",
        "df['proper_format'] = df['has_reasoning'] & df['has_solution']\n",
        "\n",
        "print(\"üìä GRPO DATASET STATISTICS:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total samples: {len(df):,}\")\n",
        "print(f\"Properly formatted: {df['proper_format'].sum():,} ({df['proper_format'].mean()*100:.1f}%)\")\n",
        "print(f\"\\nQuestion length (words):\")\n",
        "print(df['question_len'].describe())\n",
        "print(f\"\\nAnswer length (words):\")\n",
        "print(df['answer_len'].describe())\n",
        "\n",
        "# Preview samples\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAMPLE GRPO Q&A PAIRS:\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(min(3, len(df))):\n",
        "    print(f\"\\n[Sample {i+1}]\")\n",
        "    print(f\"Q: {df.iloc[i]['question']}\")\n",
        "    print(f\"A: {df.iloc[i]['answer'][:400]}...\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d2295f",
      "metadata": {
        "id": "c2d2295f"
      },
      "source": [
        "## 12. Validation & Quality Check cho GRPO Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e52667be",
      "metadata": {
        "id": "e52667be"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra ch·∫•t l∆∞·ª£ng format GRPO\n",
        "print(\"üîç GRPO FORMAT QUALITY CHECKS:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check duplicates\n",
        "duplicate_questions = df[df.duplicated(subset=['question'], keep=False)]\n",
        "print(f\"Duplicate questions: {len(duplicate_questions)}\")\n",
        "\n",
        "# Check format structure\n",
        "missing_reasoning = df[~df['has_reasoning']]\n",
        "print(f\"Missing reasoning format: {len(missing_reasoning)}\")\n",
        "\n",
        "missing_solution = df[~df['has_solution']]\n",
        "print(f\"Missing solution format: {len(missing_solution)}\")\n",
        "\n",
        "# Check empty sections\n",
        "def extract_reasoning_section(answer):\n",
        "    \"\"\"Extract text between reasoning markers\"\"\"\n",
        "    try:\n",
        "        start_idx = answer.find(reasoning_start) + len(reasoning_start)\n",
        "        end_idx = answer.find(reasoning_end)\n",
        "        return answer[start_idx:end_idx].strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def extract_solution_section(answer):\n",
        "    \"\"\"Extract text between solution markers\"\"\"\n",
        "    try:\n",
        "        start_idx = answer.find(solution_start) + len(solution_start)\n",
        "        end_idx = answer.find(solution_end)\n",
        "        return answer[start_idx:end_idx].strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# Check reasoning section quality\n",
        "reasoning_lengths = []\n",
        "solution_lengths = []\n",
        "\n",
        "for answer in df['answer']:\n",
        "    reasoning = extract_reasoning_section(answer)\n",
        "    solution = extract_solution_section(answer)\n",
        "    reasoning_lengths.append(len(reasoning.split()))\n",
        "    solution_lengths.append(len(solution.split()))\n",
        "\n",
        "df['reasoning_len'] = reasoning_lengths\n",
        "df['solution_len'] = solution_lengths\n",
        "\n",
        "print(f\"\\nReasoning section length (words):\")\n",
        "print(f\"  Mean: {np.mean(reasoning_lengths):.1f}\")\n",
        "print(f\"  Min: {min(reasoning_lengths)}, Max: {max(reasoning_lengths)}\")\n",
        "\n",
        "print(f\"\\nSolution section length (words):\")\n",
        "print(f\"  Mean: {np.mean(solution_lengths):.1f}\")\n",
        "print(f\"  Min: {min(solution_lengths)}, Max: {max(solution_lengths)}\")\n",
        "\n",
        "# Check for very short sections\n",
        "short_reasoning = sum(1 for x in reasoning_lengths if x < 10)\n",
        "short_solution = sum(1 for x in solution_lengths if x < 5)\n",
        "\n",
        "print(f\"\\nVery short reasoning (< 10 words): {short_reasoning}\")\n",
        "print(f\"Very short solution (< 5 words): {short_solution}\")\n",
        "\n",
        "print(\"\\n‚úÖ GRPO format quality check completed!\")\n",
        "\n",
        "# Sample format validation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAMPLE FORMAT VALIDATION:\")\n",
        "sample_answer = df.iloc[0]['answer']\n",
        "reasoning_text = extract_reasoning_section(sample_answer)\n",
        "solution_text = extract_solution_section(sample_answer)\n",
        "\n",
        "print(f\"\\nExtracted Reasoning:\\n{reasoning_text}\")\n",
        "print(f\"\\nExtracted Solution:\\n{solution_text}\")\n",
        "print(f\"\\nFormat valid: {bool(reasoning_text and solution_text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3822f388",
      "metadata": {
        "id": "3822f388"
      },
      "source": [
        "## 13. Export cho SFT Training format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b01faf1",
      "metadata": {
        "id": "7b01faf1"
      },
      "outputs": [],
      "source": [
        "# T·∫°o format training cho SFT v·ªõi system prompt\n",
        "system_prompt = f\"\"\"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. Khi tr·∫£ l·ªùi c√¢u h·ªèi, h√£y:\n",
        "1. Suy nghƒ© v√† ph√¢n t√≠ch c√¢u h·ªèi trong ph·∫ßn {reasoning_start} {reasoning_end}\n",
        "2. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c trong ph·∫ßn {solution_start}{solution_end}\n",
        "\n",
        "C√¢u tr·∫£ l·ªùi c·∫ßn d·ª±a tr√™n quy ƒë·ªãnh ph√°p lu·∫≠t hi·ªán h√†nh v√† ph·∫£i r√µ r√†ng, d·ªÖ hi·ªÉu.\"\"\"\n",
        "\n",
        "# Convert sang format training\n",
        "training_data = []\n",
        "for item in grpo_synthetic_data:\n",
        "    if item['answer'] and reasoning_start in item['answer']:  # Only properly formatted ones\n",
        "        training_item = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": item['question']},\n",
        "                {\"role\": \"assistant\", \"content\": item['answer']}\n",
        "            ]\n",
        "        }\n",
        "        training_data.append(training_item)\n",
        "\n",
        "# L∆∞u training format\n",
        "training_output_path = 'synthetic_legal_qa_grpo_training.jsonl'\n",
        "\n",
        "with open(training_output_path, 'w', encoding='utf-8') as f:\n",
        "    for item in training_data:\n",
        "        json.dump(item, f, ensure_ascii=False)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(f\"‚úÖ Saved {len(training_data)} training examples to {training_output_path}\")\n",
        "print(f\"üìà Ready for SFT training with GRPO format!\")\n",
        "\n",
        "# Download training file\n",
        "files.download(training_output_path)\n",
        "\n",
        "print(\"\\nüéØ SUMMARY:\")\n",
        "print(f\"- Generated {len(grpo_synthetic_data)} synthetic samples\")\n",
        "print(f\"- {len(training_data)} properly formatted for GRPO training\")\n",
        "print(f\"- Format includes {reasoning_start}...{reasoning_end} and {solution_start}...{solution_end}\")\n",
        "print(f\"- Ready for SFT training to align with your GRPO model output format\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01953596ae9a445cb09a724a87851df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7463396c9004303bc8044d9b51e2fab",
              "IPY_MODEL_aaf11151f9d0489fabc09e97a6083f82",
              "IPY_MODEL_09498438f4fa4584b7b9369476efb77b"
            ],
            "layout": "IPY_MODEL_3b59eef6af4c4f81bd6121f2bf7974e6"
          }
        },
        "f7463396c9004303bc8044d9b51e2fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f4c62404534b018c899b0308530a1d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2573209d56fc465099bc3e5f6740c746",
            "value": "Generating‚Äáembeddings:‚Äá100%"
          }
        },
        "aaf11151f9d0489fabc09e97a6083f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c42a3ebbbea462abb698ede5fccfd21",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0155bfbbecfc411babab8573800dc593",
            "value": 69
          }
        },
        "09498438f4fa4584b7b9369476efb77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_007912efae324b12ac2bea5532d1826d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_74e1c1de6d474e5f823136a0f0b62cea",
            "value": "‚Äá69/69‚Äá[00:02&lt;00:00,‚Äá28.60it/s]"
          }
        },
        "3b59eef6af4c4f81bd6121f2bf7974e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f4c62404534b018c899b0308530a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2573209d56fc465099bc3e5f6740c746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c42a3ebbbea462abb698ede5fccfd21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0155bfbbecfc411babab8573800dc593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "007912efae324b12ac2bea5532d1826d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74e1c1de6d474e5f823136a0f0b62cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}