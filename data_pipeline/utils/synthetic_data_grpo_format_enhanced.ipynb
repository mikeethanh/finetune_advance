{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f45f52",
   "metadata": {},
   "source": [
    "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe713ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llama-index llama-index-llms-openai llama-index-embeddings-openai llama-index-vector-stores-faiss\n",
    "!pip install -q faiss-cpu beautifulsoup4 requests tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17101945",
   "metadata": {},
   "source": [
    "## 2. Import libraries v√† setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9232c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# LlamaIndex\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "# Setup OpenAI API key t·ª´ Colab Secrets\n",
    "from google.colab import userdata\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "print(\"‚úÖ Setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926a022",
   "metadata": {},
   "source": [
    "## 3. C·∫•u h√¨nh LlamaIndex v√† GRPO Format v·ªõi Enhanced Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1967ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh LLM v√† Embedding\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Set global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "# Define GRPO reasoning format markers\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "# Enhanced requirements with word ranges\n",
    "MIN_REASONING_WORDS = 200\n",
    "MAX_REASONING_WORDS = 300\n",
    "MIN_SOLUTION_WORDS = 200\n",
    "MAX_SOLUTION_WORDS = 400\n",
    "\n",
    "print(\"‚úÖ LlamaIndex configured with GPT-4o-mini\")\n",
    "print(f\"‚úÖ GRPO format markers configured:\")\n",
    "print(f\"   Reasoning: {reasoning_start} ... {reasoning_end} ({MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} words)\")\n",
    "print(f\"   Solution: {solution_start} ... {solution_end} ({MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cf2c1",
   "metadata": {},
   "source": [
    "## 4. Crawl vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ ThuvienPhapluat.vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0557374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_legal_document_thuvienphapluat(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Crawl n·ªôi dung vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ ThuvienPhapluat.vn\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # T√¨m n·ªôi dung ch√≠nh - ThuvienPhapluat th∆∞·ªùng d√πng c√°c selectors n√†y\n",
    "        content_text = \"\"\n",
    "\n",
    "        # C√°ch 1: T√¨m div c√≥ class ho·∫∑c id ch·ª©a n·ªôi dung\n",
    "        selectors = [\n",
    "            {'class': 'content1'},\n",
    "            {'class': 'fulltext'},\n",
    "            {'class': 'noidung'},\n",
    "            {'class': 'content'},\n",
    "            {'id': 'content'},\n",
    "            {'class': 'content-text'},\n",
    "            {'class': 'lawtext'},\n",
    "            {'class': 'law-content'}\n",
    "        ]\n",
    "\n",
    "        for selector in selectors:\n",
    "            content_div = soup.find('div', selector)\n",
    "            if content_div:\n",
    "                content_text = content_div.get_text(separator='\\n', strip=True)\n",
    "                if len(content_text) > 1000:  # N·ªôi dung ƒë·ªß d√†i\n",
    "                    break\n",
    "\n",
    "        # C√°ch 2: T√¨m trong c√°c th·∫ª article ho·∫∑c main\n",
    "        if len(content_text) < 1000:\n",
    "            article = soup.find('article')\n",
    "            if article:\n",
    "                content_text = article.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # C√°ch 3: T√¨m c√°c div c√≥ id ho·∫∑c class ch·ª©a 'text', 'content'\n",
    "        if len(content_text) < 1000:\n",
    "            for div in soup.find_all('div'):\n",
    "                if div.get('class') and any('text' in cls.lower() or 'content' in cls.lower() for cls in div.get('class')):\n",
    "                    text = div.get_text(separator='\\n', strip=True)\n",
    "                    if len(text) > len(content_text):\n",
    "                        content_text = text\n",
    "\n",
    "        # C√°ch 4: N·∫øu kh√¥ng t√¨m th·∫•y, t√¨m trong body v√† lo·∫°i b·ªè script, style\n",
    "        if len(content_text) < 1000:\n",
    "            # X√≥a c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt\n",
    "            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu']):\n",
    "                tag.decompose()\n",
    "\n",
    "            # L·∫•y text t·ª´ body\n",
    "            body = soup.find('body')\n",
    "            if body:\n",
    "                content_text = body.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # L√†m s·∫°ch text\n",
    "        content_text = re.sub(r'\\n\\s*\\n', '\\n\\n', content_text)\n",
    "        lines = content_text.split('\\n')\n",
    "        cleaned_lines = [line for line in lines if len(line.strip()) > 10]\n",
    "        content_text = '\\n'.join(cleaned_lines)\n",
    "\n",
    "        return content_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error crawling {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Crawl t·ª´ 2 URLs ƒë∆∞·ª£c y√™u c·∫ßu\n",
    "document_urls = [\n",
    "    \"https://thuvienphapluat.vn/van-ban/Giao-thong-Van-tai/Luat-trat-tu-an-toan-giao-thong-duong-bo-2024-so-36-2024-QH15-444251.aspx\",\n",
    "    \"https://thuvienphapluat.vn/van-ban/Giao-thong-Van-tai/Luat-Duong-bo-2024-588811.aspx\"\n",
    "]\n",
    "\n",
    "all_legal_text = \"\"\n",
    "\n",
    "for i, url in enumerate(document_urls, 1):\n",
    "    print(f\"üì• Crawling document {i} from: {url}\")\n",
    "    legal_text = crawl_legal_document_thuvienphapluat(url)\n",
    "    \n",
    "    if legal_text:\n",
    "        all_legal_text += f\"\\n\\n=== LU·∫¨T {i} ===\\n\\n\" + legal_text\n",
    "        print(f\"‚úÖ Crawled {len(legal_text):,} characters from document {i}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to crawl document {i}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total crawled: {len(all_legal_text):,} characters\")\n",
    "print(f\"üìÑ Number of lines: {len(all_legal_text.splitlines())}\")\n",
    "print(f\"\\nPreview (first 1000 chars):\\n{all_legal_text[:1000]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8178f5",
   "metadata": {},
   "source": [
    "## 5. Chunking v√† t·∫°o Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o Document t·ª´ text crawled\n",
    "document = Document(text=all_legal_text)\n",
    "\n",
    "# Chunking v·ªõi SentenceSplitter (tƒÉng chunk size ƒë·ªÉ ph√π h·ª£p v·ªõi y√™u c·∫ßu enhanced)\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=800,  # TƒÉng chunk size ƒë·ªÉ c√≥ nhi·ªÅu context h∆°n\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents([document])\n",
    "\n",
    "print(f\"‚úÖ Created {len(nodes)} chunks\")\n",
    "print(f\"\\nSample chunk:\\n{nodes[0].text[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22075b41",
   "metadata": {},
   "source": [
    "## 6. T·∫°o FAISS Vector Store v√† merge similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aea884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o FAISS index\n",
    "dimension = 1536  # text-embedding-3-small dimension\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# T·∫°o vector store\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "# T·∫°o index t·ª´ nodes\n",
    "print(\"üîÑ Creating vector index...\")\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    vector_store=vector_store,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector index created with {len(nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138be93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_similar_chunks(nodes: List, similarity_threshold: float = 0.70) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Merge c√°c chunks c√≥ cosine similarity cao ƒë·ªÉ t·∫°o contexts phong ph√∫ h∆°n\n",
    "    Gi·∫£m threshold ƒë·ªÉ t·∫°o contexts l·ªõn h∆°n ph√π h·ª£p v·ªõi y√™u c·∫ßu 300+ words\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Merging similar chunks (threshold={similarity_threshold})...\")\n",
    "    print(f\"üìä Input: {len(nodes)} chunks\")\n",
    "\n",
    "    # Get embeddings cho t·∫•t c·∫£ nodes\n",
    "    embeddings = []\n",
    "    for node in tqdm(nodes, desc=\"Getting embeddings\"):\n",
    "        emb = embed_model.get_text_embedding(node.text)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    # T√≠nh cosine similarity matrix\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    # Merge chunks\n",
    "    merged_contexts = []\n",
    "    used_indices = set()\n",
    "\n",
    "    for i in range(len(nodes)):\n",
    "        if i in used_indices:\n",
    "            continue\n",
    "\n",
    "        # T√¨m c√°c chunks similar\n",
    "        similar_indices = np.where(similarity_matrix[i] > similarity_threshold)[0]\n",
    "        similar_indices = [idx for idx in similar_indices if idx not in used_indices]\n",
    "\n",
    "        # Merge text\n",
    "        merged_text = \"\\n\\n\".join([nodes[idx].text for idx in similar_indices])\n",
    "\n",
    "        # Ch·ªâ keep contexts ƒë·ªß l·ªõn ƒë·ªÉ t·∫°o ra reasoning v√† solution d√†i\n",
    "        if len(merged_text.split()) >= 150:  # Minimum context size\n",
    "            merged_contexts.append({\n",
    "                'context': merged_text,\n",
    "                'num_chunks': len(similar_indices),\n",
    "                'chunk_indices': similar_indices\n",
    "            })\n",
    "\n",
    "        used_indices.update(similar_indices)\n",
    "\n",
    "    print(f\"‚úÖ Merged {len(nodes)} chunks into {len(merged_contexts)} contexts\")\n",
    "    print(f\"üìà Avg chunks per context: {len(nodes)/len(merged_contexts):.1f}\")\n",
    "    return merged_contexts\n",
    "\n",
    "# Merge chunks v·ªõi threshold th·∫•p h∆°n ƒë·ªÉ t·∫°o contexts l·ªõn h∆°n\n",
    "merged_contexts = merge_similar_chunks(nodes, similarity_threshold=0.70)\n",
    "print(f\"\\nüìä Number of merged contexts: {len(merged_contexts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa93a5",
   "metadata": {},
   "source": [
    "## 7. Generate Questions t·ª´ Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1263435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_context(context: str, num_questions: int = 8) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate c√¢u h·ªèi ph·ª©c t·∫°p h∆°n t·ª´ context ƒë·ªÉ ph√π h·ª£p v·ªõi enhanced requirements\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"B·∫°n l√† chuy√™n gia ph√°p lu·∫≠t giao th√¥ng Vi·ªát Nam. D·ª±a tr√™n ƒëo·∫°n vƒÉn b·∫£n ph√°p lu·∫≠t d∆∞·ªõi ƒë√¢y, h√£y t·∫°o {num_questions} c√¢u h·ªèi hay, ph·ª©c t·∫°p v√† ƒëa d·∫°ng m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ h·ªèi.\n",
    "\n",
    "VƒÉn b·∫£n ph√°p lu·∫≠t:\n",
    "{context}\n",
    "\n",
    "Y√™u c·∫ßu:\n",
    "- C√¢u h·ªèi ph·∫£i ph·ª©c t·∫°p, y√™u c·∫ßu ph√¢n t√≠ch s√¢u v√† gi·∫£i th√≠ch chi ti·∫øt\n",
    "- ƒêa d·∫°ng v·ªÅ lo·∫°i: h·ªèi ƒë·ªãnh nghƒ©a chi ti·∫øt, so s√°nh, ph√¢n t√≠ch t√¨nh hu·ªëng, th·ªß t·ª•c ph·ª©c t·∫°p, quy·ªÅn v√† nghƒ©a v·ª•\n",
    "- C√¢u h·ªèi c·∫ßn c√≥ th·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi d√†i √≠t nh·∫•t 300 t·ª´\n",
    "- ∆Øu ti√™n c√°c c√¢u h·ªèi v·ªÅ t√¨nh hu·ªëng th·ª±c t·∫ø, case study, ho·∫∑c y√™u c·∫ßu gi·∫£i th√≠ch logic ph√°p l√Ω\n",
    "- C√¢u h·ªèi ph·∫£i c√≥ th·ªÉ tr·∫£ l·ªùi ƒë∆∞·ª£c t·ª´ context tr√™n\n",
    "- Tr·∫£ v·ªÅ ONLY danh s√°ch c√°c c√¢u h·ªèi, m·ªói c√¢u m·ªôt d√≤ng, kh√¥ng ƒë√°nh s·ªë\n",
    "\n",
    "C√¢u h·ªèi:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.complete(prompt)\n",
    "        questions_text = response.text.strip()\n",
    "\n",
    "        # Parse questions\n",
    "        questions = [q.strip() for q in questions_text.split('\\n') if q.strip()]\n",
    "        # Lo·∫°i b·ªè numbering n·∫øu c√≥\n",
    "        questions = [re.sub(r'^\\d+\\.\\s*', '', q) for q in questions]\n",
    "\n",
    "        return questions[:num_questions]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating questions: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test v·ªõi 1 context\n",
    "test_context = merged_contexts[0]['context']\n",
    "test_questions = generate_questions_from_context(test_context, num_questions=8)\n",
    "\n",
    "print(\"üìù Sample generated questions:\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f375762",
   "metadata": {},
   "source": [
    "## 8. Generate Enhanced Structured Answers cho GRPO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85896486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enhanced_structured_answer(question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate c√¢u tr·∫£ l·ªùi c√≥ c·∫•u tr√∫c GRPO format v·ªõi y√™u c·∫ßu enhanced:\n",
    "    - Reasoning 200-300 t·ª´\n",
    "    - Solution 200-400 t·ª´\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"B·∫°n l√† tr·ª£ l√Ω AI chuy√™n s√¢u v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. Khi tr·∫£ l·ªùi c√¢u h·ªèi, h√£y:\n",
    "1. Suy nghƒ© v√† ph√¢n t√≠ch chi ti·∫øt trong ph·∫ßn {reasoning_start} {reasoning_end} ({MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} t·ª´)\n",
    "2. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt trong ph·∫ßn {solution_start}{solution_end} ({MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} t·ª´)\n",
    "\n",
    "VƒÉn b·∫£n ph√°p lu·∫≠t:\n",
    "{context}\n",
    "\n",
    "C√¢u h·ªèi: {question}\n",
    "\n",
    "Y√™u c·∫ßu cho ph·∫ßn suy nghƒ© (working_out) - {MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} t·ª´:\n",
    "- Ph√¢n t√≠ch c√¢u h·ªèi: ng∆∞·ªùi d√πng mu·ªën bi·∫øt g√¨ c·ª• th·ªÉ?\n",
    "- X√°c ƒë·ªãnh v√† li·ªát k√™ c√°c ƒëi·ªÅu lu·∫≠t, kho·∫£n, ƒëi·ªÉm li√™n quan\n",
    "- Ph√¢n t√≠ch t·ª´ng quy ƒë·ªãnh m·ªôt c√°ch chi ti·∫øt\n",
    "- Xem x√©t c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát, ngo·∫°i l·ªá\n",
    "- So s√°nh v·ªõi c√°c quy ƒë·ªãnh kh√°c n·∫øu c√≥\n",
    "- ƒê√°nh gi√° t√°c ƒë·ªông v√† √Ω nghƒ©a c·ªßa quy ƒë·ªãnh\n",
    "- Gi·∫£i th√≠ch logic v√† nguy√™n t·∫Øc ph√°p l√Ω ƒë·∫±ng sau\n",
    "\n",
    "Y√™u c·∫ßu cho ph·∫ßn solution - {MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} t·ª´:\n",
    "- Tr·∫£ l·ªùi tr·ª±c ti·∫øp v√† to√†n di·ªán c√¢u h·ªèi\n",
    "- Tr√≠ch d·∫´n ch√≠nh x√°c c√°c ƒëi·ªÅu lu·∫≠t, kho·∫£n, ƒëi·ªÉm\n",
    "- Gi·∫£i th√≠ch r√µ r√†ng c√°c kh√°i ni·ªám ph√°p l√Ω\n",
    "- ƒê∆∞a ra v√≠ d·ª• c·ª• th·ªÉ n·∫øu ph√π h·ª£p\n",
    "- H∆∞·ªõng d·∫´n th·ª±c hi·ªán ho·∫∑c tu√¢n th·ªß\n",
    "- L∆∞u √Ω c√°c ƒëi·ªÉm quan tr·ªçng c·∫ßn ch√∫ √Ω\n",
    "- Tham kh·∫£o th√™m n·∫øu c·∫ßn thi·∫øt\n",
    "\n",
    "Format b·∫Øt bu·ªôc:\n",
    "{reasoning_start}\n",
    "[Ph·∫ßn ph√¢n t√≠ch v√† suy nghƒ© chi ti·∫øt - {MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} t·ª´]\n",
    "{reasoning_end}\n",
    "\n",
    "{solution_start}[C√¢u tr·∫£ l·ªùi chi ti·∫øt v√† to√†n di·ªán - {MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} t·ª´]{solution_end}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.complete(prompt)\n",
    "        answer = response.text.strip()\n",
    "\n",
    "        # Validate format v√† ƒë·ªô d√†i\n",
    "        if reasoning_start in answer and reasoning_end in answer and solution_start in answer and solution_end in answer:\n",
    "            # Extract v√† check word count\n",
    "            reasoning_section = extract_reasoning_section(answer)\n",
    "            solution_section = extract_solution_section(answer)\n",
    "            \n",
    "            reasoning_words = len(reasoning_section.split())\n",
    "            solution_words = len(solution_section.split())\n",
    "            \n",
    "            # If either section doesn't meet word count requirements, regenerate with emphasis\n",
    "            reasoning_valid = MIN_REASONING_WORDS <= reasoning_words <= MAX_REASONING_WORDS\n",
    "            solution_valid = MIN_SOLUTION_WORDS <= solution_words <= MAX_SOLUTION_WORDS\n",
    "            \n",
    "            if not reasoning_valid or not solution_valid:\n",
    "                print(f\"‚ö†Ô∏è Regenerating - Reasoning: {reasoning_words} (need {MIN_REASONING_WORDS}-{MAX_REASONING_WORDS}), Solution: {solution_words} (need {MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS})\")\n",
    "                \n",
    "                # Try again with more emphasis\n",
    "                enhanced_prompt = prompt + f\"\\n\\nQUAN TR·ªåNG: Reasoning ph·∫£i c√≥ {MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} t·ª´, Solution ph·∫£i c√≥ {MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} t·ª´. H√£y ƒë·∫£m b·∫£o ƒë·ªô d√†i ch√≠nh x√°c!\"\n",
    "                response = llm.complete(enhanced_prompt)\n",
    "                answer = response.text.strip()\n",
    "            \n",
    "            return answer\n",
    "        else:\n",
    "            # Fallback: create structure if not properly formatted\n",
    "            simple_answer = response.text.strip()\n",
    "            structured = f\"\"\"{reasoning_start}\n",
    "ƒê·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y m·ªôt c√°ch to√†n di·ªán, t√¥i c·∫ßn ph√¢n t√≠ch chi ti·∫øt vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p v√† xem x√©t t·∫•t c·∫£ c√°c kh√≠a c·∫°nh li√™n quan ƒë·∫øn: {question}\n",
    "\n",
    "Tr∆∞·ªõc ti√™n, t√¥i s·∫Ω x√°c ƒë·ªãnh c√°c ƒëi·ªÅu kho·∫£n ph√°p lu·∫≠t √°p d·ª•ng tr·ª±c ti·∫øp cho v·∫•n ƒë·ªÅ n√†y. Vi·ªác ph√¢n t√≠ch n√†y ƒë√≤i h·ªèi s·ª± hi·ªÉu bi·∫øt s√¢u s·∫Øc v·ªÅ h·ªá th·ªëng ph√°p lu·∫≠t giao th√¥ng Vi·ªát Nam v√† c√°ch c√°c quy ƒë·ªãnh ƒë∆∞·ª£c √°p d·ª•ng trong th·ª±c t·∫ø.\n",
    "\n",
    "Ti·∫øp theo, t√¥i c·∫ßn xem x√©t b·ªëi c·∫£nh l·ªãch s·ª≠ v√† m·ª•c ƒë√≠ch c·ªßa c√°c quy ƒë·ªãnh n√†y, ƒë·ªÉ hi·ªÉu r√µ √Ω ƒë·ªãnh c·ªßa nh√† l√†m lu·∫≠t v√† c√°ch th·ª©c √°p d·ª•ng ph√π h·ª£p. ƒêi·ªÅu n√†y bao g·ªìm vi·ªác ph√¢n t√≠ch c√°c nguy√™n t·∫Øc c∆° b·∫£n c·ªßa lu·∫≠t giao th√¥ng, c√°c quy·ªÅn v√† nghƒ©a v·ª• c·ªßa c√°c ch·ªß th·ªÉ tham gia giao th√¥ng, v√† m·ªëi quan h·ªá gi·ªØa c√°c quy ƒë·ªãnh kh√°c nhau trong h·ªá th·ªëng ph√°p lu·∫≠t.\n",
    "\n",
    "Cu·ªëi c√πng, t√¥i s·∫Ω ƒë√°nh gi√° c√°c t√°c ƒë·ªông th·ª±c ti·ªÖn v√† ƒë∆∞a ra c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ph√¢n t√≠ch ph√°p l√Ω ch√≠nh x√°c v√† to√†n di·ªán.\n",
    "{reasoning_end}\n",
    "\n",
    "{solution_start}{simple_answer}{solution_end}\"\"\"\n",
    "            return structured\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating enhanced structured answer: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_reasoning_section(answer):\n",
    "    \"\"\"Extract text between reasoning markers\"\"\"\n",
    "    try:\n",
    "        start_idx = answer.find(reasoning_start) + len(reasoning_start)\n",
    "        end_idx = answer.find(reasoning_end)\n",
    "        return answer[start_idx:end_idx].strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_solution_section(answer):\n",
    "    \"\"\"Extract text between solution markers\"\"\"\n",
    "    try:\n",
    "        start_idx = answer.find(solution_start) + len(solution_start)\n",
    "        end_idx = answer.find(solution_end)\n",
    "        return answer[start_idx:end_idx].strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# Test v·ªõi 1 question\n",
    "test_enhanced_answer = generate_enhanced_structured_answer(test_questions[0], test_context)\n",
    "\n",
    "print(\"üí¨ Sample Enhanced Structured Q&A:\")\n",
    "print(f\"Q: {test_questions[0]}\")\n",
    "print(f\"A: {test_enhanced_answer[:800]}...\")\n",
    "\n",
    "# Check word counts\n",
    "reasoning_section = extract_reasoning_section(test_enhanced_answer)\n",
    "solution_section = extract_solution_section(test_enhanced_answer)\n",
    "print(f\"\\nüìä Word counts - Reasoning: {len(reasoning_section.split())}, Solution: {len(solution_section.split())}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33687571",
   "metadata": {},
   "source": [
    "## 9. Generate to√†n b·ªô Enhanced GRPO dataset (Target: 1000+ samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enhanced_grpo_dataset(\n",
    "    contexts: List[Dict],\n",
    "    target_samples: int = 1000,\n",
    "    questions_per_context: int = 20\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate enhanced synthetic dataset v·ªõi GRPO structured format\n",
    "    Y√™u c·∫ßu: reasoning 200-300 t·ª´, solution 200-400 t·ª´\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    failed_generations = 0\n",
    "\n",
    "    # T√≠nh s·ªë contexts c·∫ßn d√πng\n",
    "    num_contexts_needed = min(len(contexts), (target_samples // questions_per_context) + 1)\n",
    "\n",
    "    print(f\"üöÄ Generating {target_samples} Enhanced GRPO Q&A pairs from {num_contexts_needed} contexts...\")\n",
    "    print(f\"üìè Requirements: Reasoning {MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} words, Solution {MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} words\")\n",
    "\n",
    "    for ctx_idx, ctx_data in enumerate(tqdm(contexts[:num_contexts_needed], desc=\"Processing contexts\")):\n",
    "        context = ctx_data['context']\n",
    "\n",
    "        # Skip contexts qu√° ng·∫Øn\n",
    "        if len(context.split()) < 100:\n",
    "            continue\n",
    "\n",
    "        # Generate questions\n",
    "        questions = generate_questions_from_context(context, num_questions=questions_per_context)\n",
    "\n",
    "        # Generate enhanced structured answers cho m·ªói question\n",
    "        for q_idx, question in enumerate(questions):\n",
    "            if len(dataset) >= target_samples:\n",
    "                break\n",
    "\n",
    "            print(f\"\\rüîÑ Context {ctx_idx+1}/{num_contexts_needed}, Q {q_idx+1}/{len(questions)}, Total: {len(dataset)}/{target_samples}\", end=\"\")\n",
    "            \n",
    "            structured_answer = generate_enhanced_structured_answer(question, context)\n",
    "\n",
    "            if structured_answer and reasoning_start in structured_answer:\n",
    "                # Validate word counts\n",
    "                reasoning_section = extract_reasoning_section(structured_answer)\n",
    "                solution_section = extract_solution_section(structured_answer)\n",
    "                \n",
    "                reasoning_words = len(reasoning_section.split())\n",
    "                solution_words = len(solution_section.split())\n",
    "                \n",
    "                # Only add if meets word count requirements\n",
    "                reasoning_valid = MIN_REASONING_WORDS <= reasoning_words <= MAX_REASONING_WORDS\n",
    "                solution_valid = MIN_SOLUTION_WORDS <= solution_words <= MAX_SOLUTION_WORDS\n",
    "                \n",
    "                if reasoning_valid and solution_valid:\n",
    "                    dataset.append({\n",
    "                        'question': question,\n",
    "                        'answer': structured_answer,\n",
    "                        'format': 'enhanced_grpo_structured',\n",
    "                        'reasoning_words': reasoning_words,\n",
    "                        'solution_words': solution_words,\n",
    "                        'context_length': len(context.split())\n",
    "                    })\n",
    "                else:\n",
    "                    failed_generations += 1\n",
    "                    print(f\"\\n‚ö†Ô∏è Skipped: Reasoning {reasoning_words} (need {MIN_REASONING_WORDS}-{MAX_REASONING_WORDS}) or Solution {solution_words} (need {MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS})\")\n",
    "            else:\n",
    "                failed_generations += 1\n",
    "\n",
    "        if len(dataset) >= target_samples:\n",
    "            break\n",
    "\n",
    "    print(f\"\\n\\n‚úÖ Generated {len(dataset)} Enhanced GRPO Q&A pairs\")\n",
    "    print(f\"‚ùå Failed generations: {failed_generations}\")\n",
    "    return dataset\n",
    "\n",
    "# Generate enhanced dataset\n",
    "enhanced_grpo_data = generate_enhanced_grpo_dataset(\n",
    "    merged_contexts,\n",
    "    target_samples=1000,\n",
    "    questions_per_context=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbe684",
   "metadata": {},
   "source": [
    "## 10. L∆∞u Enhanced GRPO dataset d∆∞·ªõi d·∫°ng JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19438d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u ra file JSONL v·ªõi t√™n ph√¢n bi·ªát\n",
    "output_path = 'enhanced_synthetic_legal_qa_grpo_format.jsonl'\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for item in enhanced_grpo_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"‚úÖ Saved {len(enhanced_grpo_data)} Enhanced GRPO Q&A pairs to {output_path}\")\n",
    "\n",
    "# Download file (n·∫øu ch·∫°y tr√™n Colab)\n",
    "from google.colab import files\n",
    "files.download(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36360879",
   "metadata": {},
   "source": [
    "## 11. Th·ªëng k√™ & Preview Enhanced GRPO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3531a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# T·∫°o DataFrame ƒë·ªÉ ph√¢n t√≠ch\n",
    "df = pd.DataFrame(enhanced_grpo_data)\n",
    "\n",
    "# Th·ªëng k√™ ƒë·ªô d√†i\n",
    "df['question_len'] = df['question'].apply(lambda x: len(x.split()))\n",
    "df['answer_len'] = df['answer'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"üìä ENHANCED GRPO DATASET STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"All samples meet enhanced requirements: ‚úÖ\")\n",
    "print(f\"\\nQuestion length (words):\")\n",
    "print(df['question_len'].describe())\n",
    "print(f\"\\nAnswer length (words):\")\n",
    "print(df['answer_len'].describe())\n",
    "print(f\"\\nReasoning section length (words):\")\n",
    "print(df['reasoning_words'].describe())\n",
    "print(f\"\\nSolution section length (words):\")\n",
    "print(df['solution_words'].describe())\n",
    "print(f\"\\nContext length (words):\")\n",
    "print(df['context_length'].describe())\n",
    "\n",
    "# Quality checks\n",
    "print(f\"\\nüìè QUALITY CHECKS:\")\n",
    "reasoning_in_range = ((df['reasoning_words'] >= MIN_REASONING_WORDS) & (df['reasoning_words'] <= MAX_REASONING_WORDS))\n",
    "solution_in_range = ((df['solution_words'] >= MIN_SOLUTION_WORDS) & (df['solution_words'] <= MAX_SOLUTION_WORDS))\n",
    "print(f\"Reasoning sections {MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} words: {reasoning_in_range.sum()}/{len(df)} ({reasoning_in_range.mean()*100:.1f}%)\")\n",
    "print(f\"Solution sections {MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} words: {solution_in_range.sum()}/{len(df)} ({solution_in_range.mean()*100:.1f}%)\")\n",
    "\n",
    "# Preview samples\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE ENHANCED GRPO Q&A PAIRS:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(min(2, len(df))):\n",
    "    print(f\"\\n[Sample {i+1}]\")\n",
    "    print(f\"Q: {df.iloc[i]['question']}\")\n",
    "    print(f\"A: {df.iloc[i]['answer'][:600]}...\")\n",
    "    print(f\"üìä Stats: R={df.iloc[i]['reasoning_words']}w, S={df.iloc[i]['solution_words']}w, Total={df.iloc[i]['answer_len']}w\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60455d8",
   "metadata": {},
   "source": [
    "## 12. Enhanced Validation & Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra ch·∫•t l∆∞·ª£ng format Enhanced GRPO\n",
    "print(\"üîç ENHANCED GRPO FORMAT QUALITY CHECKS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check duplicates\n",
    "duplicate_questions = df[df.duplicated(subset=['question'], keep=False)]\n",
    "print(f\"Duplicate questions: {len(duplicate_questions)}\")\n",
    "\n",
    "# Check format structure\n",
    "has_reasoning = df['answer'].apply(lambda x: reasoning_start in x and reasoning_end in x)\n",
    "has_solution = df['answer'].apply(lambda x: solution_start in x and solution_end in x)\n",
    "proper_format = has_reasoning & has_solution\n",
    "\n",
    "print(f\"Has reasoning format: {has_reasoning.sum()}/{len(df)} ({has_reasoning.mean()*100:.1f}%)\")\n",
    "print(f\"Has solution format: {has_solution.sum()}/{len(df)} ({has_solution.mean()*100:.1f}%)\")\n",
    "print(f\"Proper format: {proper_format.sum()}/{len(df)} ({proper_format.mean()*100:.1f}%)\")\n",
    "\n",
    "# Check enhanced requirements compliance\n",
    "meets_reasoning_req = (df['reasoning_words'] >= MIN_REASONING_WORDS) & (df['reasoning_words'] <= MAX_REASONING_WORDS)\n",
    "meets_solution_req = (df['solution_words'] >= MIN_SOLUTION_WORDS) & (df['solution_words'] <= MAX_SOLUTION_WORDS)\n",
    "meets_both_req = meets_reasoning_req & meets_solution_req\n",
    "\n",
    "print(f\"\\nüìè ENHANCED REQUIREMENTS COMPLIANCE:\")\n",
    "print(f\"Reasoning {MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} words: {meets_reasoning_req.sum()}/{len(df)} ({meets_reasoning_req.mean()*100:.1f}%)\")\n",
    "print(f\"Solution {MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} words: {meets_solution_req.sum()}/{len(df)} ({meets_solution_req.mean()*100:.1f}%)\")\n",
    "print(f\"Meets both requirements: {meets_both_req.sum()}/{len(df)} ({meets_both_req.mean()*100:.1f}%)\")\n",
    "\n",
    "# Distribution analysis\n",
    "print(f\"\\nüìà LENGTH DISTRIBUTION:\")\n",
    "print(f\"Reasoning words - Min: {df['reasoning_words'].min()}, Max: {df['reasoning_words'].max()}, Avg: {df['reasoning_words'].mean():.1f}\")\n",
    "print(f\"Solution words - Min: {df['solution_words'].min()}, Max: {df['solution_words'].max()}, Avg: {df['solution_words'].mean():.1f}\")\n",
    "print(f\"Total answer words - Min: {df['answer_len'].min()}, Max: {df['answer_len'].max()}, Avg: {df['answer_len'].mean():.1f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced GRPO format quality check completed!\")\n",
    "\n",
    "# Sample detailed validation\n",
    "if len(df) > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE DETAILED VALIDATION:\")\n",
    "    sample_answer = df.iloc[0]['answer']\n",
    "    reasoning_text = extract_reasoning_section(sample_answer)\n",
    "    solution_text = extract_solution_section(sample_answer)\n",
    "\n",
    "    reasoning_valid = MIN_REASONING_WORDS <= len(reasoning_text.split()) <= MAX_REASONING_WORDS\n",
    "    solution_valid = MIN_SOLUTION_WORDS <= len(solution_text.split()) <= MAX_SOLUTION_WORDS\n",
    "\n",
    "    print(f\"\\nExtracted Reasoning ({len(reasoning_text.split())} words):\\n{reasoning_text[:400]}...\")\n",
    "    print(f\"\\nExtracted Solution ({len(solution_text.split())} words):\\n{solution_text[:400]}...\")\n",
    "    print(f\"\\nFormat valid: {bool(reasoning_text and solution_text)}\")\n",
    "    print(f\"Requirements met: R({MIN_REASONING_WORDS}-{MAX_REASONING_WORDS}){'‚úÖ' if reasoning_valid else '‚ùå'}, S({MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS}){'‚úÖ' if solution_valid else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878761f",
   "metadata": {},
   "source": [
    "## 13. Export cho Enhanced SFT Training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o enhanced format training cho SFT v·ªõi system prompt\n",
    "enhanced_system_prompt = f\"\"\"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n s√¢u v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. Khi tr·∫£ l·ªùi c√¢u h·ªèi, h√£y:\n",
    "1. Suy nghƒ© v√† ph√¢n t√≠ch chi ti·∫øt c√¢u h·ªèi trong ph·∫ßn {reasoning_start} {reasoning_end} ({MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} t·ª´)\n",
    "2. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán trong ph·∫ßn {solution_start}{solution_end} ({MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} t·ª´)\n",
    "\n",
    "Ph·∫ßn suy nghƒ© c·∫ßn bao g·ªìm: ph√¢n t√≠ch c√¢u h·ªèi, x√°c ƒë·ªãnh ƒëi·ªÅu lu·∫≠t li√™n quan, gi·∫£i th√≠ch logic ph√°p l√Ω, v√† ƒë√°nh gi√° c√°c kh√≠a c·∫°nh kh√°c nhau.\n",
    "Ph·∫ßn gi·∫£i ƒë√°p c·∫ßn: tr·∫£ l·ªùi tr·ª±c ti·∫øp, tr√≠ch d·∫´n ch√≠nh x√°c ph√°p lu·∫≠t, gi·∫£i th√≠ch r√µ r√†ng, v√† ƒë∆∞a ra h∆∞·ªõng d·∫´n c·ª• th·ªÉ.\n",
    "\n",
    "C√¢u tr·∫£ l·ªùi c·∫ßn d·ª±a tr√™n quy ƒë·ªãnh ph√°p lu·∫≠t hi·ªán h√†nh v√† ph·∫£i chi ti·∫øt, to√†n di·ªán, d·ªÖ hi·ªÉu.\"\"\"\n",
    "\n",
    "# Convert sang format training\n",
    "enhanced_training_data = []\n",
    "for item in enhanced_grpo_data:\n",
    "    reasoning_valid = MIN_REASONING_WORDS <= item['reasoning_words'] <= MAX_REASONING_WORDS\n",
    "    solution_valid = MIN_SOLUTION_WORDS <= item['solution_words'] <= MAX_SOLUTION_WORDS\n",
    "    \n",
    "    if (item['answer'] and \n",
    "        reasoning_start in item['answer'] and\n",
    "        reasoning_valid and solution_valid):\n",
    "        \n",
    "        training_item = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": enhanced_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": item['question']},\n",
    "                {\"role\": \"assistant\", \"content\": item['answer']}\n",
    "            ],\n",
    "            \"metadata\": {\n",
    "                \"reasoning_words\": item['reasoning_words'],\n",
    "                \"solution_words\": item['solution_words'],\n",
    "                \"format\": \"enhanced_grpo\"\n",
    "            }\n",
    "        }\n",
    "        enhanced_training_data.append(training_item)\n",
    "\n",
    "# L∆∞u enhanced training format\n",
    "enhanced_training_output_path = 'enhanced_synthetic_legal_qa_grpo_training.jsonl'\n",
    "\n",
    "with open(enhanced_training_output_path, 'w', encoding='utf-8') as f:\n",
    "    for item in enhanced_training_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"‚úÖ Saved {len(enhanced_training_data)} enhanced training examples to {enhanced_training_output_path}\")\n",
    "print(f\"üìà Ready for Enhanced SFT training with GRPO format!\")\n",
    "\n",
    "# Download training file\n",
    "files.download(enhanced_training_output_path)\n",
    "\n",
    "print(\"\\nüéØ ENHANCED SUMMARY:\")\n",
    "print(f\"- Crawled from 2 legal documents: Lu·∫≠t tr·∫≠t t·ª± ATGT ƒë∆∞·ªùng b·ªô 2024 & Lu·∫≠t ƒê∆∞·ªùng b·ªô 2024\")\n",
    "print(f\"- Generated {len(enhanced_grpo_data)} enhanced synthetic samples\")\n",
    "print(f\"- {len(enhanced_training_data)} samples meet enhanced requirements\")\n",
    "print(f\"- Reasoning sections: {MIN_REASONING_WORDS}-{MAX_REASONING_WORDS} words (avg: {df['reasoning_words'].mean():.0f})\")\n",
    "print(f\"- Solution sections: {MIN_SOLUTION_WORDS}-{MAX_SOLUTION_WORDS} words (avg: {df['solution_words'].mean():.0f})\")\n",
    "print(f\"- Format includes detailed {reasoning_start}...{reasoning_end} and {solution_start}...{solution_end}\")\n",
    "print(f\"- Ready for Enhanced SFT training with comprehensive reasoning and solutions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
