{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f02e2f75",
      "metadata": {
        "id": "f02e2f75"
      },
      "source": [
        "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4487fab7",
      "metadata": {
        "id": "4487fab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac32728-012e-412a-cd66-1e7b09662004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index llama-index-llms-openai llama-index-embeddings-openai llama-index-vector-stores-faiss\n",
        "!pip install -q faiss-cpu beautifulsoup4 requests tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab94629",
      "metadata": {
        "id": "3ab94629"
      },
      "source": [
        "## 2. Import libraries v√† setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0e6fb963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e6fb963",
        "outputId": "d8483fd9-974d-466c-9dfc-457bd32c1a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setup completed!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "# LlamaIndex\n",
        "from llama_index.core import Document, VectorStoreIndex, Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "import faiss\n",
        "\n",
        "# Setup OpenAI API key t·ª´ Colab Secrets\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "print(\"‚úÖ Setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f71e3ae5",
      "metadata": {
        "id": "f71e3ae5"
      },
      "source": [
        "## 3. C·∫•u h√¨nh LlamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "56144234",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56144234",
        "outputId": "7ab061fa-f212-460a-b54f-19098de9a7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LlamaIndex configured with GPT-4o-mini\n"
          ]
        }
      ],
      "source": [
        "# C·∫•u h√¨nh LLM v√† Embedding\n",
        "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Set global settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 50\n",
        "\n",
        "print(\"‚úÖ LlamaIndex configured with GPT-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad516d98",
      "metadata": {
        "id": "ad516d98"
      },
      "source": [
        "## 4. Crawl vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ VBPL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3d79cbf2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d79cbf2",
        "outputId": "5eee577c-dcd2-45eb-8dae-b69103b0decb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Crawling document from: https://www.moj.gov.vn/vbpq/lists/vn%20bn%20php%20lut/view_detail.aspx?itemid=23311\n",
            "\n",
            "‚úÖ Crawled 62,877 characters\n",
            "üìÑ Number of lines: 602\n",
            "\n",
            "Preview (first 1000 chars):\n",
            "Turn on more accessible mode\n",
            "Turn off more accessible mode\n",
            "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ B·ªô T∆∞ ph√°p - Ministry of Justice‚Äôs portal\n",
            "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠\n",
            "CSDLQG v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t\n",
            "Ch∆∞∆°ng VIII\n",
            "C·ª•c C√¥ng ngh·ªá th√¥ng tin, B·ªô T∆∞ ph√°p tr√¢n tr·ªçng c·∫£m ∆°n Qu√Ω ƒë·ªôc gi·∫£ trong th·ªùi gian qua ƒë√£ s·ª≠ d·ª•ng h·ªá th·ªëng vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t t·∫°i ƒë·ªãa ch·ªâ\n",
            "http://www.moj.gov.vn/pages/vbpq.aspx\n",
            "ƒê·∫øn nay, nh·∫±m ph·ª•c v·ª• t·ªët h∆°n nhu c·∫ßu khai th√°c, tra c·ª©u vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t t·ª´ Trung ∆∞∆°ng ƒë·∫øn ƒë·ªãa ph∆∞∆°ng, C·ª•c C√¥ng ngh·ªá th√¥ng tin ƒë√£ ƒë∆∞a C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t v√†o s·ª≠ d·ª•ng t·∫°i ƒë·ªãa ch·ªâ\n",
            "http://vbpl.vn/Pages/portal.aspx\n",
            "ƒë·ªÉ thay th·∫ø cho h·ªá th·ªëng c≈© n√≥i tr√™n.\n",
            "C·ª•c C√¥ng ngh·ªá th√¥ng tin tr√¢n tr·ªçng th√¥ng b√°o t·ªõi Qu√Ω ƒë·ªôc gi·∫£ ƒë∆∞·ª£c bi·∫øt v√† mong r·∫±ng C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t s·∫Ω ti·∫øp t·ª•c l√† ƒë·ªãa ch·ªâ tin c·∫≠y ƒë·ªÉ khai th√°c, tra c·ª©u vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t.\n",
            "Trong qu√° tr√¨nh s·ª≠ d·ª•ng, ch√∫ng t√¥i lu√¥n hoan ngh√™nh m·ªçi √Ω ki·∫øn g√≥p √Ω c·ªßa Qu√Ω ƒë·ªôc gi·∫£ ƒë·ªÉ C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c ...\n",
            "\n",
            "...============================================================\n",
            "Preview (middle section):\n",
            "hi·ªÉn giao th√¥ng tr√™n ƒë∆∞·ªùng; h∆∞·ªõng d·∫´n, b·∫Øt bu·ªôc ng∆∞·ªùi tham gia giao th√¥ng ch·∫•p h√†nh quy t·∫Øc giao th√¥ng;\n",
            "b) Khi c√≥ t√¨nh hu·ªëng ƒë·ªôt xu·∫•t g√¢y √°ch t·∫Øc giao th√¥ng ho·∫∑c c√≥ y√™u c·∫ßu c·∫ßn thi·∫øt kh√°c v·ªÅ b·∫£o ƒë·∫£m an ninh, tr·∫≠t t·ª± ƒë∆∞·ª£c t·∫°m th·ªùi ƒë√¨nh ch·ªâ ƒëi l·∫°i ·ªü m·ªôt s·ªë ƒëo·∫°n ƒë∆∞·ªùng nh·∫•t ƒë·ªãnh, ph√¢n l·∫°i lu·ªìng, ph√¢n l·∫°i tuy·∫øn v√† n∆°i t·∫°m d·ª´ng xe, ƒë·ªó xe.\n",
            "36. Tr√°ch nhi·ªám c·ªßa c√° nh√¢n, c∆° quan, t·ªï ch·ª©c khi x·∫£y ra tai n·∫°n giao th√¥ng\n",
            "1. Ng∆∞·ªùi l√°i xe v√† nh·ªØng ng∆∞·ªùi li√™n quan tr·ª±c ti·∫øp ƒë·∫øn v·ª• tai n·∫°n ph·∫£i c√≥ tr√°ch nhi·ªám:\n",
            "a)...\n"
          ]
        }
      ],
      "source": [
        "def crawl_legal_document(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Crawl n·ªôi dung vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ VBPL.vn\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # T√¨m n·ªôi dung ch√≠nh - VBPL th∆∞·ªùng d√πng c√°c selectors n√†y\n",
        "        # Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ t√¨m n·ªôi dung\n",
        "        content_text = \"\"\n",
        "\n",
        "        # C√°ch 1: T√¨m div c√≥ id ho·∫∑c class ch·ª©a \"content\", \"fulltext\", \"noidung\"\n",
        "        selectors = [\n",
        "            {'id': 'divContent'},\n",
        "            {'id': 'ctl00_Content'},\n",
        "            {'class': 'content1'},\n",
        "            {'class': 'fulltext'},\n",
        "            {'class': 'noidung'},\n",
        "            {'id': 'content'},\n",
        "        ]\n",
        "\n",
        "        for selector in selectors:\n",
        "            content_div = soup.find('div', selector)\n",
        "            if content_div:\n",
        "                content_text = content_div.get_text(separator='\\n', strip=True)\n",
        "                if len(content_text) > 500:  # N·ªôi dung ƒë·ªß d√†i\n",
        "                    break\n",
        "\n",
        "        # C√°ch 2: N·∫øu kh√¥ng t√¨m th·∫•y, t√¨m trong body v√† lo·∫°i b·ªè script, style\n",
        "        if len(content_text) < 500:\n",
        "            # X√≥a c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt\n",
        "            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n",
        "                tag.decompose()\n",
        "\n",
        "            # L·∫•y text t·ª´ body\n",
        "            body = soup.find('body')\n",
        "            if body:\n",
        "                content_text = body.get_text(separator='\\n', strip=True)\n",
        "\n",
        "        # L√†m s·∫°ch text\n",
        "        # Lo·∫°i b·ªè whitespace th·ª´a\n",
        "        content_text = re.sub(r'\\n\\s*\\n', '\\n\\n', content_text)\n",
        "        # Lo·∫°i b·ªè c√°c d√≤ng qu√° ng·∫Øn (th∆∞·ªùng l√† menu, navigation)\n",
        "        lines = content_text.split('\\n')\n",
        "        cleaned_lines = [line for line in lines if len(line.strip()) > 10]\n",
        "        content_text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "        return content_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error crawling {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Crawl vƒÉn b·∫£n\n",
        "document_url = \"https://www.moj.gov.vn/vbpq/lists/vn%20bn%20php%20lut/view_detail.aspx?itemid=23311\"\n",
        "print(f\"üì• Crawling document from: {document_url}\")\n",
        "legal_text = crawl_legal_document(document_url)\n",
        "\n",
        "print(f\"\\n‚úÖ Crawled {len(legal_text):,} characters\")\n",
        "print(f\"üìÑ Number of lines: {len(legal_text.splitlines())}\")\n",
        "print(f\"\\nPreview (first 1000 chars):\\n{legal_text[:1000]}...\")\n",
        "print(f\"\\n...\" + \"=\"*60)\n",
        "print(f\"Preview (middle section):\\n{legal_text[len(legal_text)//2:len(legal_text)//2+500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd1eac54",
      "metadata": {
        "id": "bd1eac54"
      },
      "source": [
        "## 5. Chunking v√† t·∫°o Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "11717e33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11717e33",
        "outputId": "eb95d30c-4b4c-41c2-c256-f0ac36b2d849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created 69 chunks\n",
            "\n",
            "Sample chunk:\n",
            "Turn on more accessible mode\n",
            "Turn off more accessible mode\n",
            "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ B·ªô T∆∞ ph√°p - Ministry of Justice‚Äôs portal\n",
            "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠\n",
            "CSDLQG v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t\n",
            "Ch∆∞∆°ng VIII\n",
            "C·ª•c C√¥ng ngh·ªá th√¥ng tin, B·ªô T∆∞ ph√°p tr√¢n tr·ªçng c·∫£m ∆°n Qu√Ω ƒë·ªôc gi·∫£ trong th·ªùi gian qua ƒë√£ s·ª≠ d·ª•ng h·ªá th·ªëng vƒÉn b·∫£n...\n"
          ]
        }
      ],
      "source": [
        "# T·∫°o Document t·ª´ text crawled\n",
        "document = Document(text=legal_text)\n",
        "\n",
        "# Chunking v·ªõi SentenceSplitter\n",
        "splitter = SentenceSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=50,\n",
        "    separator=\"\\n\"\n",
        ")\n",
        "\n",
        "nodes = splitter.get_nodes_from_documents([document])\n",
        "\n",
        "print(f\"‚úÖ Created {len(nodes)} chunks\")\n",
        "print(f\"\\nSample chunk:\")\n",
        "print(f\"{nodes[0].text[:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12ab644",
      "metadata": {
        "id": "e12ab644"
      },
      "source": [
        "## 6. T·∫°o FAISS Vector Store v√† merge similar chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1b57679a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "8a07b46704c545a395bd4df1c47c9340",
            "54c8726218414328b692e598e06e92b5",
            "21422861f06f432db1c3f55f71c61e2a",
            "56990bc1d9294f2ebc637be0f0cbab4e",
            "b7f0f848bf1a4f938cab7743b14e81b8",
            "06247ec31e654ac5a523dc497314b45b",
            "3fda7fcd2d064c20b90392ab1a27810d",
            "883d5099b7ef47cebe57dee09cf8a662",
            "299e94909bb24f069a1fee27825abb5e",
            "ea57b2f3679f4901bf87af393df3ffd8",
            "4f8a076c9aa848d99214e8a26fb2e41f"
          ]
        },
        "id": "1b57679a",
        "outputId": "140386d4-1465-49d1-d3bf-081c9bb38b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Creating vector index...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/69 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a07b46704c545a395bd4df1c47c9340"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Vector index created with 69 nodes\n"
          ]
        }
      ],
      "source": [
        "# T·∫°o FAISS index\n",
        "dimension = 1536  # text-embedding-3-small dimension\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# T·∫°o vector store\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "\n",
        "# T·∫°o index t·ª´ nodes\n",
        "print(\"üîÑ Creating vector index...\")\n",
        "index = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    vector_store=vector_store,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Vector index created with {len(nodes)} nodes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e7fa9e71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7fa9e71",
        "outputId": "7ab50c2b-6fef-486f-e0c5-6d1829f346a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Merging similar chunks (threshold=0.75)...\n",
            "üìä Input: 69 chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Getting embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 69/69 [00:27<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Merged 69 chunks into 43 contexts\n",
            "üìà Avg chunks per context: 1.6\n"
          ]
        }
      ],
      "source": [
        "def merge_similar_chunks(nodes: List, similarity_threshold: float = 0.75) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Merge c√°c chunks c√≥ cosine similarity cao ƒë·ªÉ t·∫°o contexts phong ph√∫ h∆°n\n",
        "    V·ªõi 182 chunks, threshold 0.75 s·∫Ω t·∫°o ra kho·∫£ng 60-90 contexts\n",
        "    \"\"\"\n",
        "    print(f\"üîÑ Merging similar chunks (threshold={similarity_threshold})...\")\n",
        "    print(f\"üìä Input: {len(nodes)} chunks\")\n",
        "\n",
        "    # Get embeddings cho t·∫•t c·∫£ nodes\n",
        "    embeddings = []\n",
        "    for node in tqdm(nodes, desc=\"Getting embeddings\"):\n",
        "        emb = embed_model.get_text_embedding(node.text)\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    # T√≠nh cosine similarity matrix\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    # Merge chunks\n",
        "    merged_contexts = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(nodes)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "\n",
        "        # T√¨m c√°c chunks similar\n",
        "        similar_indices = np.where(similarity_matrix[i] > similarity_threshold)[0]\n",
        "        similar_indices = [idx for idx in similar_indices if idx not in used_indices]\n",
        "\n",
        "        # Merge text\n",
        "        merged_text = \"\\n\\n\".join([nodes[idx].text for idx in similar_indices])\n",
        "\n",
        "        merged_contexts.append({\n",
        "            'context': merged_text,\n",
        "            'num_chunks': len(similar_indices),\n",
        "            'chunk_indices': similar_indices  # ƒê√£ l√† list r·ªìi, kh√¥ng c·∫ßn .tolist()\n",
        "        })\n",
        "\n",
        "        used_indices.update(similar_indices)\n",
        "\n",
        "    print(f\"‚úÖ Merged {len(nodes)} chunks into {len(merged_contexts)} contexts\")\n",
        "    print(f\"üìà Avg chunks per context: {len(nodes)/len(merged_contexts):.1f}\")\n",
        "    return merged_contexts\n",
        "\n",
        "# Merge chunks v·ªõi threshold th·∫•p h∆°n ƒë·ªÉ t·∫°o nhi·ªÅu contexts h∆°n\n",
        "merged_contexts = merge_similar_chunks(nodes, similarity_threshold=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(merged_contexts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm1c183n6T8e",
        "outputId": "5634ee88-cbbf-4453-9587-aecbb0db6a84"
      },
      "id": "Tm1c183n6T8e",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cad4705a",
      "metadata": {
        "id": "cad4705a"
      },
      "source": [
        "## 7. Generate Questions t·ª´ Contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bc2231b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc2231b3",
        "outputId": "13abbbbb-3af5-4a1b-cea1-849ce1375fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Sample generated questions:\n",
            "1. C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ c·ªßa B·ªô T∆∞ ph√°p c√≥ ch·ª©c nƒÉng g√¨?\n",
            "2. T·∫°i sao C·ª•c C√¥ng ngh·ªá th√¥ng tin l·∫°i quy·∫øt ƒë·ªãnh thay th·∫ø h·ªá th·ªëng c≈© b·∫±ng C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t?\n",
            "3. L√†m th·∫ø n√†o ƒë·ªÉ t√¥i c√≥ th·ªÉ tra c·ª©u vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t t·ª´ Trung ∆∞∆°ng ƒë·∫øn ƒë·ªãa ph∆∞∆°ng?\n",
            "4. C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t c√≥ g√¨ kh√°c bi·ªát so v·ªõi h·ªá th·ªëng c≈©?\n",
            "5. T√¥i c√≥ th·ªÉ g·ª≠i √Ω ki·∫øn g√≥p √Ω v·ªÅ C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia ·ªü ƒë√¢u?\n",
            "6. C√≥ th·ªÉ s·ª≠ d·ª•ng C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t v√†o nh·ªØng m·ª•c ƒë√≠ch n√†o?\n"
          ]
        }
      ],
      "source": [
        "def generate_questions_from_context(context: str, num_questions: int = 6) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate c√¢u h·ªèi t·ª´ context s·ª≠ d·ª•ng GPT-4o-mini\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"B·∫°n l√† chuy√™n gia ph√°p lu·∫≠t Vi·ªát Nam. D·ª±a tr√™n ƒëo·∫°n vƒÉn b·∫£n ph√°p lu·∫≠t d∆∞·ªõi ƒë√¢y, h√£y t·∫°o {num_questions} c√¢u h·ªèi hay v√† ƒëa d·∫°ng m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ h·ªèi.\n",
        "\n",
        "VƒÉn b·∫£n ph√°p lu·∫≠t:\n",
        "{context}\n",
        "\n",
        "Y√™u c·∫ßu:\n",
        "- C√¢u h·ªèi ph·∫£i t·ª± nhi√™n, gi·ªëng nh∆∞ ng∆∞·ªùi d√πng th·∫≠t s·∫Ω h·ªèi\n",
        "- ƒêa d·∫°ng v·ªÅ lo·∫°i c√¢u h·ªèi: h·ªèi ƒë·ªãnh nghƒ©a, ƒëi·ªÅu ki·ªán, th·ªß t·ª•c, quy·ªÅn l·ª£i, tr√°ch nhi·ªám, v.v.\n",
        "- C√¢u h·ªèi ph·∫£i c√≥ th·ªÉ tr·∫£ l·ªùi ƒë∆∞·ª£c t·ª´ context tr√™n\n",
        "- Tr·∫£ v·ªÅ ONLY danh s√°ch c√°c c√¢u h·ªèi, m·ªói c√¢u m·ªôt d√≤ng, kh√¥ng ƒë√°nh s·ªë\n",
        "\n",
        "C√¢u h·ªèi:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.complete(prompt)\n",
        "        questions_text = response.text.strip()\n",
        "\n",
        "        # Parse questions\n",
        "        questions = [q.strip() for q in questions_text.split('\\n') if q.strip()]\n",
        "        # Lo·∫°i b·ªè numbering n·∫øu c√≥\n",
        "        questions = [re.sub(r'^\\d+\\.\\s*', '', q) for q in questions]\n",
        "\n",
        "        return questions[:num_questions]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating questions: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test v·ªõi 1 context\n",
        "test_context = merged_contexts[0]['context']\n",
        "test_questions = generate_questions_from_context(test_context, num_questions=6)\n",
        "\n",
        "print(\"üìù Sample generated questions:\")\n",
        "for i, q in enumerate(test_questions, 1):\n",
        "    print(f\"{i}. {q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9deed35",
      "metadata": {
        "id": "c9deed35"
      },
      "source": [
        "## 8. Generate Answers t·ª´ Questions + Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b4123b3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4123b3b",
        "outputId": "6bdff3c2-d08d-4230-acc2-f8eb9dce3568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üí¨ Sample Q&A:\n",
            "Q: C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ c·ªßa B·ªô T∆∞ ph√°p c√≥ ch·ª©c nƒÉng g√¨?\n",
            "A: C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ c·ªßa B·ªô T∆∞ ph√°p c√≥ ch·ª©c nƒÉng cung c·∫•p v√† tra c·ª©u vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t t·ª´ Trung ∆∞∆°ng ƒë·∫øn ƒë·ªãa ph∆∞∆°ng. Theo th√¥ng tin trong vƒÉn b·∫£n, C·ª•c C√¥ng ngh·ªá th√¥ng tin ƒë√£ ph√°t tri·ªÉn C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ vƒÉn b·∫£n ph√°p lu·∫≠t nh·∫±m ph·ª•c v·ª• nhu c·∫ßu khai th√°c, tra c·ª©u c·ªßa ng∆∞·ªùi d√¢n v√† c√°c t·ªï ch·ª©c. \n",
            "\n",
            "C·ª• th·ªÉ, c·ªïng th√¥ng tin n√†y cho ph√©p ng∆∞·ªùi d√πng t√¨m ki·∫øm c√°c vƒÉn b·∫£n ph√°p lu·∫≠t hi·ªán h√†nh, gi√∫p h·ªç ti·∫øp c·∫≠n th√¥ng tin ph√°p l√Ω m·ªôt c√°ch nhanh ch√≥ng v√† thu·∫≠n l·ª£i. ƒê·ªìng th·ªùi, c·ªïng c√≤n khuy·∫øn kh√≠ch ƒë·ªôc gi·∫£ g·ª≠i √Ω ki·∫øn g√≥p √Ω ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng v√† t√≠nh nƒÉng c·ªßa h·ªá th·ªëng n√†y. \n",
            "\n",
            "Nh∆∞ v·∫≠y, c·ªïng th√¥ng tin kh√¥ng ch·ªâ l√† n∆°i cung c·∫•p th√¥ng tin m√† c√≤n l√† c·∫ßu n·ªëi gi·ªØa c∆° quan nh√† n∆∞·ªõc v√† ng∆∞·ªùi d√¢n trong vi·ªác ti·∫øp c·∫≠n th√¥ng tin ph√°p lu·∫≠t.\n"
          ]
        }
      ],
      "source": [
        "def generate_answer(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate c√¢u tr·∫£ l·ªùi t·ª´ question v√† context s·ª≠ d·ª•ng GPT-4o-mini\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n ph√°p lu·∫≠t Vi·ªát Nam chuy√™n nghi·ªáp. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d∆∞·ªõi ƒë√¢y d·ª±a tr√™n vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p.\n",
        "\n",
        "VƒÉn b·∫£n ph√°p lu·∫≠t:\n",
        "{context}\n",
        "\n",
        "C√¢u h·ªèi: {question}\n",
        "\n",
        "Y√™u c·∫ßu:\n",
        "- Tr·∫£ l·ªùi ch√≠nh x√°c, ƒë·∫ßy ƒë·ªß d·ª±a tr√™n vƒÉn b·∫£n ph√°p lu·∫≠t\n",
        "- Tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t c·ª• th·ªÉ n·∫øu c√≥\n",
        "- Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu\n",
        "- N·∫øu c√≥ nhi·ªÅu tr∆∞·ªùng h·ª£p, li·ªát k√™ ƒë·∫ßy ƒë·ªß\n",
        "- ƒê·ªô d√†i: 50-200 t·ª´\n",
        "\n",
        "C√¢u tr·∫£ l·ªùi:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.complete(prompt)\n",
        "        answer = response.text.strip()\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating answer: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Test v·ªõi 1 question\n",
        "test_answer = generate_answer(test_questions[0], test_context)\n",
        "\n",
        "print(\"üí¨ Sample Q&A:\")\n",
        "print(f\"Q: {test_questions[0]}\")\n",
        "print(f\"A: {test_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12d96075",
      "metadata": {
        "id": "12d96075"
      },
      "source": [
        "## 9. Generate to√†n b·ªô dataset (Target: 1000 Q&A pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "699ee71b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "699ee71b",
        "outputId": "b7d58b16-9b42-4aa8-f98c-0c597213a059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Generating 860 Q&A pairs from 43 contexts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing contexts:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 42/43 [1:09:51<01:39, 99.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Generated 860 Q&A pairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_synthetic_dataset(\n",
        "    contexts: List[Dict],\n",
        "    target_samples: int = 860,\n",
        "    questions_per_context: int = 20\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate to√†n b·ªô synthetic dataset\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "\n",
        "    # T√≠nh s·ªë contexts c·∫ßn d√πng\n",
        "    num_contexts_needed = min(len(contexts), (target_samples // questions_per_context) + 1)\n",
        "\n",
        "    print(f\"üöÄ Generating {target_samples} Q&A pairs from {num_contexts_needed} contexts...\")\n",
        "\n",
        "    for ctx_data in tqdm(contexts[:num_contexts_needed], desc=\"Processing contexts\"):\n",
        "        context = ctx_data['context']\n",
        "\n",
        "        # Skip contexts qu√° ng·∫Øn\n",
        "        if len(context.split()) < 50:\n",
        "            continue\n",
        "\n",
        "        # Generate questions\n",
        "        questions = generate_questions_from_context(context, num_questions=questions_per_context)\n",
        "\n",
        "        # Generate answers cho m·ªói question\n",
        "        for question in questions:\n",
        "            if len(dataset) >= target_samples:\n",
        "                break\n",
        "\n",
        "            answer = generate_answer(question, context)\n",
        "\n",
        "            if answer:  # Ch·ªâ th√™m n·∫øu c√≥ answer\n",
        "                dataset.append({\n",
        "                    'question': question,\n",
        "                    'answer': answer\n",
        "                })\n",
        "\n",
        "        if len(dataset) >= target_samples:\n",
        "            break\n",
        "\n",
        "    print(f\"\\n‚úÖ Generated {len(dataset)} Q&A pairs\")\n",
        "    return dataset\n",
        "\n",
        "# Generate dataset\n",
        "synthetic_data = generate_synthetic_dataset(\n",
        "    merged_contexts,\n",
        "    target_samples=860,\n",
        "    questions_per_context=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be1ef63a",
      "metadata": {
        "id": "be1ef63a"
      },
      "source": [
        "## 10. L∆∞u dataset d∆∞·ªõi d·∫°ng JSONL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b7583e82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b7583e82",
        "outputId": "cc0ce9f1-5826-4944-a25a-0f19131320e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 860 Q&A pairs to synthetic_legal_qa_data.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_daa21e99-635b-4b19-a769-cc8254a8a8aa\", \"synthetic_legal_qa_data.jsonl\", 974114)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# L∆∞u ra file JSONL\n",
        "output_path = 'synthetic_legal_qa_data.jsonl'\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for item in synthetic_data:\n",
        "        json.dump(item, f, ensure_ascii=False)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(f\"‚úÖ Saved {len(synthetic_data)} Q&A pairs to {output_path}\")\n",
        "\n",
        "# Download file (n·∫øu ch·∫°y tr√™n Colab)\n",
        "from google.colab import files\n",
        "files.download(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff2caff0",
      "metadata": {
        "id": "ff2caff0"
      },
      "source": [
        "## 11. Th·ªëng k√™ & Preview dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38de50ae",
      "metadata": {
        "id": "38de50ae"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# T·∫°o DataFrame ƒë·ªÉ ph√¢n t√≠ch\n",
        "df = pd.DataFrame(synthetic_data)\n",
        "\n",
        "# Th·ªëng k√™ ƒë·ªô d√†i\n",
        "df['question_len'] = df['question'].apply(lambda x: len(x.split()))\n",
        "df['answer_len'] = df['answer'].apply(lambda x: len(x.split()))\n",
        "\n",
        "print(\"üìä DATASET STATISTICS:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total samples: {len(df):,}\")\n",
        "print(f\"\\nQuestion length (words):\")\n",
        "print(df['question_len'].describe())\n",
        "print(f\"\\nAnswer length (words):\")\n",
        "print(df['answer_len'].describe())\n",
        "\n",
        "# Preview samples\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAMPLE Q&A PAIRS:\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(min(5, len(df))):\n",
        "    print(f\"\\n[Sample {i+1}]\")\n",
        "    print(f\"Q: {df.iloc[i]['question']}\")\n",
        "    print(f\"A: {df.iloc[i]['answer'][:200]}...\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae0f6379",
      "metadata": {
        "id": "ae0f6379"
      },
      "source": [
        "## 12. Validation & Quality Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "155a4bfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "155a4bfa",
        "outputId": "3c397948-e4c0-4b30-afd1-85d4ba6b5f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç QUALITY CHECKS:\n",
            "============================================================\n",
            "Duplicate questions: 0\n",
            "Empty answers: 0\n",
            "Very short answers (< 10 words): 0\n",
            "Very long answers (> 300 words): 1\n",
            "\n",
            "‚úÖ Quality check completed!\n"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra ch·∫•t l∆∞·ª£ng\n",
        "print(\"üîç QUALITY CHECKS:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check duplicates\n",
        "duplicate_questions = df[df.duplicated(subset=['question'], keep=False)]\n",
        "print(f\"Duplicate questions: {len(duplicate_questions)}\")\n",
        "\n",
        "# Check empty\n",
        "empty_answers = df[df['answer'].str.strip() == '']\n",
        "print(f\"Empty answers: {len(empty_answers)}\")\n",
        "\n",
        "# Check very short answers (< 10 words)\n",
        "short_answers = df[df['answer_len'] < 10]\n",
        "print(f\"Very short answers (< 10 words): {len(short_answers)}\")\n",
        "\n",
        "# Check very long answers (> 300 words)\n",
        "long_answers = df[df['answer_len'] > 300]\n",
        "print(f\"Very long answers (> 300 words): {len(long_answers)}\")\n",
        "\n",
        "print(\"\\n‚úÖ Quality check completed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a07b46704c545a395bd4df1c47c9340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54c8726218414328b692e598e06e92b5",
              "IPY_MODEL_21422861f06f432db1c3f55f71c61e2a",
              "IPY_MODEL_56990bc1d9294f2ebc637be0f0cbab4e"
            ],
            "layout": "IPY_MODEL_b7f0f848bf1a4f938cab7743b14e81b8"
          }
        },
        "54c8726218414328b692e598e06e92b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06247ec31e654ac5a523dc497314b45b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3fda7fcd2d064c20b90392ab1a27810d",
            "value": "Generating‚Äáembeddings:‚Äá100%"
          }
        },
        "21422861f06f432db1c3f55f71c61e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_883d5099b7ef47cebe57dee09cf8a662",
            "max": 69,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_299e94909bb24f069a1fee27825abb5e",
            "value": 69
          }
        },
        "56990bc1d9294f2ebc637be0f0cbab4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea57b2f3679f4901bf87af393df3ffd8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4f8a076c9aa848d99214e8a26fb2e41f",
            "value": "‚Äá69/69‚Äá[00:03&lt;00:00,‚Äá22.72it/s]"
          }
        },
        "b7f0f848bf1a4f938cab7743b14e81b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06247ec31e654ac5a523dc497314b45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fda7fcd2d064c20b90392ab1a27810d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "883d5099b7ef47cebe57dee09cf8a662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "299e94909bb24f069a1fee27825abb5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea57b2f3679f4901bf87af393df3ffd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8a076c9aa848d99214e8a26fb2e41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}