# ğŸš€ Vietnamese Legal AI - Fine-tuning Project

Fine-tuning Llama 3.2 3B model for Vietnamese legal question answering, specifically focusing on traffic law domain.

## ğŸ“‹ Project Structure

```
Finetune_advance/
â”œâ”€â”€ data_pipeline/          # Data processing and preparation
â”‚   â”œâ”€â”€ data/              # Training datasets
â”‚   â”‚   â”œâ”€â”€ finetune_data/
â”‚   â”‚   â”œâ”€â”€ finetune_data2/
â”‚   â”‚   â”œâ”€â”€ finetune_data3/
â”‚   â”‚   â””â”€â”€ finetune_llm/  # Main training data
â”‚   â””â”€â”€ utils/             # Data processing notebooks
â”œâ”€â”€ downloaded_model/       # Pre-downloaded model files
â”œâ”€â”€ finetune/              # Fine-tuning scripts
â”‚   â”œâ”€â”€ sft_vietnamese_legal_unsloth.ipynb  # Main training notebook
â”‚   â””â”€â”€ REINFORCEMENT_LEARNING_GUIDE.md
â””â”€â”€ download_and_upload_model.py
```

## ğŸ¯ Features

- **Model**: Llama 3.2 3B Instruct (optimized with Unsloth)
- **Task**: Vietnamese Legal Question Answering
- **Domain**: Traffic Law
- **Technique**: Supervised Fine-Tuning (SFT) with LoRA
- **Optimization**: 4-bit quantization, memory-efficient training

## ğŸ› ï¸ Tech Stack

- **Framework**: Unsloth (2x faster training)
- **Model**: Meta Llama 3.2 3B Instruct
- **Training**: LoRA (Low-Rank Adaptation)
- **Monitoring**: Weights & Biases (WandB)
- **Hardware**: Optimized for Kaggle T4 GPU (16GB VRAM)

## ğŸ“Š Dataset

- **Format**: JSONL with instruction-input-output structure
- **Language**: Vietnamese
- **Domain**: Traffic law questions and answers
- **Split**: 90% train, 5% validation, 5% test

## ğŸš€ Quick Start

### 1. Installation

```bash
pip install unsloth transformers datasets trl wandb
```

### 2. Training

Open and run `finetune/sft_vietnamese_legal_unsloth.ipynb` in Kaggle or local Jupyter environment.

### 3. Key Configuration

```python
model_name = "unsloth/Llama-3.2-3B-Instruct-bnb-4bit"
max_seq_length = 1536
lora_r = 32
learning_rate = 2e-4
num_epochs = 3
```

## ğŸ“ˆ Training Details

- **Max Sequence Length**: 1536 tokens
- **Batch Size**: 8 per device
- **Gradient Accumulation**: 4 steps (effective batch size: 32)
- **Learning Rate**: 2e-4 with cosine annealing
- **Optimizer**: AdamW 8-bit
- **Training Time**: ~3-4 hours on T4 GPU

## ğŸ“ LoRA Configuration

```python
r = 32                    # LoRA rank
lora_alpha = 32          # Scaling factor
target_modules = [       # Train all attention & MLP layers
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
```

## ğŸ’¾ Outputs

- **LoRA Adapters**: ~100-200MB (lightweight)
- **GGUF Models**: For Ollama/llama.cpp deployment
- **Checkpoints**: Saved every 50 steps

## ğŸ“Š Monitoring

Training metrics are logged to Weights & Biases:
- Training/Validation loss
- Learning rate schedule
- GPU memory usage
- Training time per epoch

## ğŸ§ª Inference

```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="vietnamese_legal_lora",
    max_seq_length=1536,
)

FastLanguageModel.for_inference(model)

# Generate response
response = model.generate(prompt, max_new_tokens=512)
```

## ğŸ“ Data Format

```json
{
  "instruction": "HÃ£y tráº£ lá»i cÃ¢u há»i vá» luáº­t giao thÃ´ng sau:",
  "input": "Pháº¡t bao nhiÃªu khi khÃ´ng Ä‘á»™i mÅ© báº£o hiá»ƒm?",
  "output": "Theo Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP..."
}
```

## ğŸ¯ Use Cases

- Legal consultation chatbot
- Traffic law Q&A system
- Legal document assistant
- Educational tool for traffic regulations

## ğŸ”§ Requirements

- Python 3.8+
- CUDA-capable GPU (16GB+ VRAM recommended)
- 20GB+ disk space

## ğŸ“š Resources

- [Unsloth Documentation](https://docs.unsloth.ai)
- [Llama 3.2 Model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
- [Training Notebook](finetune/sft_vietnamese_legal_unsloth.ipynb)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

## ğŸ“„ License

This project is for educational and research purposes.

## ğŸ™ Acknowledgments

- Meta AI for Llama 3.2
- Unsloth team for optimization framework
- Vietnamese legal dataset contributors

---

**Note**: This project is optimized for Kaggle T4 GPUs with 30h/week limit. Adjust batch sizes and gradient accumulation for different hardware configurations.
