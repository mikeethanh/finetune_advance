{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a87e107",
   "metadata": {},
   "source": [
    "## üìã Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcaed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "!pip install wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43176172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset, load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b37ed",
   "metadata": {},
   "source": [
    "## üîê WandB Login (for monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a704149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to WandB for experiment tracking\n",
    "# Get WandB API key from Kaggle Secrets\n",
    "# In Kaggle: Add-ons ‚Üí Secrets ‚Üí Add new secret with key \"WANDB_API_KEY\"\n",
    "# Get your API key from: https://wandb.ai/authorize\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login with API key from Kaggle Secrets\n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "# Initialize WandB project with detailed config\n",
    "wandb.init(\n",
    "    project=\"vietnamese-legal-ai\",\n",
    "    name=\"llama3.2-3b-traffic-law-v1\",\n",
    "    config={\n",
    "        \"model\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        \"dataset\": \"traffic_law_data.jsonl\",\n",
    "        \"task\": \"legal_qa\",\n",
    "        \"language\": \"vietnamese\",\n",
    "        \"max_seq_length\": 1536,\n",
    "        \"lora_r\": 32,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation\": 8,\n",
    "        \"effective_batch_size\": 16,\n",
    "    },\n",
    "    settings=wandb.Settings(\n",
    "        _disable_meta=False,\n",
    "        _disable_stats=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ WandB initialized with detailed logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0102a",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Model Configuration\n",
    "\n",
    "### T·∫°i sao ch·ªçn Llama-3.2-3B-Instruct?\n",
    "- ‚úÖ **3B parameters**: V·ª´a ƒë·ªß m·∫°nh, v·ª´a ti·∫øt ki·ªám GPU\n",
    "- ‚úÖ **Multilingual support**: H·ªó tr·ª£ nhi·ªÅu ng√¥n ng·ªØ bao g·ªìm ti·∫øng Vi·ªát\n",
    "- ‚úÖ **Instruct version**: ƒê√£ ƒë∆∞·ª£c train theo instruction format\n",
    "- ‚úÖ **Fit Kaggle T4**: ~15GB VRAM v·ªõi 4-bit quantization\n",
    "- ‚úÖ **Unsloth optimized**: H·ªó tr·ª£ t·ªët, train nhanh 2x\n",
    "- ‚úÖ **Meta's latest**: Phi√™n b·∫£n m·ªõi nh·∫•t t·ª´ Meta (2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration for Kaggle T4 (16GB VRAM)\n",
    "max_seq_length = 1536  # Based on data analysis (covers 95% of samples)\n",
    "dtype = None  # Auto-detect. Use Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n",
    "\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # Meta Llama 3.2 - Pre-quantized by Unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"üìè Max sequence length: {max_seq_length}\")\n",
    "print(f\"üî¢ 4-bit quantization: {load_in_4bit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c71974",
   "metadata": {},
   "source": [
    "## üéØ LoRA Configuration\n",
    "\n",
    "### LoRA Parameters Explained:\n",
    "- **r (rank)**: 16-32 cho balance quality/speed. Higher = better but slower\n",
    "- **lora_alpha**: Scaling factor, th∆∞·ªùng = r ho·∫∑c 2*r\n",
    "- **target_modules**: Train all attention & MLP layers cho best result\n",
    "- **lora_dropout**: 0 cho faster training (Unsloth optimized)\n",
    "- **bias**: \"none\" cho faster & less overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  # LoRA rank - higher = more expressive but slower (16, 32, 64)\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],  # All attention & MLP layers\n",
    "    lora_alpha=32,  # LoRA scaling (usually = r or 2*r)\n",
    "    lora_dropout=0,  # 0 is optimized by Unsloth\n",
    "    bias=\"none\",  # \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's long context support\n",
    "    random_state=3407,  # For reproducibility\n",
    "    use_rslora=False,  # Rank stabilized LoRA\n",
    "    loftq_config=None,  # LoftQ quantization\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters applied\")\n",
    "print(f\"üìä Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"üí° Trainable ratio: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5744f0e",
   "metadata": {},
   "source": [
    "## üìä Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7838d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Kaggle input (adjust path if uploading to Kaggle)\n",
    "# For local testing, adjust the path\n",
    "data_path = \"/kaggle/input/traffic-law-data/traffic_law_data.jsonl\"  # Kaggle path\n",
    "# data_path = \"../data/finetune_llm/traffic_law_data.jsonl\"  # Local path\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"‚ö†Ô∏è Data file not found at {data_path}\")\n",
    "    print(\"For Kaggle: Upload dataset or adjust path\")\n",
    "    print(\"For local: Make sure you're in the correct directory\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found data at: {data_path}\")\n",
    "\n",
    "# Load JSONL data\n",
    "data = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"üìä Total samples: {len(data):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìù Sample data:\")\n",
    "sample = data[0]\n",
    "for key, value in sample.items():\n",
    "    if key == 'output':\n",
    "        print(f\"{key}: {value[:200]}...\")  # Truncate long output\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d877a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 90% train, 5% validation, 5% test\n",
    "train_data, temp_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"üìä Train: {len(train_data):,} samples\")\n",
    "print(f\"üìä Validation: {len(val_data):,} samples\")\n",
    "print(f\"üìä Test: {len(test_data):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6982a6",
   "metadata": {},
   "source": [
    "## üìù Prompt Template\n",
    "\n",
    "S·ª≠ d·ª•ng format chu·∫©n Alpaca v·ªõi Vietnamese context cho Llama 3.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for Vietnamese legal QA\n",
    "alpaca_prompt = \"\"\"D∆∞·ªõi ƒë√¢y l√† m·ªôt h∆∞·ªõng d·∫´n m√¥ t·∫£ m·ªôt nhi·ªám v·ª•, ƒë∆∞·ª£c gh√©p n·ªëi v·ªõi m·ªôt ƒë·∫ßu v√†o cung c·∫•p th√™m ng·ªØ c·∫£nh. H√£y vi·∫øt m·ªôt ph·∫£n h·ªìi ho√†n th√†nh ƒë·∫ßy ƒë·ªß y√™u c·∫ßu.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS token for proper generation\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format data into prompt template\"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(\"‚úÖ Data formatted with prompt template\")\n",
    "print(\"\\nüìù Example formatted prompt:\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba40d90",
   "metadata": {},
   "source": [
    "## üéì Training Configuration\n",
    "\n",
    "### Optimized for Kaggle T4 (30h/week limit):\n",
    "- **Epochs**: 3 (sufficient for legal domain)\n",
    "- **Batch size**: 2 per device (optimized for ~13GB VRAM usage)\n",
    "- **Gradient accumulation**: 8 steps (effective batch = 16)\n",
    "- **Learning rate**: 2e-4 (standard for LoRA)\n",
    "- **Warmup**: 10% of steps\n",
    "- **FP16**: Enabled for speed\n",
    "- **Gradient checkpointing**: Unsloth optimized\n",
    "\n",
    "- **Logging**: Every 5 steps for detailed monitoring- **Evaluation**: Every 50 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e73242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for Kaggle T4\n",
    "training_args = TrainingArguments(\n",
    "    # Output & Logging\n",
    "    output_dir=\"./outputs\",\n",
    "    run_name=\"llama3.2-3b-traffic-law-v1\",\n",
    "    \n",
    "    # Training dynamics\n",
    "    num_train_epochs=3,  # 2-3 epochs is usually enough\n",
    "    per_device_train_batch_size=8,  # Reduced from 4 to better utilize VRAM\n",
    "    gradient_accumulation_steps=4,  # Increased to maintain effective batch size = 16\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_8bit\",  # 8-bit AdamW for memory efficiency\n",
    "    learning_rate=2e-4,  # Standard for LoRA fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,  # 10% warmup\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine annealing\n",
    "    \n",
    "    # Performance\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # Use FP16 for T4\n",
    "    bf16=torch.cuda.is_bf16_supported(),  # Use BF16 if supported (A100, H100)\n",
    "    \n",
    "    # Logging & Saving (more frequent for better monitoring)\n",
    "    logging_steps=5,  # Log every 5 steps for better visibility\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,  # Log first step\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Save more frequently\n",
    "    save_total_limit=3,  # Keep only 3 best checkpoints\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,  # Evaluate more frequently\n",
    "    eval_accumulation_steps=1,  # Accumulate eval predictions\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # WandB integration with detailed logging\n",
    "    report_to=\"wandb\",\n",
    "    logging_nan_inf_filter=True,  # Filter out NaN/Inf values\n",
    "    include_inputs_for_metrics=False,  # Don't log inputs (save space)\n",
    "    \n",
    "    # Progress bar and output control\n",
    "    disable_tqdm=False,  # Enable progress bar\n",
    "    log_level=\"info\",  # Show info messages\n",
    "    log_level_replica=\"warning\",\n",
    "    log_on_each_node=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"üíæ Per device batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"üìä Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"üìà Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"üìä Logging every {training_args.logging_steps} steps\")\n",
    "print(f\"üìä Evaluating every {training_args.eval_steps} steps\")\n",
    "print(f\"üïê Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ad7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with callbacks for better logging\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to ensure metrics are logged\"\"\"\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            # Print metrics to console\n",
    "            step = state.global_step\n",
    "            if \"loss\" in logs:\n",
    "                print(f\"Step {step} | Loss: {logs['loss']:.4f}\", end=\"\")\n",
    "            if \"eval_loss\" in logs:\n",
    "                print(f\" | Eval Loss: {logs['eval_loss']:.4f}\", end=\"\")\n",
    "            if \"learning_rate\" in logs:\n",
    "                print(f\" | LR: {logs['learning_rate']:.2e}\", end=\"\")\n",
    "            print()  # New line\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,  # Can make training 5x faster for short sequences\n",
    "    dataset_num_proc=2,\n",
    "    callbacks=[MetricsCallback()],  # Add custom callback\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized with metrics logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed48c48",
   "metadata": {},
   "source": [
    "## üöÄ Start Training!\n",
    "\n",
    "**Estimated time on T4**: ~3-4 hours for 3 epochs  \n",
    "**Memory usage**: ~14-15GB VRAM  \n",
    "**Kaggle time budget**: ~4h / 30h week (leaves 26h for experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1deacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show GPU stats before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"üñ•Ô∏è GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"üíæ {start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show final stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚è±Ô∏è Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"üíæ Peak reserved memory: {used_memory} GB\")\n",
    "print(f\"üìä Memory used for training: {used_memory_for_lora} GB\")\n",
    "print(f\"üìà Percentage of max memory: {used_percentage}%\")\n",
    "print(f\"üéØ Final train loss: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86713df2",
   "metadata": {},
   "source": [
    "## üìä Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ff1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"üìä Evaluating on validation set...\\n\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
    "\n",
    "# Log to WandB\n",
    "wandb.log({\"final_eval_loss\": eval_results['eval_loss']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b1fee",
   "metadata": {},
   "source": [
    "## üß™ Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def test_model(instruction, input_text, max_new_tokens=512):\n",
    "    \"\"\"Test model with a prompt\"\"\"\n",
    "    prompt = alpaca_prompt.format(instruction, input_text, \"\")\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    # Extract only the response part\n",
    "    response = response.split(\"### Response:\")[1].split(EOS_TOKEN)[0].strip()\n",
    "    return response\n",
    "\n",
    "# Test with samples from test set\n",
    "print(\"üß™ Testing model on random samples...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import random\n",
    "test_samples = random.sample(test_data, 3)\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST SAMPLE #{i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nüìù Instruction: {sample['instruction']}\")\n",
    "    print(f\"\\n‚ùì Input: {sample['input']}\")\n",
    "    print(f\"\\nüéØ Expected Output:\\n{sample['output'][:300]}...\")\n",
    "    \n",
    "    # Generate response\n",
    "    response = test_model(sample['instruction'], sample['input'])\n",
    "    print(f\"\\nü§ñ Model Response:\\n{response}\")\n",
    "    print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2678b9",
   "metadata": {},
   "source": [
    "## üíæ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters (only ~100-200MB!)\n",
    "model.save_pretrained(\"vietnamese_legal_lora\")\n",
    "tokenizer.save_pretrained(\"vietnamese_legal_lora\")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters saved to: vietnamese_legal_lora/\")\n",
    "print(\"üì¶ Size: ~100-200MB (adapters only)\")\n",
    "\n",
    "# Optional: Save merged model (full size ~6GB)\n",
    "# model.save_pretrained_merged(\"vietnamese_legal_merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "# print(\"‚úÖ Merged model saved to: vietnamese_legal_merged/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10556004",
   "metadata": {},
   "source": [
    "## üì§ Push to HuggingFace Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6cb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to push to HuggingFace Hub\n",
    "# You need to login first: huggingface-cli login\n",
    "\n",
    "# model.push_to_hub(\n",
    "#     \"your-username/vietnamese-legal-llama3.2-3b-lora\",\n",
    "#     token=\"your_hf_token\",\n",
    "#     commit_message=\"Vietnamese Legal AI - Traffic Law QA\"\n",
    "# )\n",
    "# tokenizer.push_to_hub(\n",
    "#     \"your-username/vietnamese-legal-llama3.2-3b-lora\",\n",
    "#     token=\"your_hf_token\"\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Model pushed to HuggingFace Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8e8a4",
   "metadata": {},
   "source": [
    "## üìä Quantization Export (for deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF for llama.cpp / Ollama deployment\n",
    "# Uncomment the quantization method you want\n",
    "\n",
    "quantization_methods = [\n",
    "    \"q8_0\",    # Fast inference, good quality (recommended)\n",
    "    # \"q4_k_m\",  # Smaller size, still good quality\n",
    "    # \"q5_k_m\",  # Balance between size and quality\n",
    "]\n",
    "\n",
    "for method in quantization_methods:\n",
    "    print(f\"\\nüì¶ Exporting to {method.upper()}...\")\n",
    "    model.save_pretrained_gguf(\n",
    "        \"vietnamese_legal_model\",\n",
    "        tokenizer,\n",
    "        quantization_method=method,\n",
    "    )\n",
    "    print(f\"‚úÖ Exported: vietnamese_legal_model-{method.upper()}.gguf\")\n",
    "\n",
    "print(\"\\n‚úÖ All quantization exports completed!\")\n",
    "print(\"üìù You can now use these with Ollama or llama.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed1db1",
   "metadata": {},
   "source": [
    "## üéâ Finish & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f96c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish WandB run\n",
    "wandb.finish()\n",
    "\n",
    "# Clear GPU memory\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Training completed successfully!\")\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"  - Model: Llama-3.2-3B-Instruct\")\n",
    "print(f\"  - Training samples: {len(train_data):,}\")\n",
    "print(f\"  - Validation samples: {len(val_data):,}\")\n",
    "print(f\"  - Test samples: {len(test_data):,}\")\n",
    "print(f\"  - Training time: ~{trainer_stats.metrics['train_runtime']/3600:.2f} hours\")\n",
    "print(f\"  - Final eval loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(\"\\nüìÇ Saved outputs:\")\n",
    "print(\"  - LoRA adapters: vietnamese_legal_lora/\")\n",
    "print(\"  - GGUF models: vietnamese_legal_model-*.gguf\")\n",
    "print(\"\\nüéØ Next steps:\")\n",
    "print(\"  1. Test model on more samples\")\n",
    "print(\"  2. Deploy with Ollama or llama.cpp\")\n",
    "print(\"  3. Collect feedback and iterate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72173f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö References & Resources\n",
    "\n",
    "- **Unsloth**: https://github.com/unslothai/unsloth\n",
    "- **Unsloth Docs**: https://docs.unsloth.ai\n",
    "- **WandB**: https://wandb.ai\n",
    "- **Llama 3.2**: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "\n",
    "## üí° Tips for Better Results\n",
    "\n",
    "1. **More data**: Collect more Vietnamese legal Q&A pairs\n",
    "2. **Data quality**: Clean and verify answers\n",
    "3. **Hyperparameter tuning**: Try different learning rates (1e-4, 5e-5)\n",
    "4. **Longer training**: Try 4-5 epochs if not overfitting\n",
    "5. **Larger model**: Try Llama-3.2-11B if you have more GPU\n",
    "6. **Domain adaptation**: Continue pretraining on legal documents first\n",
    "\n",
    "## üêõ Troubleshooting\n",
    "\n",
    "- **OOM (Out of Memory)**: Reduce batch size or max_seq_length\n",
    "- **Slow training**: Enable packing=True for short sequences\n",
    "- **Poor results**: Increase LoRA rank or training epochs\n",
    "- **Overfitting**: Reduce epochs or add more data augmentation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
