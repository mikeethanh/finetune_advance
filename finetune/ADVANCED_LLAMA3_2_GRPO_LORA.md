# Advanced Llama3.2 (3B) GRPO LoRA - T√†i li·ªáu K·ªπ thu·∫≠t

## üìã T·ªïng quan

File `Advanced_Llama3_2_(3B)_GRPO_LoRA.ipynb` th·ª±c hi·ªán **Group Relative Policy Optimization (GRPO)** cho model Vietnamese Legal Llama3.2-3B ƒë√£ qua SFT, nh·∫±m c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi th√¥ng qua reward-based learning.

### üéØ M·ª•c ti√™u 
- **Reinforcement Learning**: S·ª≠ d·ª•ng GRPO ƒë·ªÉ t·ªëi ∆∞u ch·∫•t l∆∞·ª£ng response
- **Structured Reasoning**: Training model output format `<start_working_out>` v√† `<SOLUTION>`

## üß† GRPO Framework

### Kh√°i ni·ªám GRPO
**Group Relative Policy Optimization** l√† m·ªôt variant c·ªßa PPO ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ:
- **Group-based Comparison**: So s√°nh responses trong c√πng m·ªôt group
- **Relative Scoring**: ƒê√°nh gi√° t∆∞∆°ng ƒë·ªëi thay v√¨ absolute scoring  
- **Stability**: ·ªîn ƒë·ªãnh h∆°n PPO truy·ªÅn th·ªëng cho conversational AI
- **Efficiency**: √çt computational overhead h∆°n

### Structured Response Format
Model ƒë∆∞·ª£c train ƒë·ªÉ output theo format:
```
<start_working_out>
[Ph·∫ßn ph√¢n t√≠ch v√† suy nghƒ© c·ªßa AI]
<end_working_out>

<SOLUTION>
[C√¢u tr·∫£ l·ªùi cu·ªëi c√πng]
</SOLUTION>
```

## üîß C·∫•u h√¨nh K·ªπ thu·∫≠t

### Model Configuration
```python
max_seq_length = 1536         # Optimized cho T4 15GB
lora_rank = 32               # Balance quality/memory
load_in_4bit = False         # Disable cho GRPO training
fast_inference = True        # Enable vLLM backend
gpu_memory_utilization = 0.85 # Conservative cho T4
```

### GRPO Specific Settings  
```python
reasoning_start = "<start_working_out>"
reasoning_end = "<end_working_out>"
solution_start = "<SOLUTION>"
solution_end = "</SOLUTION>"

system_prompt = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. 
Khi tr·∫£ l·ªùi c√¢u h·ªèi, h√£y:
1. Suy nghƒ© v√† ph√¢n t√≠ch c√¢u h·ªèi trong ph·∫ßn <start_working_out> <end_working_out>
2. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c trong ph·∫ßn <SOLUTION></SOLUTION>"""
```

### LoRA Configuration
```python
r = 32                       # LoRA rank cho GRPO
target_modules = [           # Full attention + MLP
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
lora_alpha = 32             # Scaling factor
use_gradient_checkpointing = "unsloth"  # Memory optimization
```

