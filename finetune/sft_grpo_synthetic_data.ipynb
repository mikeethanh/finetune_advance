{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71fe92e",
   "metadata": {},
   "source": [
    "## üìã Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset, load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d854241",
   "metadata": {},
   "source": [
    "## üîê WandB Login (for monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to WandB for experiment tracking\n",
    "from google.colab import userdata\n",
    "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "\n",
    "# Login with API key from Colab Secrets\n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "# Initialize WandB project with GRPO synthetic data config\n",
    "wandb.init(\n",
    "    project=\"vietnamese-legal-ai-grpo\",\n",
    "    name=\"llama3.2-3b-grpo-synthetic-sft-v1\",\n",
    "    config={\n",
    "        \"base_model\": \"mikeethanh/vietnamese-legal-llama3.2-3b-merged-grpo\",\n",
    "        \"dataset\": \"synthetic_legal_qa_grpo_training.jsonl\",\n",
    "        \"task\": \"structured_legal_qa\",\n",
    "        \"language\": \"vietnamese\",\n",
    "        \"format\": \"grpo_structured\",\n",
    "        \"max_seq_length\": 2048,  # Increased for structured format\n",
    "        \"lora_r\": 16,  # Reduced since base model already fine-tuned\n",
    "        \"lora_alpha\": 16,\n",
    "        \"learning_rate\": 1e-4,  # Lower LR for already fine-tuned model\n",
    "        \"num_epochs\": 1,  # Less epochs needed\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation\": 8,\n",
    "        \"effective_batch_size\": 16,\n",
    "    },\n",
    "    settings=wandb.Settings(\n",
    "        _disable_meta=False,\n",
    "        _disable_stats=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ WandB initialized for GRPO synthetic data training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914eb73",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Model Configuration - GRPO Merged Model\n",
    "\n",
    "### T·∫°i sao s·ª≠ d·ª•ng GRPO merged model?\n",
    "- ‚úÖ **Already GRPO trained**: Model ƒë√£ ƒë∆∞·ª£c train v·ªõi GRPO format\n",
    "- ‚úÖ **Structured reasoning**: ƒê√£ bi·∫øt format `<start_working_out>` v√† `<SOLUTION>`\n",
    "- ‚úÖ **Domain adapted**: ƒê√£ fine-tune tr√™n legal domain\n",
    "- ‚úÖ **Consistent format**: S·∫Ω d·ªÖ d√†ng h·ªçc synthetic data c√πng format\n",
    "- ‚úÖ **Less training needed**: Ch·ªâ c·∫ßn √≠t epochs ƒë·ªÉ adapt v·ªõi synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration for GRPO merged model\n",
    "max_seq_length = 2048  # Increased for structured format with reasoning\n",
    "dtype = None  # Auto-detect. Use Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n",
    "\n",
    "# Load the GRPO merged model\n",
    "model_name = \"mikeethanh/vietnamese-legal-llama3.2-3b-merged-grpo\"  # Your GRPO merged model\n",
    "\n",
    "print(f\"üîÑ Loading GRPO merged model: {model_name}\")\n",
    "print(\"‚ö†Ô∏è This model already contains GRPO training adaptations\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ GRPO merged model loaded: {model_name}\")\n",
    "print(f\"üìè Max sequence length: {max_seq_length}\")\n",
    "print(f\"üî¢ 4-bit quantization: {load_in_4bit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202967d3",
   "metadata": {},
   "source": [
    "## üéØ LoRA Configuration - Lighter for Already Fine-tuned Model\n",
    "\n",
    "### LoRA Parameters cho model ƒë√£ fine-tune:\n",
    "- **r (rank)**: 8-16 thay v√¨ 32 (model ƒë√£ c√≥ knowledge)\n",
    "- **lora_alpha**: T∆∞∆°ng ·ª©ng v·ªõi r\n",
    "- **learning_rate**: Th·∫•p h∆°n (5e-5 ƒë·∫øn 1e-4)\n",
    "- **epochs**: 1-2 epochs thay v√¨ 3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cdfd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA adapters v·ªõi settings nh·∫π h∆°n cho model ƒë√£ fine-tune\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Reduced from 32 since model is already fine-tuned\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],  # All attention & MLP layers\n",
    "    lora_alpha=16,  # Equal to r\n",
    "    lora_dropout=0,  # 0 is optimized by Unsloth\n",
    "    bias=\"none\",  # \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's long context support\n",
    "    random_state=3407,  # For reproducibility\n",
    "    use_rslora=False,  # Rank stabilized LoRA\n",
    "    loftq_config=None,  # LoftQ quantization\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters applied (lighter config for pre-trained model)\")\n",
    "print(f\"üìä Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"üí° Trainable ratio: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09db0d2",
   "metadata": {},
   "source": [
    "## üìä GRPO Synthetic Data Preparation\n",
    "\n",
    "### Expected Data Format:\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"System prompt v·ªõi GRPO format\"},\n",
    "    {\"role\": \"user\", \"content\": \"C√¢u h·ªèi ph√°p lu·∫≠t\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"<start_working_out>\\nSuy nghƒ©...\\n<end_working_out>\\n\\n<SOLUTION>C√¢u tr·∫£ l·ªùi</SOLUTION>\"}\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GRPO synthetic data\n",
    "# Update path to your synthetic data file\n",
    "data_path = \"synthetic_legal_qa_grpo_training.jsonl\"  # From your synthetic data generation\n",
    "\n",
    "# For Colab, upload the file first or use Google Drive\n",
    "# For Kaggle, add it as dataset\n",
    "\n",
    "print(f\"üîç Looking for GRPO synthetic data at: {data_path}\")\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"‚ö†Ô∏è Data file not found at {data_path}\")\n",
    "    print(\"üì• Please upload the 'synthetic_legal_qa_grpo_training.jsonl' file\")\n",
    "    print(\"üîß Or update the path in the code above\")\n",
    "    \n",
    "    # Alternative: Try different common paths\n",
    "    alternative_paths = [\n",
    "        \"/content/synthetic_legal_qa_grpo_training.jsonl\",  # Colab\n",
    "        \"/kaggle/input/grpo-synthetic-data/synthetic_legal_qa_grpo_training.jsonl\",  # Kaggle\n",
    "        \"../data_pipeline/utils/synthetic_legal_qa_grpo_training.jsonl\",  # Local\n",
    "    ]\n",
    "    \n",
    "    for alt_path in alternative_paths:\n",
    "        if os.path.exists(alt_path):\n",
    "            data_path = alt_path\n",
    "            print(f\"‚úÖ Found data at alternative path: {data_path}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"‚ùå Please ensure the GRPO synthetic data file is available\")\n",
    "        # Create dummy data for demonstration\n",
    "        dummy_data = [{\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. Khi tr·∫£ l·ªùi c√¢u h·ªèi, h√£y: 1. Suy nghƒ© v√† ph√¢n t√≠ch c√¢u h·ªèi trong ph·∫ßn <start_working_out> <end_working_out> 2. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c trong ph·∫ßn <SOLUTION></SOLUTION>\"},\n",
    "                {\"role\": \"user\", \"content\": \"M·ª©c ph·∫°t cho vi·ªác v∆∞·ª£t ƒë√®n ƒë·ªè l√† bao nhi√™u?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"<start_working_out>\\nƒê·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ m·ª©c ph·∫°t v∆∞·ª£t ƒë√®n ƒë·ªè, t√¥i c·∫ßn tham kh·∫£o Ngh·ªã ƒë·ªãnh 100/2019/Nƒê-CP v·ªÅ x·ª≠ ph·∫°t vi ph·∫°m h√†nh ch√≠nh trong lƒ©nh v·ª±c giao th√¥ng ƒë∆∞·ªùng b·ªô.\\n<end_working_out>\\n\\n<SOLUTION>Theo Ngh·ªã ƒë·ªãnh 100/2019/Nƒê-CP, m·ª©c ph·∫°t cho h√†nh vi v∆∞·ª£t ƒë√®n ƒë·ªè ƒë·ªëi v·ªõi √¥ t√¥ l√† t·ª´ 4.000.000 - 6.000.000 ƒë·ªìng, ƒë·ªëi v·ªõi xe m√°y l√† t·ª´ 800.000 - 1.000.000 ƒë·ªìng.</SOLUTION>\"}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        with open(\"dummy_grpo_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in dummy_data:\n",
    "                json.dump(item, f, ensure_ascii=False)\n",
    "                f.write(\"\\n\")\n",
    "        data_path = \"dummy_grpo_data.jsonl\"\n",
    "        print(\"üìù Created dummy data for demonstration\")\n",
    "\n",
    "# Load JSONL data\n",
    "data = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(data):,} GRPO training samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìù Sample GRPO data structure:\")\n",
    "sample = data[0]\n",
    "print(f\"Keys: {list(sample.keys())}\")\n",
    "if \"messages\" in sample:\n",
    "    print(f\"\\nMessages structure:\")\n",
    "    for i, msg in enumerate(sample[\"messages\"]):\n",
    "        content_preview = msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"]\n",
    "        print(f\"  {i+1}. {msg['role']}: {content_preview}\")\n",
    "        \n",
    "# Validate GRPO format\n",
    "grpo_format_count = 0\n",
    "for item in data[:100]:  # Check first 100 samples\n",
    "    if \"messages\" in item:\n",
    "        for msg in item[\"messages\"]:\n",
    "            if msg[\"role\"] == \"assistant\":\n",
    "                if \"<start_working_out>\" in msg[\"content\"] and \"<SOLUTION>\" in msg[\"content\"]:\n",
    "                    grpo_format_count += 1\n",
    "                break\n",
    "\n",
    "print(f\"\\n‚úÖ GRPO format validation: {grpo_format_count}/100 samples have proper structure\")\n",
    "if grpo_format_count < 50:\n",
    "    print(\"‚ö†Ô∏è Warning: Low GRPO format compliance. Check data generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8359083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 85% train, 10% validation, 5% test (less aggressive split for synthetic data)\n",
    "train_data, temp_data = train_test_split(data, test_size=0.15, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)  # 5% test, 10% val\n",
    "\n",
    "print(f\"üìä Train: {len(train_data):,} samples ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"üìä Validation: {len(val_data):,} samples ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "print(f\"üìä Test: {len(test_data):,} samples ({len(test_data)/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f0387",
   "metadata": {},
   "source": [
    "## üìù Chat Template cho GRPO Format\n",
    "\n",
    "Data ƒë√£ c√≥ format messages, ch·ªâ c·∫ßn convert sang text format cho training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import apply_chat_template\n",
    "\n",
    "def convert_messages_to_text(examples):\n",
    "    \"\"\"Convert messages format to conversation list for chat template\"\"\"\n",
    "    conversations = []\n",
    "    \n",
    "    for item in examples[\"messages\"]:\n",
    "        # Each item should already be a list of messages\n",
    "        conversations.append(item)\n",
    "    \n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(\"üîÑ Converting to conversation format...\")\n",
    "\n",
    "# Convert messages to conversations\n",
    "train_dataset = train_dataset.map(convert_messages_to_text, batched=True)\n",
    "val_dataset = val_dataset.map(convert_messages_to_text, batched=True)\n",
    "\n",
    "# Apply chat template using Unsloth\n",
    "print(\"üîÑ Applying chat template...\")\n",
    "\n",
    "train_dataset = apply_chat_template(\n",
    "    train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    chat_template=\"llama-3.1\",  # Use Llama 3.1 template (compatible with 3.2)\n",
    ")\n",
    "\n",
    "val_dataset = apply_chat_template(\n",
    "    val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data formatted with Llama chat template for GRPO format\")\n",
    "print(\"\\nüìù Example formatted conversation:\")\n",
    "print(train_dataset[0]['text'][:800] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001bd9b",
   "metadata": {},
   "source": [
    "## üéì Training Configuration - Optimized for GRPO Synthetic Data\n",
    "\n",
    "### Settings cho model ƒë√£ fine-tune + synthetic data:\n",
    "- **Epochs**: 1-2 (model ƒë√£ c√≥ base knowledge)\n",
    "- **Learning rate**: 5e-5 ƒë·∫øn 1e-4 (th·∫•p h∆°n)\n",
    "- **Batch size**: Nh·ªè h∆°n do sequence d√†i h∆°n\n",
    "- **More evaluation**: Monitor overfitting carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49be28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for GRPO synthetic data\n",
    "training_args = TrainingArguments(\n",
    "    # Output & Logging\n",
    "    output_dir=\"./outputs-grpo-synthetic\",\n",
    "    run_name=\"llama3.2-3b-grpo-synthetic-v1\",\n",
    "    \n",
    "    # Training dynamics - Conservative for already fine-tuned model\n",
    "    num_train_epochs=1,  # Start with 1 epoch, can increase if needed\n",
    "    per_device_train_batch_size=2,  # Reduced due to longer sequences\n",
    "    gradient_accumulation_steps=8,  # Increased to maintain effective batch size = 16\n",
    "    \n",
    "    # Optimization - Lower LR for already fine-tuned model\n",
    "    optim=\"adamw_8bit\",  # 8-bit AdamW for memory efficiency\n",
    "    learning_rate=5e-5,  # Lower LR than fresh model (was 2e-4)\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.05,  # Shorter warmup (5% instead of 10%)\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine annealing\n",
    "    \n",
    "    # Performance\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # Use FP16 for T4\n",
    "    bf16=torch.cuda.is_bf16_supported(),  # Use BF16 if supported\n",
    "    \n",
    "    # Logging & Saving - More frequent for careful monitoring\n",
    "    logging_steps=2,  # Very frequent logging for synthetic data\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,  # Save very frequently to avoid overfitting\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Evaluation - Very frequent to catch overfitting early\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,  # Frequent evaluation\n",
    "    eval_accumulation_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Early stopping to prevent overfitting on synthetic data\n",
    "    # early_stopping_patience=3,\n",
    "    \n",
    "    # WandB integration\n",
    "    report_to=\"wandb\",\n",
    "    logging_nan_inf_filter=True,\n",
    "    include_inputs_for_metrics=False,\n",
    "    \n",
    "    # Progress bar\n",
    "    disable_tqdm=False,\n",
    "    log_level=\"info\",\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured for GRPO synthetic data\")\n",
    "print(f\"üíæ Per device batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"üìä Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"üìö Learning rate: {training_args.learning_rate} (reduced for fine-tuned model)\")\n",
    "print(f\"üìä Epochs: {training_args.num_train_epochs} (conservative approach)\")\n",
    "print(f\"üïê Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea386905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Keep False for structured GRPO format\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SFT Trainer initialized for GRPO synthetic data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6230ac6",
   "metadata": {},
   "source": [
    "## üöÄ Start Training!\n",
    "\n",
    "**Estimated time**: ~30-60 min for 1 epoch (shorter due to pre-trained model)  \n",
    "**Memory usage**: ~15-16GB VRAM (longer sequences)  \n",
    "**Watch for**: Overfitting (eval loss increasing while train loss decreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7317cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show GPU stats before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"üñ•Ô∏è GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"üíæ {start_gpu_memory} GB of memory reserved.\")\n",
    "print(f\"üéØ Training GRPO model with synthetic structured data...\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüöÄ Starting GRPO synthetic data training...\\n\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show final stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ GRPO SYNTHETIC DATA TRAINING COMPLETED!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚è±Ô∏è Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"üíæ Peak reserved memory: {used_memory} GB\")\n",
    "print(f\"üìä Memory used for training: {used_memory_for_lora} GB\")\n",
    "print(f\"üìà Percentage of max memory: {used_percentage}%\")\n",
    "print(f\"üéØ Final train loss: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59518c97",
   "metadata": {},
   "source": [
    "## üìä Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ffa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"üìä Evaluating on validation set...\\n\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"VALIDATION RESULTS - GRPO SYNTHETIC\")\n",
    "print(\"=\"*50)\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
    "\n",
    "# Log to WandB\n",
    "wandb.log({\n",
    "    \"final_eval_loss\": eval_results['eval_loss'],\n",
    "    \"model_type\": \"grpo_synthetic_sft\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a590a",
   "metadata": {},
   "source": [
    "## üß™ Inference Testing - GRPO Format Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b904fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# GRPO format markers for validation\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "def test_grpo_model(user_message, max_new_tokens=512):\n",
    "    \"\"\"Test model with GRPO system prompt\"\"\"\n",
    "    \n",
    "    # GRPO system prompt\n",
    "    system_prompt = f\"\"\"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. Khi tr·∫£ l·ªùi c√¢u h·ªèi, h√£y:\n",
    "1. Suy nghƒ© v√† ph√¢n t√≠ch c√¢u h·ªèi trong ph·∫ßn {reasoning_start} {reasoning_end}\n",
    "2. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c trong ph·∫ßn {solution_start}{solution_end}\n",
    "\n",
    "C√¢u tr·∫£ l·ªùi c·∫ßn d·ª±a tr√™n quy ƒë·ªãnh ph√°p lu·∫≠t hi·ªán h√†nh v√† ph·∫£i r√µ r√†ng, d·ªÖ hi·ªÉu.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    # Extract only the response part\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n",
    "        response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "        response = response.split(\"<|eot_id|>\")[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def validate_grpo_format(response):\n",
    "    \"\"\"Validate if response follows GRPO format\"\"\"\n",
    "    has_reasoning = reasoning_start in response and reasoning_end in response\n",
    "    has_solution = solution_start in response and solution_end in response\n",
    "    \n",
    "    return {\n",
    "        \"has_reasoning\": has_reasoning,\n",
    "        \"has_solution\": has_solution,\n",
    "        \"proper_format\": has_reasoning and has_solution\n",
    "    }\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"M·ª©c ph·∫°t cho vi·ªác v∆∞·ª£t ƒë√®n ƒë·ªè ƒë·ªëi v·ªõi xe m√°y l√† bao nhi√™u?\",\n",
    "    \"ƒêi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞·ª£c c·∫•p b·∫±ng l√°i xe √¥ t√¥ h·∫°ng B1 l√† g√¨?\",\n",
    "    \"H√†nh vi n√†o b·ªã c·∫•m khi tham gia giao th√¥ng ƒë∆∞·ªùng b·ªô?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing GRPO model with structured format validation...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "format_validation_results = []\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST QUESTION #{i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    \n",
    "    # Generate response\n",
    "    response = test_grpo_model(question)\n",
    "    print(f\"\\nü§ñ Model Response:\\n{response}\")\n",
    "    \n",
    "    # Validate format\n",
    "    validation = validate_grpo_format(response)\n",
    "    format_validation_results.append(validation)\n",
    "    \n",
    "    print(f\"\\nüìä Format Validation:\")\n",
    "    print(f\"   Has reasoning section: {validation['has_reasoning']} ‚úÖ\" if validation['has_reasoning'] else f\"   Has reasoning section: {validation['has_reasoning']} ‚ùå\")\n",
    "    print(f\"   Has solution section: {validation['has_solution']} ‚úÖ\" if validation['has_solution'] else f\"   Has solution section: {validation['has_solution']} ‚ùå\")\n",
    "    print(f\"   Proper GRPO format: {validation['proper_format']} ‚úÖ\" if validation['proper_format'] else f\"   Proper GRPO format: {validation['proper_format']} ‚ùå\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Summary of format validation\n",
    "proper_format_count = sum(1 for r in format_validation_results if r['proper_format'])\n",
    "print(f\"\\nüìä GRPO FORMAT VALIDATION SUMMARY:\")\n",
    "print(f\"   Proper format: {proper_format_count}/{len(test_questions)} ({proper_format_count/len(test_questions)*100:.1f}%)\")\n",
    "\n",
    "if proper_format_count == len(test_questions):\n",
    "    print(\"üéâ Excellent! Model consistently follows GRPO format\")\n",
    "elif proper_format_count >= len(test_questions) * 0.7:\n",
    "    print(\"‚úÖ Good! Model mostly follows GRPO format\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Model needs more training on GRPO format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afa2dd",
   "metadata": {},
   "source": [
    "## üíæ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"grpo_synthetic_lora\")\n",
    "tokenizer.save_pretrained(\"grpo_synthetic_lora\")\n",
    "\n",
    "print(\"‚úÖ GRPO synthetic LoRA adapters saved to: grpo_synthetic_lora/\")\n",
    "print(\"üì¶ Size: ~100-200MB (adapters only)\")\n",
    "\n",
    "# Optional: Save merged model (full size ~6GB)\n",
    "model.save_pretrained_merged(\"grpo_synthetic_merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "print(\"‚úÖ GRPO synthetic merged model saved to: grpo_synthetic_merged/\")\n",
    "print(\"üéØ This model now has: Base ‚Üí GRPO training ‚Üí Synthetic data SFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0930b",
   "metadata": {},
   "source": [
    "## üì§ Model Upload & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to HuggingFace Hub\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"/content\"):  # Colab environment\n",
    "    print(\"üöÄ Uploading GRPO synthetic model to HuggingFace Hub...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        hf_token = userdata.get(\"HF_TOKEN\")\n",
    "        \n",
    "        from huggingface_hub import HfApi, login\n",
    "        \n",
    "        # Login to HuggingFace\n",
    "        login(token=hf_token)\n",
    "        print(\"‚úÖ Logged in to HuggingFace\")\n",
    "        \n",
    "        # Update with your username\n",
    "        YOUR_HF_USERNAME = \"mikeethanh\"  # ‚ö†Ô∏è UPDATE THIS!\n",
    "        repo_name = f\"{YOUR_HF_USERNAME}/vietnamese-legal-llama3.2-3b-grpo-synthetic\"\n",
    "        \n",
    "        print(f\"\\nüì§ Uploading to: {repo_name}\")\n",
    "        print(\"‚è≥ Uploading merged model (~6GB)...\\n\")\n",
    "        \n",
    "        # Upload merged model\n",
    "        if os.path.exists(\"grpo_synthetic_merged\"):\n",
    "            from huggingface_hub import create_repo, upload_folder\n",
    "            \n",
    "            # Create repo\n",
    "            try:\n",
    "                create_repo(repo_name, repo_type=\"model\", exist_ok=True)\n",
    "                print(f\"‚úÖ Repository created: https://huggingface.co/{repo_name}\")\n",
    "            except:\n",
    "                print(f\"‚ÑπÔ∏è Repository already exists: https://huggingface.co/{repo_name}\")\n",
    "            \n",
    "            # Upload folder\n",
    "            upload_folder(\n",
    "                folder_path=\"grpo_synthetic_merged\",\n",
    "                repo_id=repo_name,\n",
    "                commit_message=\"Vietnamese Legal AI - GRPO + Synthetic Data SFT Model\",\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"‚úÖ UPLOAD SUCCESSFUL!\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"\\nüéØ Model evolution: Base ‚Üí GRPO ‚Üí Synthetic SFT\")\n",
    "            print(f\"üì• Download: git clone https://huggingface.co/{repo_name}\")\n",
    "            print(f\"üåê View: https://huggingface.co/{repo_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"\\nüìù Setup instructions:\")\n",
    "        print(\"  1. Get token: https://huggingface.co/settings/tokens\")\n",
    "        print(\"  2. Add to Colab Secrets: HF_TOKEN\")\n",
    "        print(\"  3. Update YOUR_HF_USERNAME in code\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è For local environments, use model.push_to_hub() method\")\n",
    "    print(\"üìÇ Models saved locally at:\")\n",
    "    print(\"   - grpo_synthetic_lora/\")\n",
    "    print(\"   - grpo_synthetic_merged/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877158b",
   "metadata": {},
   "source": [
    "## üìä Quantization Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7eba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF for deployment\n",
    "quantization_methods = [\n",
    "    \"q8_0\",    # Fast inference, good quality\n",
    "    \"q4_k_m\",  # Smaller size, good balance\n",
    "]\n",
    "\n",
    "for method in quantization_methods:\n",
    "    print(f\"\\nüì¶ Exporting GRPO synthetic model to {method.upper()}...\")\n",
    "    model.save_pretrained_gguf(\n",
    "        \"grpo_synthetic_model\",\n",
    "        tokenizer,\n",
    "        quantization_method=method,\n",
    "    )\n",
    "    print(f\"‚úÖ Exported: grpo_synthetic_model-{method.upper()}.gguf\")\n",
    "\n",
    "print(\"\\n‚úÖ All GGUF exports completed!\")\n",
    "print(\"üöÄ Ready for deployment with Ollama or llama.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b7b3b",
   "metadata": {},
   "source": [
    "## üéâ Training Summary & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa02820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish WandB run\n",
    "wandb.finish()\n",
    "\n",
    "# Clear GPU memory\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ GRPO Synthetic Data Training completed successfully!\")\n",
    "print(\"\\nüìä FINAL SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ü§ñ Base Model: mikeethanh/vietnamese-legal-llama3.2-3b-merged-grpo\")\n",
    "print(f\"üìä Training samples: {len(train_data):,} (synthetic GRPO format)\")\n",
    "print(f\"üìä Validation samples: {len(val_data):,}\")\n",
    "print(f\"üìä Test samples: {len(test_data):,}\")\n",
    "print(f\"‚è±Ô∏è Training time: ~{trainer_stats.metrics['train_runtime']/60:.1f} minutes\")\n",
    "print(f\"üéØ Final eval loss: {eval_results['eval_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nüìÇ SAVED OUTPUTS:\")\n",
    "print(\"  ‚úÖ LoRA adapters: grpo_synthetic_lora/\")\n",
    "print(\"  ‚úÖ Merged model: grpo_synthetic_merged/\")\n",
    "print(\"  ‚úÖ GGUF models: grpo_synthetic_model-*.gguf\")\n",
    "\n",
    "print(\"\\nüéØ MODEL EVOLUTION COMPLETE:\")\n",
    "print(\"  1Ô∏è‚É£ Base: Llama-3.2-3B-Instruct\")\n",
    "print(\"  2Ô∏è‚É£ GRPO: Reinforcement learning v·ªõi reward functions\")\n",
    "print(\"  3Ô∏è‚É£ SFT: Synthetic data v·ªõi structured reasoning format\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"  1. Test model on real user queries\")\n",
    "print(\"  2. Validate GRPO format consistency\")\n",
    "print(\"  3. Deploy and collect feedback\")\n",
    "print(\"  4. Iterate with more synthetic data if needed\")\n",
    "\n",
    "print(\"\\nüéâ Training pipeline complete! Model ready for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
