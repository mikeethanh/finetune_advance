# Reinforcement Learning cho Fine-tuning LLM: Tá»•ng Quan & Äá» Xuáº¥t

## ğŸ“š Má»¥c lá»¥c
1. [Giá»›i thiá»‡u chung](#1-giá»›i-thiá»‡u-chung)
2. [RLHF - Reinforcement Learning from Human Feedback](#2-rlhf---reinforcement-learning-from-human-feedback)
3. [DPO - Direct Preference Optimization](#3-dpo---direct-preference-optimization)
4. [GRPO - Group Relative Policy Optimization](#4-grpo---group-relative-policy-optimization)
5. [CÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c](#5-cÃ¡c-phÆ°Æ¡ng-phÃ¡p-khÃ¡c)
6. [So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p](#6-so-sÃ¡nh-cÃ¡c-phÆ°Æ¡ng-phÃ¡p)
7. [Äá» xuáº¥t cho bÃ i toÃ¡n luáº­t phÃ¡p](#7-Ä‘á»-xuáº¥t-cho-bÃ i-toÃ¡n-luáº­t-phÃ¡p)

---

## 1. Giá»›i thiá»‡u chung

### Táº¡i sao cáº§n Reinforcement Learning sau SFT?

**Supervised Fine-Tuning (SFT)** giÃºp model há»c Ä‘Æ°á»£c:
- âœ… Äá»‹nh dáº¡ng cÃ¢u tráº£ lá»i
- âœ… Kiáº¿n thá»©c tá»« dá»¯ liá»‡u training
- âœ… CÃ¡ch tráº£ lá»i theo instruction

**NhÆ°ng SFT cÃ³ háº¡n cháº¿:**
- âŒ KhÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c cÃ¢u tráº£ lá»i "tá»‘t" vs "xuáº¥t sáº¯c"
- âŒ Dá»… bá»‹ overfitting vÃ o format cá»¥ thá»ƒ
- âŒ KhÃ´ng tá»‘i Æ°u hÃ³a cho má»¥c tiÃªu con ngÆ°á»i (helpfulness, harmlessness, honesty)

**Reinforcement Learning (RL)** giáº£i quyáº¿t báº±ng cÃ¡ch:
- ğŸ¯ Há»c tá»« feedback/preference cá»§a con ngÆ°á»i
- ğŸ¯ Tá»‘i Æ°u hÃ³a cho cÃ¡c má»¥c tiÃªu cá»¥ thá»ƒ (accuracy, safety, coherence)
- ğŸ¯ Cáº£i thiá»‡n cháº¥t lÆ°á»£ng output theo hÆ°á»›ng mong muá»‘n

---

## 2. RLHF - Reinforcement Learning from Human Feedback

### 2.1. NguyÃªn lÃ½

RLHF lÃ  phÆ°Æ¡ng phÃ¡p RL cá»• Ä‘iá»ƒn cho LLM, Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi OpenAI (ChatGPT), Anthropic (Claude).

**Pipeline 3 bÆ°á»›c:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. SFT Model   â”‚  â†’   â”‚  2. Reward Model â”‚  â†’   â”‚  3. RL Training â”‚
â”‚  (Base model)   â”‚      â”‚  (Preference)    â”‚      â”‚  (PPO/RLHF)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2. Chi tiáº¿t tá»«ng bÆ°á»›c

#### BÆ°á»›c 1: Supervised Fine-Tuning (SFT)
- Train model trÃªn instruction dataset (Ä‘Ã£ lÃ m á»Ÿ pháº§n trÆ°á»›c)
- Táº¡o base model cÃ³ kháº£ nÄƒng follow instructions

#### BÆ°á»›c 2: Train Reward Model (RM)
**Má»¥c tiÃªu:** Há»c Ä‘Æ°á»£c preference cá»§a con ngÆ°á»i

**Dá»¯ liá»‡u cáº§n:**
```json
{
  "prompt": "Tuá»•i nghá»‰ hÆ°u cá»§a cÃ´ng nhÃ¢n viÃªn chá»©c lÃ  bao nhiÃªu?",
  "chosen": "Theo Luáº­t Lao Ä‘á»™ng 2019, tuá»•i nghá»‰ hÆ°u lÃ ...",  // Better response
  "rejected": "KhÃ´ng rÃµ, báº¡n nÃªn há»i luáº­t sÆ°."  // Worse response
}
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- Train má»™t model phÃ¢n loáº¡i Ä‘á»ƒ score responses
- Model há»c predict: `score(chosen) > score(rejected)`
- Loss function: `L = -log(Ïƒ(r_chosen - r_rejected))`

**Sá»‘ lÆ°á»£ng data cáº§n:**
- Tá»‘i thiá»ƒu: 1,000-5,000 pairs
- LÃ½ tÆ°á»Ÿng: 10,000-50,000 pairs
- CÃ ng nhiá»u cÃ ng tá»‘t!

#### BÆ°á»›c 3: RL Training vá»›i PPO

**Proximal Policy Optimization (PPO):**

CÃ´ng thá»©c objective:
```
L_PPO = E[min(r(Î¸)A, clip(r(Î¸), 1-Îµ, 1+Îµ)A)] - Î²Â·KL(Ï€_Î¸ || Ï€_ref)
```

Trong Ä‘Ã³:
- `Ï€_Î¸`: Policy hiá»‡n táº¡i (model Ä‘ang train)
- `Ï€_ref`: Reference policy (SFT model)
- `KL`: KL divergence (Ä‘áº£m báº£o khÃ´ng Ä‘i quÃ¡ xa reference)
- `Î²`: KL penalty coefficient
- `A`: Advantage (tá»« reward model)

**Training loop:**
```
for batch in dataset:
    1. Generate responses tá»« current policy
    2. Score responses báº±ng reward model
    3. Compute advantages
    4. Update policy vá»›i PPO objective
    5. Ensure KL divergence khÃ´ng quÃ¡ lá»›n
```

### 2.3. Æ¯u Ä‘iá»ƒm

âœ… **Hiá»‡u quáº£ cao**: ÄÃ£ Ä‘Æ°á»£c chá»©ng minh vá»›i ChatGPT, Claude
âœ… **Linh hoáº¡t**: CÃ³ thá»ƒ tá»‘i Æ°u cho nhiá»u objectives khÃ¡c nhau
âœ… **Controllable**: KL penalty giá»¯ model khÃ´ng Ä‘i quÃ¡ xa base

### 2.4. NhÆ°á»£c Ä‘iá»ƒm

âŒ **Phá»©c táº¡p**: Cáº§n train 2 models (reward model + policy)
âŒ **Tá»‘n tÃ i nguyÃªn**: 
  - Cáº§n ~2x VRAM (lÆ°u cáº£ reward model vÃ  policy)
  - Training cháº­m hÆ¡n SFT nhiá»u
âŒ **Dá»¯ liá»‡u Ä‘áº¯t**: Cáº§n human preference data
âŒ **KhÃ³ train**: PPO unstable, cáº§n tune nhiá»u hyperparameters
âŒ **Thá»i gian**: ~3-5x thá»i gian SFT

### 2.5. Æ¯á»›c tÃ­nh thá»i gian cho Kaggle

**Vá»›i 2xT4 GPU (30h/week):**
- Train reward model: ~5-8 giá» (trÃªn 10k pairs)
- RL training: ~20-25 giá»
- **Tá»•ng: ~30 giá»** (vá»«a khÃ­t giá»›i háº¡n!)

---

## 3. DPO - Direct Preference Optimization

### 3.1. NguyÃªn lÃ½

DPO lÃ  **phÆ°Æ¡ng phÃ¡p RL khÃ´ng cáº§n reward model**, Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Stanford 2023.

**Key insight:** Thay vÃ¬ train reward model rá»“i dÃ¹ng RL, ta cÃ³ thá»ƒ tá»‘i Æ°u trá»±c tiáº¿p tá»« preference data!

### 3.2. CÃ¡ch hoáº¡t Ä‘á»™ng

**DPO objective:**

```
L_DPO = -E[log Ïƒ(Î² log(Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x)) - Î² log(Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x)))]
```

Trong Ä‘Ã³:
- `y_w`: Chosen response (better)
- `y_l`: Rejected response (worse)
- `Ï€_Î¸`: Model Ä‘ang train
- `Ï€_ref`: Reference model (SFT)
- `Î²`: Temperature parameter

**Hiá»ƒu Ä‘Æ¡n giáº£n:**
- TÄƒng probability cá»§a `y_w` (chosen)
- Giáº£m probability cá»§a `y_l` (rejected)
- Giá»¯ model gáº§n vá»›i reference model

### 3.3. Pipeline Ä‘Æ¡n giáº£n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. SFT Model   â”‚  â†’   â”‚  2. DPO Training â”‚
â”‚  (Base model)   â”‚      â”‚  (Preference)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Chá»‰ 2 bÆ°á»›c thay vÃ¬ 3!

### 3.4. Dá»¯ liá»‡u cáº§n

Giá»‘ng RLHF, cáº§n preference pairs:
```json
{
  "prompt": "Äiá»u kiá»‡n cáº¥p báº±ng lÃ¡i xe lÃ  gÃ¬?",
  "chosen": "Theo Luáº­t Giao thÃ´ng 2008, Ä‘iá»u kiá»‡n cáº¥p báº±ng lÃ¡i xe...",
  "rejected": "Cáº§n Ä‘á»§ tuá»•i vÃ  thi Ä‘áº­u."
}
```

**Sá»‘ lÆ°á»£ng:**
- Tá»‘i thiá»ƒu: 1,000 pairs
- Khuyáº¿n nghá»‹: 5,000-10,000 pairs
- Ãt hÆ¡n RLHF vÃ¬ khÃ´ng cáº§n train reward model riÃªng

### 3.5. Æ¯u Ä‘iá»ƒm

âœ… **ÄÆ¡n giáº£n hÆ¡n RLHF**: KhÃ´ng cáº§n reward model
âœ… **Ãt tÃ i nguyÃªn hÆ¡n**: Chá»‰ cáº§n 1 model thay vÃ¬ 2
âœ… **Training á»•n Ä‘á»‹nh hÆ¡n**: KhÃ´ng cÃ³ PPO instability
âœ… **Nhanh hÆ¡n**: ~2x nhanh hÆ¡n RLHF
âœ… **Hiá»‡u quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng**: Káº¿t quáº£ tá»‘t nhÆ° RLHF trong nhiá»u tasks

### 3.6. NhÆ°á»£c Ä‘iá»ƒm

âŒ **Váº«n cáº§n preference data**: Tá»‘n cÃ´ng táº¡o/label
âŒ **Ãt linh hoáº¡t hÆ¡n RLHF**: KhÃ³ tá»‘i Æ°u cho multiple objectives
âŒ **Má»›i hÆ¡n**: Ãt Ä‘Æ°á»£c test trong production

### 3.7. Æ¯á»›c tÃ­nh thá»i gian cho Kaggle

**Vá»›i 2xT4 GPU:**
- DPO training: ~8-12 giá» (trÃªn 5k pairs)
- **Tá»•ng: ~10 giá»** (ráº¥t phÃ¹ há»£p vá»›i 30h/week!)

---

## 4. GRPO - Group Relative Policy Optimization

### 4.1. NguyÃªn lÃ½

GRPO lÃ  biáº¿n thá»ƒ cá»§a RLHF, Ä‘Æ°á»£c DeepSeek phÃ¡t triá»ƒn, tá»‘i Æ°u cho **nhÃ³m responses**.

**Key idea:** Thay vÃ¬ so sÃ¡nh 2 responses, ta generate nhiá»u responses vÃ  rank chÃºng.

### 4.2. CÃ¡ch hoáº¡t Ä‘á»™ng

**Pipeline:**

```
1. Generate K responses cho má»—i prompt (K=4-8)
2. Score táº¥t cáº£ responses báº±ng reward model (hoáº·c auto metric)
3. Rank responses theo score
4. Update policy Ä‘á»ƒ:
   - TÄƒng prob cá»§a top responses
   - Giáº£m prob cá»§a bottom responses
```

**Objective:**

```
L_GRPO = E[âˆ‘(r_i - r_mean) * log Ï€_Î¸(y_i|x)]
```

Trong Ä‘Ã³:
- `r_i`: Reward cá»§a response i
- `r_mean`: Average reward trong group
- Responses tá»‘t hÆ¡n mean Ä‘Æ°á»£c tÄƒng prob, ngÆ°á»£c láº¡i giáº£m

### 4.3. Æ¯u Ä‘iá»ƒm

âœ… **á»”n Ä‘á»‹nh hÆ¡n PPO**: Group-based normalization
âœ… **Sample efficient**: Há»c tá»« nhiá»u responses cÃ¹ng lÃºc
âœ… **Tá»‘t cho ranking tasks**: PhÃ¹ há»£p khi cÃ³ nhiá»u levels of quality

### 4.4. NhÆ°á»£c Ä‘iá»ƒm

âŒ **Tá»‘n compute**: Pháº£i generate K responses má»—i láº§n
âŒ **Váº«n cáº§n reward model**: Giá»‘ng RLHF
âŒ **Implementation phá»©c táº¡p**: Ãt library support

### 4.5. Æ¯á»›c tÃ­nh thá»i gian

**Vá»›i 2xT4 GPU:**
- Train reward model: ~5-8 giá»
- GRPO training: ~15-20 giá»
- **Tá»•ng: ~25 giá»**

---

## 5. CÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

### 5.1. RLAIF - RL from AI Feedback

**Ã tÆ°á»Ÿng:** DÃ¹ng AI (GPT-4, Claude) Ä‘á»ƒ generate preference data thay vÃ¬ human.

**Æ¯u Ä‘iá»ƒm:**
âœ… Ráº» hÆ¡n human labeling
âœ… Scale dá»… dÃ ng
âœ… Consistent

**NhÆ°á»£c Ä‘iá»ƒm:**
âŒ Phá»¥ thuá»™c vÃ o AI teacher (cÃ³ thá»ƒ biased)
âŒ Cáº§n API access (GPT-4)

**PhÃ¹ há»£p:** Khi khÃ´ng cÃ³ budget cho human labeling

### 5.2. KTO - Kahneman-Tversky Optimization

**Ã tÆ°á»Ÿng:** Dá»±a trÃªn prospect theory, optimize cho binary feedback (good/bad).

**Dá»¯ liá»‡u:**
```json
{
  "prompt": "...",
  "response": "...",
  "label": "good"  // hoáº·c "bad"
}
```

**Æ¯u Ä‘iá»ƒm:**
âœ… ÄÆ¡n giáº£n hÆ¡n preference pairs
âœ… Dá»¯ liá»‡u dá»… collect (chá»‰ cáº§n thumbs up/down)

**NhÆ°á»£c Ä‘iá»ƒm:**
âŒ KÃ©m thÃ´ng tin hÆ¡n pairs
âŒ Má»›i, chÆ°a Ä‘Æ°á»£c test rá»™ng rÃ£i

### 5.3. IPO - Identity Preference Optimization

**Ã tÆ°á»Ÿng:** Variant of DPO vá»›i regularization tá»‘t hÆ¡n.

**Objective:**
```
L_IPO = E[(log(Ï€_Î¸(y_w|x)/Ï€_Î¸(y_l|x)) - 1/Î²)^2]
```

**Æ¯u Ä‘iá»ƒm:**
âœ… á»”n Ä‘á»‹nh hÆ¡n DPO
âœ… Less sensitive to Î²

### 5.4. ORPO - Odds Ratio Preference Optimization

**Ã tÆ°á»Ÿng:** Combine SFT vÃ  preference learning trong 1 loss.

**Loss:**
```
L_ORPO = L_SFT + Î»Â·L_OR
```

**Æ¯u Ä‘iá»ƒm:**
âœ… Train trong 1 bÆ°á»›c (khÃ´ng cáº§n SFT riÃªng)
âœ… Hiá»‡u quáº£ tá»‘t

**NhÆ°á»£c Ä‘iá»ƒm:**
âŒ Má»›i, Ã­t documentation

---

## 6. So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p

### 6.1. Báº£ng so sÃ¡nh tá»•ng quan

| PhÆ°Æ¡ng phÃ¡p | Äá»™ phá»©c táº¡p | Thá»i gian train | VRAM cáº§n | Dá»¯ liá»‡u cáº§n | Hiá»‡u quáº£ | Äá»™ á»•n Ä‘á»‹nh |
|-------------|-------------|-----------------|----------|-------------|----------|------------|
| **RLHF** | â­â­â­â­â­ | 25-30h | 2x model | 10k+ pairs | â­â­â­â­â­ | â­â­â­ |
| **DPO** | â­â­â­ | 10-12h | 1.5x model | 5k+ pairs | â­â­â­â­ | â­â­â­â­ |
| **GRPO** | â­â­â­â­ | 20-25h | 2x model | 10k+ pairs | â­â­â­â­ | â­â­â­â­ |
| **KTO** | â­â­ | 8-10h | 1.5x model | 5k+ samples | â­â­â­ | â­â­â­â­ |
| **IPO** | â­â­â­ | 10-12h | 1.5x model | 5k+ pairs | â­â­â­â­ | â­â­â­â­â­ |
| **ORPO** | â­â­ | 8-10h | 1x model | 5k+ pairs | â­â­â­â­ | â­â­â­â­ |

### 6.2. Flowchart lá»±a chá»n

```
Báº¡n cÃ³ preference data?
â”‚
â”œâ”€ CÃ³ (pairs: chosen/rejected)
â”‚  â”‚
â”‚  â”œâ”€ CÃ³ nhiá»u GPU resources & thá»i gian?
â”‚  â”‚  â””â”€ YES â†’ RLHF (best quality)
â”‚  â”‚  â””â”€ NO â†’ DPO (recommended)
â”‚  â”‚
â”‚  â””â”€ Muá»‘n Ä‘Æ¡n giáº£n nháº¥t?
â”‚     â””â”€ ORPO hoáº·c IPO
â”‚
â””â”€ KhÃ´ng
   â”‚
   â”œâ”€ CÃ³ budget dÃ¹ng GPT-4?
   â”‚  â””â”€ YES â†’ RLAIF (generate preference data)
   â”‚
   â”œâ”€ Chá»‰ cÃ³ binary feedback (good/bad)?
   â”‚  â””â”€ KTO
   â”‚
   â””â”€ KhÃ´ng cÃ³ gÃ¬
      â””â”€ Táº¡o preference data báº±ng:
         - Self-critique
         - Rule-based scoring
         - Hoáº·c skip RL, chá»‰ lÃ m SFT
```

---

## 7. Äá» xuáº¥t cho bÃ i toÃ¡n luáº­t phÃ¡p

### 7.1. PhÃ¢n tÃ­ch yÃªu cáº§u

**Äáº·c Ä‘iá»ƒm bÃ i toÃ¡n:**
- âœ… CÃ³ 97k instruction data (SFT done)
- âŒ KHÃ”NG cÃ³ preference data
- âœ… Giá»›i háº¡n thá»i gian: 30h/week Kaggle
- âœ… GPU: 2xT4 (limited VRAM)
- ğŸ¯ Má»¥c tiÃªu: CÃ¢u tráº£ lá»i chÃ­nh xÃ¡c, cÃ³ trÃ­ch dáº«n luáº­t

### 7.2. Äá» xuáº¥t: **DPO vá»›i Synthetic Preference Data**

**Táº¡i sao DPO?**

1. âœ… **ÄÆ¡n giáº£n & á»•n Ä‘á»‹nh**: Dá»… implement, Ã­t bug
2. âœ… **Tiáº¿t kiá»‡m thá»i gian**: ~10-12h training
3. âœ… **Hiá»‡u quáº£ cao**: Gáº§n báº±ng RLHF
4. âœ… **PhÃ¹ há»£p VRAM**: Chá»‰ cáº§n ~20-24GB (OK vá»›i 2xT4)

**Giáº£i quyáº¿t váº¥n Ä‘á» khÃ´ng cÃ³ preference data:**

### 7.3. CÃ¡ch táº¡o Synthetic Preference Data

#### PhÆ°Æ¡ng Ã¡n 1: Self-Critique (Tá»± Ä‘á»™ng 100%)

**Ã tÆ°á»Ÿng:** DÃ¹ng chÃ­nh SFT model Ä‘á»ƒ generate multiple responses, sau Ä‘Ã³ tá»± rank.

```python
# Pseudo-code
for sample in dataset:
    prompt = sample['instruction'] + sample['input']
    
    # Generate 3-5 responses vá»›i different temps
    responses = []
    for temp in [0.7, 0.9, 1.1]:
        response = model.generate(prompt, temperature=temp)
        responses.append(response)
    
    # Score responses báº±ng cÃ¡c metrics
    scores = []
    for resp in responses:
        score = (
            check_citation(resp) * 0.4 +      # CÃ³ trÃ­ch dáº«n luáº­t?
            check_accuracy(resp, reference) * 0.4 +  # ChÃ­nh xÃ¡c?
            check_coherence(resp) * 0.2       # Máº¡ch láº¡c?
        )
        scores.append(score)
    
    # Táº¡o preference pair
    best_idx = argmax(scores)
    worst_idx = argmin(scores)
    preference_data.append({
        'prompt': prompt,
        'chosen': responses[best_idx],
        'rejected': responses[worst_idx]
    })
```

**Metrics tá»± Ä‘á»™ng:**
- **Citation check**: Regex tÃ¬m "Äiá»u", "Luáº­t", "Nghá»‹ Ä‘á»‹nh"
- **Accuracy**: Rouge score vá»›i reference answer
- **Length**: Prefer longer, detailed answers
- **Coherence**: Perplexity score

**Æ¯u Ä‘iá»ƒm:**
- âœ… HoÃ n toÃ n tá»± Ä‘á»™ng
- âœ… Free, scalable
- âœ… CÃ³ thá»ƒ táº¡o 10k+ pairs

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Quality khÃ´ng cao báº±ng human
- âŒ CÃ³ thá»ƒ reinforcement bias cá»§a SFT model

#### PhÆ°Æ¡ng Ã¡n 2: RLAIF vá»›i GPT-4 (Ná»­a tá»± Ä‘á»™ng)

**Setup:**
```python
# Generate 2 responses
response_a = sft_model.generate(prompt, temperature=0.7)
response_b = sft_model.generate(prompt, temperature=1.0)

# DÃ¹ng GPT-4 Ä‘á»ƒ judge
judge_prompt = f"""
Báº¡n lÃ  chuyÃªn gia Ä‘Ã¡nh giÃ¡ cÃ¢u tráº£ lá»i phÃ¡p luáº­t.

CÃ¢u há»i: {prompt}

Tráº£ lá»i A: {response_a}
Tráº£ lá»i B: {response_b}

ÄÃ¡nh giÃ¡ tráº£ lá»i nÃ o tá»‘t hÆ¡n theo tiÃªu chÃ­:
1. ChÃ­nh xÃ¡c vá» máº·t phÃ¡p luáº­t
2. CÃ³ trÃ­ch dáº«n cá»¥ thá»ƒ
3. Äáº§y Ä‘á»§, chi tiáº¿t
4. Dá»… hiá»ƒu

Tráº£ lá»i: (A/B/Tie)
LÃ½ do: ...
"""

result = gpt4_api.chat(judge_prompt)
# Parse result vÃ  táº¡o preference pair
```

**Chi phÃ­ Æ°á»›c tÃ­nh:**
- 10k pairs Ã— $0.01/request = **$100**
- Hoáº·c dÃ¹ng GPT-3.5: **$20-30**

**Æ¯u Ä‘iá»ƒm:**
- âœ… Quality cao hÆ¡n self-critique
- âœ… CÃ³ thá»ƒ control criteria

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Tá»‘n tiá»n (nhÆ°ng khÃ´ng nhiá»u)
- âŒ Cáº§n API access

#### PhÆ°Æ¡ng Ã¡n 3: Rule-Based + Manual Sampling (Hybrid)

**BÆ°á»›c 1:** Táº¡o 80% data báº±ng rule-based
**BÆ°á»›c 2:** Manual review 20% quan trá»ng nháº¥t

```python
def score_response(response, reference):
    score = 0
    
    # Rule 1: Pháº£i cÃ³ trÃ­ch dáº«n
    if re.search(r'(Äiá»u|Luáº­t|Nghá»‹ Ä‘á»‹nh) \d+', response):
        score += 30
    
    # Rule 2: Äá»™ dÃ i phÃ¹ há»£p (100-500 words)
    words = len(response.split())
    if 100 <= words <= 500:
        score += 20
    
    # Rule 3: Giá»‘ng reference
    rouge = compute_rouge(response, reference)
    score += rouge * 30
    
    # Rule 4: KhÃ´ng cÃ³ cÃ¢u láº·p
    if not has_repetition(response):
        score += 10
    
    # Rule 5: CÃ³ káº¿t luáº­n rÃµ rÃ ng
    if has_conclusion(response):
        score += 10
    
    return score
```

### 7.4. Implementation Plan

**Tuáº§n 1: Táº¡o Preference Data**
- NgÃ y 1-2: Implement self-critique pipeline
- NgÃ y 3-4: Generate 10k preference pairs
- NgÃ y 5: Validate quality (sample 100 pairs)
- NgÃ y 6-7: (Optional) Refine vá»›i GPT-4 cho 1k critical cases

**Tuáº§n 2: DPO Training**
- Setup DPO vá»›i TRL library
- Train 10-12 giá»
- Evaluate & iterate

**Tuáº§n 3: Testing & Refinement**
- A/B test vá»›i SFT model
- Human evaluation trÃªn 50-100 cases
- Fine-tune hyperparameters náº¿u cáº§n

### 7.5. Code template cho DPO

```python
from trl import DPOTrainer, DPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load SFT model
model = AutoModelForCausalLM.from_pretrained("path/to/sft-model")
ref_model = AutoModelForCausalLM.from_pretrained("path/to/sft-model")
tokenizer = AutoTokenizer.from_pretrained("path/to/sft-model")

# Load preference dataset
train_dataset = load_dataset("json", data_files="preference_train.jsonl")
eval_dataset = load_dataset("json", data_files="preference_eval.jsonl")

# DPO Config
dpo_config = DPOConfig(
    output_dir="./legal-model-dpo",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=5e-7,  # Lower than SFT
    num_train_epochs=1,  # Usually 1 epoch enough
    beta=0.1,  # Temperature for DPO
    max_length=2048,
    max_prompt_length=1024,
    logging_steps=10,
    save_steps=500,
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
)

# Initialize DPO Trainer
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=dpo_config,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)

# Train!
dpo_trainer.train()
```

### 7.6. Metrics Ä‘á»ƒ evaluate

**Automatic metrics:**
1. **Citation Rate**: % responses cÃ³ trÃ­ch dáº«n luáº­t
2. **Rouge Score**: So vá»›i reference answers
3. **Win Rate**: A/B test vá»›i SFT model (dÃ¹ng GPT-4 judge)

**Human evaluation (sample 50-100):**
1. Accuracy (1-5 scale)
2. Completeness (1-5 scale)
3. Citation quality (1-5 scale)
4. Helpfulness (1-5 scale)

### 7.7. Timeline & Resource estimate

| Phase | Thá»i gian | GPU time | Cost |
|-------|-----------|----------|------|
| Generate preference data | 2-3 ngÃ y | 5-8h | $0-100 |
| DPO training | 1 ngÃ y | 10-12h | $0 |
| Evaluation | 1-2 ngÃ y | 2-3h | $0 |
| **Tá»•ng** | **5-6 ngÃ y** | **~20h** | **$0-100** |

**Vá»«a khÃ­t vá»›i giá»›i háº¡n 30h/week Kaggle! âœ…**

---

## 8. Káº¿t luáº­n & Recommendations

### 8.1. Top recommendations cho project cá»§a báº¡n

**ğŸ¥‡ Tier 1 (Highly Recommended):**
1. **DPO vá»›i Self-Critique** - Best balance of effort/quality
   - Thá»i gian: ~20h
   - Chi phÃ­: $0
   - Complexity: Medium
   - Expected improvement: +15-25% over SFT

2. **DPO vá»›i RLAIF (GPT-3.5)** - If have small budget
   - Thá»i gian: ~20h
   - Chi phÃ­: ~$30
   - Complexity: Medium
   - Expected improvement: +20-30% over SFT

**ğŸ¥ˆ Tier 2 (Alternative):**
3. **ORPO** - Náº¿u muá»‘n Ä‘Æ¡n giáº£n tá»‘i Ä‘a
   - Thá»i gian: ~15h
   - Chi phÃ­: $0
   - Complexity: Low
   - Expected improvement: +10-20% over SFT

**ğŸ¥‰ Tier 3 (Advanced):**
4. **RLHF** - Náº¿u cÃ³ thÃªm compute & data
   - Thá»i gian: ~30h
   - Chi phÃ­: $0-200 (for RM training data)
   - Complexity: High
   - Expected improvement: +25-35% over SFT

### 8.2. Recommended learning path

**Náº¿u báº¡n má»›i báº¯t Ä‘áº§u:**
```
Week 1: SFT (done!) âœ“
Week 2: Generate preference data (self-critique)
Week 3: DPO training
Week 4: Evaluation & iteration
```

**Náº¿u báº¡n cÃ³ kinh nghiá»‡m:**
```
Week 1: SFT + Start preference data generation âœ“
Week 2: DPO training + RLAIF for critical cases
Week 3: RLHF or advanced methods
```

### 8.3. Khi nÃ o nÃªn SKIP RL?

**Skip RL náº¿u:**
- âŒ SFT model Ä‘Ã£ Ä‘á»§ tá»‘t cho use case
- âŒ KhÃ´ng cÃ³ resources Ä‘á»ƒ táº¡o preference data
- âŒ Deadline gáº¥p
- âŒ Data quÃ¡ domain-specific (RL cÃ³ thá»ƒ lÃ m worse)

**Dáº¥u hiá»‡u SFT Ä‘Ã£ Ä‘á»§:**
- âœ… Model tráº£ lá»i Ä‘Ãºng >80% test cases
- âœ… Citations Ä‘áº§y Ä‘á»§
- âœ… Format nháº¥t quÃ¡n
- âœ… Users hÃ i lÃ²ng

---

## 9. Resources & References

### 9.1. Papers

**RLHF:**
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (InstructGPT, 2022)
- [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325) (OpenAI, 2020)

**DPO:**
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) (Stanford, 2023)

**GRPO:**
- [DeepSeekMath: Pushing the Limits](https://arxiv.org/abs/2402.03300) (DeepSeek, 2024)

**Other methods:**
- [KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/abs/2402.01306)
- [IPO: A General Framework for Preference Optimization](https://arxiv.org/abs/2310.12036)
- [ORPO: Monolithic Preference Optimization](https://arxiv.org/abs/2403.07691)

### 9.2. Code & Libraries

**TRL (Transformer Reinforcement Learning):**
```bash
pip install trl
```
- Supports: DPO, PPO, RLHF, KTO, ORPO
- Docs: https://huggingface.co/docs/trl

**Unsloth vá»›i DPO:**
```python
from unsloth import FastLanguageModel, is_bfloat16_supported
from trl import DPOTrainer, DPOConfig
```

**Alignment Handbook (HF):**
- https://github.com/huggingface/alignment-handbook
- Best practices & recipes

### 9.3. Datasets (for reference)

**Preference datasets:**
- Anthropic HH-RLHF: https://huggingface.co/datasets/Anthropic/hh-rlhf
- OpenAssistant Conversations: https://huggingface.co/datasets/OpenAssistant/oasst1
- UltraFeedback: https://huggingface.co/datasets/openbmb/UltraFeedback

### 9.4. Tools for creating preference data

**LabelStudio:**
- UI Ä‘á»ƒ human annotation
- https://labelstud.io/

**Argilla:**
- Annotation platform with AI assistance
- https://argilla.io/

**LLM-as-a-Judge:**
- Prometheus: https://huggingface.co/prometheus-eval
- Auto-evaluation framework

---

## 10. Next Steps

**Immediate (Ngay bÃ¢y giá»):**
1. âœ… Äá»c xong document nÃ y
2. â¬œ Quyáº¿t Ä‘á»‹nh method: DPO recommended
3. â¬œ Báº¯t Ä‘áº§u táº¡o preference data

**This Week:**
1. Implement self-critique pipeline
2. Generate 5-10k preference pairs
3. Validate quality

**Next Week:**
1. Setup DPO training
2. Train model
3. Evaluate & compare vá»›i SFT

**Future (Optional):**
1. Try RLHF náº¿u DPO khÃ´ng Ä‘á»§
2. Experiment vá»›i ORPO, IPO
3. A/B test trong production

---

**ChÃºc báº¡n thÃ nh cÃ´ng vá»›i project! ğŸš€**

Náº¿u cÃ³ cÃ¢u há»i gÃ¬, hÃ£y há»i tÃ´i nhÃ©!
