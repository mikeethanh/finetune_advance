{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13865022,"sourceType":"datasetVersion","datasetId":8833508}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9a87e107","cell_type":"markdown","source":"## üìã Setup & Installation","metadata":{}},{"id":"9fcaed2c","cell_type":"code","source":"%%capture\nimport os, re\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\n!pip install wandb ","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:29:51.805945Z","iopub.execute_input":"2025-11-28T08:29:51.806656Z","iopub.status.idle":"2025-11-28T08:30:38.715325Z","shell.execute_reply.started":"2025-11-28T08:29:51.806621Z","shell.execute_reply":"2025-11-28T08:30:38.714406Z"},"trusted":true},"outputs":[],"execution_count":1},{"id":"43176172","cell_type":"code","source":"import os\nimport json\nimport torch\nimport wandb\nfrom datasets import Dataset, load_dataset\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport gc\n\n# Check GPU\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:30:38.717267Z","iopub.execute_input":"2025-11-28T08:30:38.717739Z","iopub.status.idle":"2025-11-28T08:31:37.105974Z","shell.execute_reply.started":"2025-11-28T08:30:38.717717Z","shell.execute_reply":"2025-11-28T08:31:37.105160Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-28 08:30:50.823339: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764318651.280937      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764318651.398819      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\nGPU Available: True\nGPU Name: Tesla T4\nGPU Memory: 15.83 GB\n","output_type":"stream"}],"execution_count":2},{"id":"e66b37ed","cell_type":"markdown","source":"## üîê WandB Login (for monitoring)","metadata":{}},{"id":"3a704149","cell_type":"code","source":"# Login to WandB for experiment tracking\n# Get WandB API key from Kaggle Secrets\n# In Kaggle: Add-ons ‚Üí Secrets ‚Üí Add new secret with key \"WANDB_API_KEY\"\n# Get your API key from: https://wandb.ai/authorize\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n# Login with API key from Kaggle Secrets\nwandb.login(key=wandb_api_key)\n\n# Initialize WandB project with detailed config\nwandb.init(\n    project=\"vietnamese-legal-ai\",\n    name=\"llama3.2-3b-traffic-law-v1\",\n    config={\n        \"model\": \"meta-llama/Llama-3.2-3B-Instruct\",\n        \"dataset\": \"traffic_law_data.jsonl\",\n        \"task\": \"legal_qa\",\n        \"language\": \"vietnamese\",\n        \"max_seq_length\": 1536,\n        \"lora_r\": 32,\n        \"lora_alpha\": 32,\n        \"learning_rate\": 2e-4,\n        \"num_epochs\": 2,\n        \"batch_size\": 4,\n        \"gradient_accumulation\": 4,\n        \"effective_batch_size\": 32,\n    },\n    settings=wandb.Settings(\n        _disable_meta=False,\n        _disable_stats=False,\n    )\n)\n\nprint(\"‚úÖ WandB initialized with detailed logging\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:31:43.233526Z","iopub.execute_input":"2025-11-28T08:31:43.234163Z","iopub.status.idle":"2025-11-28T08:31:56.937484Z","shell.execute_reply.started":"2025-11-28T08:31:43.234140Z","shell.execute_reply":"2025-11-28T08:31:56.936730Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmikeethanh04\u001b[0m (\u001b[33mmikeethanh04-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251128_083150-6tqwc20g</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai/runs/6tqwc20g' target=\"_blank\">llama3.2-3b-traffic-law-v1</a></strong> to <a href='https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai' target=\"_blank\">https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai/runs/6tqwc20g' target=\"_blank\">https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai/runs/6tqwc20g</a>"},"metadata":{}},{"name":"stdout","text":"‚úÖ WandB initialized with detailed logging\n","output_type":"stream"}],"execution_count":4},{"id":"82c0102a","cell_type":"markdown","source":"## ‚öôÔ∏è Model Configuration\n\n### T·∫°i sao ch·ªçn Llama-3.2-3B-Instruct?\n- ‚úÖ **3B parameters**: V·ª´a ƒë·ªß m·∫°nh, v·ª´a ti·∫øt ki·ªám GPU\n- ‚úÖ **Multilingual support**: H·ªó tr·ª£ nhi·ªÅu ng√¥n ng·ªØ bao g·ªìm ti·∫øng Vi·ªát\n- ‚úÖ **Instruct version**: ƒê√£ ƒë∆∞·ª£c train theo instruction format\n- ‚úÖ **Fit Kaggle T4**: ~15GB VRAM v·ªõi 4-bit quantization\n- ‚úÖ **Unsloth optimized**: H·ªó tr·ª£ t·ªët, train nhanh 2x\n- ‚úÖ **Meta's latest**: Phi√™n b·∫£n m·ªõi nh·∫•t t·ª´ Meta (2024)","metadata":{}},{"id":"020a85bd","cell_type":"code","source":"# Model configuration for Kaggle T4 (16GB VRAM)\nmax_seq_length = 1536  # Based on data analysis (covers 95% of samples)\ndtype = None  # Auto-detect. Use Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True  # Use 4bit quantization to reduce memory usage\n\n# Alternative models (uncomment to try):\n# model_name = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"  # Qwen 2.5 3B\n# model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\"  # Gemma 2B (smaller, faster)\n# model_name = \"unsloth/Phi-3-mini-4k-instruct\"  # Microsoft Phi-3\n\nmodel_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # Meta Llama 3.2 - Pre-quantized by Unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\nprint(f\"‚úÖ Model loaded: {model_name}\")\nprint(f\"üìè Max sequence length: {max_seq_length}\")\nprint(f\"üî¢ 4-bit quantization: {load_in_4bit}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:32:08.503347Z","iopub.execute_input":"2025-11-28T08:32:08.503861Z","iopub.status.idle":"2025-11-28T08:32:21.699376Z","shell.execute_reply.started":"2025-11-28T08:32:08.503837Z","shell.execute_reply":"2025-11-28T08:32:21.698672Z"},"trusted":true},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78867a98a1ab4e9198943cf155644216"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f1272bac362463b8cc49cc116cad3b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71884ddc8b0441ba77df03d916204ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82fba5a966bb4d03bed658779e81bbcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e32871f2014700b434aa069aec3d46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92d6f51a6aee4812a6d6281dca56f9a8"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Model loaded: unsloth/Llama-3.2-3B-Instruct-bnb-4bit\nüìè Max sequence length: 1536\nüî¢ 4-bit quantization: True\n","output_type":"stream"}],"execution_count":5},{"id":"50c71974","cell_type":"markdown","source":"## üéØ LoRA Configuration\n\n### LoRA Parameters Explained:\n- **r (rank)**: 16-32 cho balance quality/speed. Higher = better but slower\n- **lora_alpha**: Scaling factor, th∆∞·ªùng = r ho·∫∑c 2*r\n- **target_modules**: Train all attention & MLP layers cho best result\n- **lora_dropout**: 0 cho faster training (Unsloth optimized)\n- **bias**: \"none\" cho faster & less overfitting","metadata":{}},{"id":"8d60ddd4","cell_type":"code","source":"# Apply LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,  # LoRA rank - higher = more expressive but slower (16, 32, 64)\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],  # All attention & MLP layers\n    lora_alpha=32,  # LoRA scaling (usually = r or 2*r)\n    lora_dropout=0,  # 0 is optimized by Unsloth\n    bias=\"none\",  # \"none\" is optimized\n    use_gradient_checkpointing=\"unsloth\",  # Unsloth's long context support\n    random_state=3407,  # For reproducibility\n    use_rslora=False,  # Rank stabilized LoRA\n    loftq_config=None,  # LoftQ quantization\n)\n\nprint(\"‚úÖ LoRA adapters applied\")\nprint(f\"üìä Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\nprint(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"üí° Trainable ratio: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()):.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:32:39.103611Z","iopub.execute_input":"2025-11-28T08:32:39.103914Z","iopub.status.idle":"2025-11-28T08:32:46.174438Z","shell.execute_reply.started":"2025-11-28T08:32:39.103893Z","shell.execute_reply":"2025-11-28T08:32:46.173747Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Unsloth 2025.11.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ LoRA adapters applied\nüìä Trainable parameters: 48,627,712\nüìä Total parameters: 1,852,091,392\nüí° Trainable ratio: 2.63%\n","output_type":"stream"}],"execution_count":6},{"id":"c5744f0e","cell_type":"markdown","source":"## üìä Data Preparation","metadata":{}},{"id":"b7838d3e","cell_type":"code","source":"# Load data from Kaggle input (adjust path if uploading to Kaggle)\n# For local testing, adjust the path\ndata_path = \"/kaggle/input/traffic-law/traffic_law_data.jsonl\"  # Kaggle path\n# data_path = \"../data/finetune_llm/traffic_law_data.jsonl\"  # Local path\n\n# Check if file exists\nif not os.path.exists(data_path):\n    print(f\"‚ö†Ô∏è Data file not found at {data_path}\")\n    print(\"For Kaggle: Upload dataset or adjust path\")\n    print(\"For local: Make sure you're in the correct directory\")\nelse:\n    print(f\"‚úÖ Found data at: {data_path}\")\n\n# Load JSONL data\ndata = []\nwith open(data_path, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"üìä Total samples: {len(data):,}\")\n\n# Show sample\nprint(\"\\nüìù Sample data:\")\nsample = data[0]\nfor key, value in sample.items():\n    if key == 'output':\n        print(f\"{key}: {value[:200]}...\")  # Truncate long output\n    else:\n        print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:32:48.676137Z","iopub.execute_input":"2025-11-28T08:32:48.676757Z","iopub.status.idle":"2025-11-28T08:32:49.127116Z","shell.execute_reply.started":"2025-11-28T08:32:48.676735Z","shell.execute_reply":"2025-11-28T08:32:49.126518Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ Found data at: /kaggle/input/traffic-law/traffic_law_data.jsonl\nüìä Total samples: 8,652\n\nüìù Sample data:\ninstruction: Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\ninput: Th·ªùi h·∫°n k√©o d√†i vi·ªác t·∫°m gi·ªØ tang v·∫≠t, ph∆∞∆°ng ti·ªán c√≥ t√≠nh ng√†y ngh·ªâ kh√¥ng?\noutput: Theo ƒêi·ªÅu 8 Lu·∫≠t X·ª≠ l√Ω vi ph·∫°m h√†nh ch√≠nh 2012 quy ƒë·ªãnh v·ªÅ c√°ch t√≠nh th·ªùi gian, th·ªùi h·∫°n, th·ªùi hi·ªáu trong x·ª≠ l√Ω vi ph·∫°m h√†nh ch√≠nh nh∆∞ sau: C√°ch t√≠nh th·ªùi gian, th·ªùi h·∫°n, th·ªùi hi·ªáu trong x·ª≠ l√Ω vi ph·∫°m...\ndomains: H√¨nh s·ª±, D√¢n s·ª±, Giao th√¥ng\ntraffic_type: Vi ph·∫°m giao th√¥ng\noutput_words: 194\ncomplexity: Unknown\n","output_type":"stream"}],"execution_count":7},{"id":"7d877a8e","cell_type":"code","source":"# Split data: 90% train, 5% validation, 5% test\ntrain_data, temp_data = train_test_split(data, test_size=0.1, random_state=42)\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n\nprint(f\"üìä Train: {len(train_data):,} samples\")\nprint(f\"üìä Validation: {len(val_data):,} samples\")\nprint(f\"üìä Test: {len(test_data):,} samples\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:32:51.829053Z","iopub.execute_input":"2025-11-28T08:32:51.829637Z","iopub.status.idle":"2025-11-28T08:32:51.839746Z","shell.execute_reply.started":"2025-11-28T08:32:51.829617Z","shell.execute_reply":"2025-11-28T08:32:51.839025Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üìä Train: 7,786 samples\nüìä Validation: 433 samples\nüìä Test: 433 samples\n","output_type":"stream"}],"execution_count":8},{"id":"fb6982a6","cell_type":"markdown","source":"## üìù Chat Template\n\nS·ª≠ d·ª•ng chat template chu·∫©n c·ªßa Llama 3.2 Instruct ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi pretrained model:","metadata":{}},{"id":"5aa2a926","cell_type":"code","source":"from unsloth import apply_chat_template\n\n# Chat template chu·∫©n cho Llama 3.2 Instruct\nchat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{OUTPUT}<|eot_id|>\"\"\"\n\ndef format_data_for_chat_template(examples):\n    \"\"\"Format data for Llama 3.2 chat template\"\"\"\n    conversations = []\n    \n    for instruction, input_text, output in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n        # Combine instruction and input as the user message\n        user_message = f\"{instruction}\\n\\n{input_text}\" if input_text else instruction\n        \n        conversation = [\n            {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. H√£y tr·∫£ l·ªùi ch√≠nh x√°c v√† chi ti·∫øt c√°c c√¢u h·ªèi v·ªÅ lu·∫≠t giao th√¥ng.\"},\n            {\"role\": \"user\", \"content\": user_message},\n            {\"role\": \"assistant\", \"content\": output}\n        ]\n        conversations.append(conversation)\n    \n    return {\"conversations\": conversations}\n\n# Convert to HuggingFace Dataset\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\ntest_dataset = Dataset.from_list(test_data)\n\n# Format data for chat template\ntrain_dataset = train_dataset.map(format_data_for_chat_template, batched=True)\nval_dataset = val_dataset.map(format_data_for_chat_template, batched=True)\n\n# Apply chat template using Unsloth\ntrain_dataset = apply_chat_template(\n    train_dataset,\n    tokenizer=tokenizer,\n    chat_template=chat_template,\n    default_system_message=\"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. H√£y tr·∫£ l·ªùi ch√≠nh x√°c v√† chi ti·∫øt c√°c c√¢u h·ªèi v·ªÅ lu·∫≠t giao th√¥ng.\"\n)\n\nval_dataset = apply_chat_template(\n    val_dataset,\n    tokenizer=tokenizer,\n    chat_template=chat_template,\n    default_system_message=\"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. H√£y tr·∫£ l·ªùi ch√≠nh x√°c v√† chi ti·∫øt c√°c c√¢u h·ªèi v·ªÅ lu·∫≠t giao th√¥ng.\"\n)\n\nprint(\"‚úÖ Data formatted with Llama 3.2 chat template\")\nprint(\"\\nüìù Example formatted conversation:\")\nprint(train_dataset[0]['text'][:500] + \"...\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:32:54.327332Z","iopub.execute_input":"2025-11-28T08:32:54.328083Z","iopub.status.idle":"2025-11-28T08:32:56.314396Z","shell.execute_reply.started":"2025-11-28T08:32:54.328056Z","shell.execute_reply":"2025-11-28T08:32:56.313627Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7786 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19b37a8b7b5243958a0163377de61855"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/433 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65e8a2a04fa944468c447bdb30fd9e88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7786 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a57e76095c9d4f1a9258394f889dc763"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/433 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e781c5fcc44c7a9e1b8bee2871e79c"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Data formatted with Llama 3.2 chat template\n\nüìù Example formatted conversation:\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nB·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. H√£y tr·∫£ l·ªùi ch√≠nh x√°c v√† chi ti·∫øt c√°c c√¢u h·ªèi v·ªÅ lu·∫≠t giao th√¥ng.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nTr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\n\nT√†i s·∫£n n√†o c·ªßa c√° nh√¢n kh√¥ng ƒë∆∞·ª£c ti·∫øn h√†nh thi h√†nh √°n?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nCƒÉn c·ª© quy ƒë·ªãnh kho·∫£n 2 ƒêi·ªÅu 87 Lu·∫≠t Thi h√†nh √°n d√¢n s·ª± 2008 quy ƒë·ªãnh v·ªÅ t√†i s·∫£n c·ªßa c√° nh√¢n kh√¥ng ƒë∆∞·ª£c ti·∫øn h√†nh thi h...\n","output_type":"stream"}],"execution_count":9},{"id":"dba40d90","cell_type":"markdown","source":"## üéì Training Configuration\n\n### Optimized for Kaggle T4 (30h/week limit):\n- **Epochs**: 3 (sufficient for legal domain)\n- **Batch size**: 4 per device (max for T4 16GB)\n- **Gradient accumulation**: 4 steps (effective batch = 16)\n- **Learning rate**: 2e-4 (standard for LoRA)\n- **Warmup**: 10% of steps\n- **FP16**: Enabled for speed\n- **Gradient checkpointing**: Unsloth optimized","metadata":{}},{"id":"25e73242","cell_type":"code","source":"# Training arguments optimized for Kaggle T4\ntraining_args = TrainingArguments(\n    # Output & Logging\n    output_dir=\"./outputs\",\n    run_name=\"llama3.2-3b-traffic-law-v1\",\n    \n    # Training dynamics\n    num_train_epochs=2,  # 2-3 epochs is usually enough\n    per_device_train_batch_size=4,  # Reduced from 4 to better utilize VRAM\n    gradient_accumulation_steps=4,  # Increased to maintain effective batch size = 16\n    \n    # Optimization\n    optim=\"adamw_8bit\",  # 8-bit AdamW for memory efficiency\n    learning_rate=2e-4,  # Standard for LoRA fine-tuning\n    weight_decay=0.01,\n    warmup_ratio=0.1,  # 10% warmup\n    lr_scheduler_type=\"cosine\",  # Cosine annealing\n    \n    # Performance\n    fp16=not torch.cuda.is_bf16_supported(),  # Use FP16 for T4\n    bf16=torch.cuda.is_bf16_supported(),  # Use BF16 if supported (A100, H100)\n    \n    # Logging & Saving (more frequent for better monitoring)\n    logging_steps=5,  # Log every 5 steps for better visibility\n    logging_strategy=\"steps\",\n    logging_first_step=True,  # Log first step\n    save_strategy=\"steps\",\n    save_steps=50,  # Save more frequently\n    save_total_limit=3,  # Keep only 3 best checkpoints\n    \n    # Evaluation\n    eval_strategy=\"steps\",\n    eval_steps=50,  # Evaluate more frequently\n    eval_accumulation_steps=1,  # Accumulate eval predictions\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    \n    # WandB integration with detailed logging\n    report_to=\"wandb\",\n    logging_nan_inf_filter=True,  # Filter out NaN/Inf values\n    include_inputs_for_metrics=False,  # Don't log inputs (save space)\n    \n    # Progress bar and output control\n    disable_tqdm=False,  # Enable progress bar\n    log_level=\"info\",  # Show info messages\n    log_level_replica=\"warning\",\n    log_on_each_node=True,\n    dataloader_num_workers=2,\n)\n\nprint(\"‚úÖ Training arguments configured\")\nprint(f\"üíæ Per device batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"üìä Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"üìà Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"üìä Logging every {training_args.logging_steps} steps\")\nprint(f\"üìä Evaluating every {training_args.eval_steps} steps\")\nprint(f\"üïê Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:34:39.183476Z","iopub.execute_input":"2025-11-28T08:34:39.183806Z","iopub.status.idle":"2025-11-28T08:34:39.222211Z","shell.execute_reply.started":"2025-11-28T08:34:39.183766Z","shell.execute_reply":"2025-11-28T08:34:39.221448Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ Training arguments configured\nüíæ Per device batch size: 4\nüìä Effective batch size: 16\nüìà Gradient accumulation: 4\nüìä Logging every 5 steps\nüìä Evaluating every 50 steps\nüïê Total training steps: 972\n","output_type":"stream"}],"execution_count":10},{"id":"ee4ad7fa","cell_type":"code","source":"# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,  # Can make training 5x faster for short sequences\n    args=training_args,\n)\n\nprint(\"‚úÖ Trainer initialized\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:34:48.178204Z","iopub.execute_input":"2025-11-28T08:34:48.178775Z","iopub.status.idle":"2025-11-28T08:35:03.175713Z","shell.execute_reply.started":"2025-11-28T08:34:48.178755Z","shell.execute_reply":"2025-11-28T08:35:03.174827Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/7786 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f16c0fa3044aea9054b3c8ed250ac4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/433 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53cf359fdc3c4f10ac4f91241a792ab1"}},"metadata":{}},{"name":"stderr","text":"Using auto half precision backend\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Trainer initialized\n","output_type":"stream"}],"execution_count":12},{"id":"5ed48c48","cell_type":"markdown","source":"## üöÄ Start Training!\n\n**Estimated time on T4**: ~3-4 hours for 3 epochs  \n**Memory usage**: ~14-15GB VRAM  \n**Kaggle time budget**: ~4h / 30h week (leaves 26h for experiments)","metadata":{}},{"id":"bd1deacc","cell_type":"code","source":"# Show GPU stats before training\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"üñ•Ô∏è GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"üíæ {start_gpu_memory} GB of memory reserved.\")\n\n# Start training\nprint(\"\\nüöÄ Starting training...\\n\")\ntrainer_stats = trainer.train()\n\n# Show final stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"‚úÖ TRAINING COMPLETED!\")\nprint(\"=\"*50)\nprint(f\"‚è±Ô∏è Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\nprint(f\"üíæ Peak reserved memory: {used_memory} GB\")\nprint(f\"üìä Memory used for training: {used_memory_for_lora} GB\")\nprint(f\"üìà Percentage of max memory: {used_percentage}%\")\nprint(f\"üéØ Final train loss: {trainer_stats.metrics['train_loss']:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:35:38.588189Z","iopub.execute_input":"2025-11-28T08:35:38.588771Z","iopub.status.idle":"2025-11-28T13:23:18.227239Z","shell.execute_reply.started":"2025-11-28T08:35:38.588743Z","shell.execute_reply":"2025-11-28T13:23:18.226453Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üñ•Ô∏è GPU = Tesla T4. Max memory = 14.741 GB.\nüíæ 3.07 GB of memory reserved.\n\nüöÄ Starting training...\n\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the Training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\nskipped Embedding(128256, 3072, padding_idx=128004): 375.75M params\nskipped: 375.75M params\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 7,786 | Num Epochs = 2 | Total steps = 488\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n \"-____-\"     Trainable parameters = 48,627,712 of 3,261,377,536 (1.49% trained)\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='488' max='488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [488/488 4:46:54, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.442100</td>\n      <td>1.378838</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.231800</td>\n      <td>1.208347</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.194700</td>\n      <td>1.139060</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.101200</td>\n      <td>1.090938</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.027300</td>\n      <td>1.052491</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.982300</td>\n      <td>1.024429</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.953000</td>\n      <td>1.002104</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.985600</td>\n      <td>0.989309</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.970900</td>\n      <td>0.984937</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\nUnsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\nSaving model checkpoint to ./outputs/checkpoint-50\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nThe following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\nSaving model checkpoint to ./outputs/checkpoint-100\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nThe following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\nSaving model checkpoint to ./outputs/checkpoint-150\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nThe following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\nSaving model checkpoint to ./outputs/checkpoint-200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [outputs/checkpoint-50] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\nSaving model checkpoint to ./outputs/checkpoint-250\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [outputs/checkpoint-100] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\nSaving model checkpoint to ./outputs/checkpoint-300\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [outputs/checkpoint-150] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\nSaving model checkpoint to ./outputs/checkpoint-350\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [outputs/checkpoint-200] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\nSaving model checkpoint to ./outputs/checkpoint-400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [outputs/checkpoint-250] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\nSaving model checkpoint to ./outputs/checkpoint-450\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [outputs/checkpoint-300] due to args.save_total_limit\nSaving model checkpoint to ./outputs/checkpoint-488\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/bb1d317a108579fb40e646af8924a5e7ec5604b1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 24,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.2\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [outputs/checkpoint-350] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./outputs/checkpoint-450 (score: 0.9849370121955872).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñá</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.98494</td></tr><tr><td>eval/runtime</td><td>217.7863</td></tr><tr><td>eval/samples_per_second</td><td>1.988</td></tr><tr><td>eval/steps_per_second</td><td>0.253</td></tr><tr><td>total_flos</td><td>2.3628365978903347e+17</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>488</td></tr><tr><td>train/grad_norm</td><td>0.28392</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8984</td></tr><tr><td>train_loss</td><td>1.12404</td></tr><tr><td>train_runtime</td><td>17256.8443</td></tr><tr><td>train_samples_per_second</td><td>0.902</td></tr><tr><td>train_steps_per_second</td><td>0.028</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">llama3.2-3b-traffic-law-v1</strong> at: <a href='https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai/runs/6tqwc20g' target=\"_blank\">https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai/runs/6tqwc20g</a><br> View project at: <a href='https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai' target=\"_blank\">https://wandb.ai/mikeethanh04-student/vietnamese-legal-ai</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251128_083150-6tqwc20g/logs</code>"},"metadata":{}},{"name":"stdout","text":"\n==================================================\n‚úÖ TRAINING COMPLETED!\n==================================================\n‚è±Ô∏è Training time: 17256.84 seconds\nüíæ Peak reserved memory: 11.223 GB\nüìä Memory used for training: 8.153 GB\nüìà Percentage of max memory: 76.135%\nüéØ Final train loss: 1.1240\n","output_type":"stream"}],"execution_count":13},{"id":"86713df2","cell_type":"markdown","source":"## üìä Evaluation","metadata":{}},{"id":"b56ff1e0","cell_type":"code","source":"# Evaluate on validation set\nprint(\"üìä Evaluating on validation set...\\n\")\neval_results = trainer.evaluate()\n\nprint(\"=\"*50)\nprint(\"VALIDATION RESULTS\")\nprint(\"=\"*50)\nfor key, value in eval_results.items():\n    print(f\"{key}: {value:.4f}\" if isinstance(value, float) else f\"{key}: {value}\")\n\n# Log to WandB\nwandb.log({\"final_eval_loss\": eval_results['eval_loss']})","metadata":{"execution":{"iopub.status.busy":"2025-11-28T13:24:01.447009Z","iopub.execute_input":"2025-11-28T13:24:01.447329Z","iopub.status.idle":"2025-11-28T13:27:39.611344Z","shell.execute_reply.started":"2025-11-28T13:24:01.447302Z","shell.execute_reply":"2025-11-28T13:27:39.610234Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity. If input, domains, output_words, instruction, traffic_type, output, text, attention_mask, conversations, complexity are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 433\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"üìä Evaluating on validation set...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [55/55 03:35]\n    </div>\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1318450365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìä Evaluating on validation set...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4491\u001b[0m         )\n\u001b[1;32m   4492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4493\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, logs, start_time)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, logs, start_time)\u001b[0m\n\u001b[1;32m   3788\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3789\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_log\u001b[0;34m(self, args, state, control, logs)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_prediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_log\u001b[0;34m(self, args, state, control, model, logs, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0mnon_scalar_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_value_scalars\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0mnon_scalar_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewrite_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_scalar_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnon_scalar_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train/global_step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"],"ename":"Error","evalue":"You must call wandb.init() before wandb.log()","output_type":"error"}],"execution_count":14},{"id":"b14b1fee","cell_type":"markdown","source":"## üß™ Inference Testing","metadata":{}},{"id":"ebb3883f","cell_type":"code","source":"# Enable native 2x faster inference\nFastLanguageModel.for_inference(model)\n\ndef test_model(instruction, input_text, max_new_tokens=512):\n    \"\"\"Test model with chat template format\"\"\"\n    # Create conversation format\n    user_message = f\"{instruction}\\n\\n{input_text}\" if input_text else instruction\n    \n    messages = [\n        {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam. H√£y tr·∫£ l·ªùi ch√≠nh x√°c v√† chi ti·∫øt c√°c c√¢u h·ªèi v·ªÅ lu·∫≠t giao th√¥ng.\"},\n        {\"role\": \"user\", \"content\": user_message}\n    ]\n    \n    # Apply chat template\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n    \n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=0.7,\n        top_p=0.9,\n        do_sample=True,\n        use_cache=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    response = tokenizer.batch_decode(outputs)[0]\n    # Extract only the response part (after the last assistant header)\n    if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n        response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n        response = response.split(\"<|eot_id|>\")[0].strip()\n    \n    return response\n\n# Test with samples from test set\nprint(\"üß™ Testing model on random samples...\\n\")\nprint(\"=\"*80)\n\nimport random\ntest_samples = random.sample(test_data, 3)\n\nfor i, sample in enumerate(test_samples, 1):\n    print(f\"\\n{'='*80}\")\n    print(f\"TEST SAMPLE #{i}\")\n    print(f\"{'='*80}\")\n    print(f\"\\nüìù Instruction: {sample['instruction']}\")\n    print(f\"\\n‚ùì Input: {sample['input']}\")\n    print(f\"\\nüéØ Expected Output:\\n{sample['output'][:300]}...\")\n    \n    # Generate response\n    response = test_model(sample['instruction'], sample['input'])\n    print(f\"\\nü§ñ Model Response:\\n{response}\")\n    print(f\"\\n{'='*80}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T13:27:45.759440Z","iopub.execute_input":"2025-11-28T13:27:45.760112Z","iopub.status.idle":"2025-11-28T13:28:56.759593Z","shell.execute_reply.started":"2025-11-28T13:27:45.760086Z","shell.execute_reply":"2025-11-28T13:28:56.758951Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üß™ Testing model on random samples...\n\n================================================================================\n\n================================================================================\nTEST SAMPLE #1\n================================================================================\n\nüìù Instruction: Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\n\n‚ùì Input: C√°c h√†nh vi n√†o b·ªã c·∫•m trong ho·∫°t ƒë·ªông qu·∫£ng c√°o?\n\nüéØ Expected Output:\nTheo quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 8 Lu·∫≠t Qu·∫£ng c√°o 2012 th√¨ c√°c h√†nh vi c·∫•m trong ho·∫°t ƒë·ªông qu·∫£ng c√°o bao g·ªìm: - Qu·∫£ng c√°o nh·ªØng s·∫£n ph·∫©m, h√†ng h√≥a, d·ªãch v·ª• quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 7 Lu·∫≠t Qu·∫£ng c√°o 2012. - Qu·∫£ng c√°o l√†m ti·∫øt l·ªô b√≠ m·∫≠t nh√† n∆∞·ªõc, ph∆∞∆°ng h·∫°i ƒë·∫øn ƒë·ªôc l·∫≠p, ch·ªß quy·ªÅn qu·ªëc gia, an ninh, qu·ªëc ph√≤ng. - Qu·∫£ng...\n\nü§ñ Model Response:\nCƒÉn c·ª© ƒêi·ªÅu 8 Lu·∫≠t Qu·∫£ng c√°o 2012 quy ƒë·ªãnh c√°c h√†nh vi b·ªã c·∫•m trong ho·∫°t ƒë·ªông qu·∫£ng c√°o nh∆∞ sau: - Qu·∫£ng c√°o c√≥ n·ªôi dung x√¢m ph·∫°m l·ª£i √≠ch qu·ªëc gia, l·ª£i √≠ch c√¥ng c·ªông, an ninh, qu·ªëc ph√≤ng, tr·∫≠t t·ª±, an to√†n x√£ h·ªôi, nh√¢n ph·∫©m, danh d·ª±, nh√¢n ph·∫©m, uy t√≠n c·ªßa t·ªï ch·ª©c, danh d·ª±, nh√¢n ph·∫©m, uy t√≠n c·ªßa c√° nh√¢n. - Qu·∫£ng c√°o c√≥ n·ªôi dung c√≥ t√≠nh ch·∫•t ƒëe d·ªça, x√∫c ph·∫°m uy t√≠n c·ªßa c∆° quan, t·ªï ch·ª©c, danh d·ª±, nh√¢n ph·∫©m, uy t√≠n c·ªßa c√° nh√¢n. - Qu·∫£ng c√°o c√≥ t√≠nh ch·∫•t k√≠ch ƒë·ªông b·∫°o l·ª±c, b·∫°o l·ª±c, t·ªôi ph·∫°m. - Qu·∫£ng c√°o c√≥ n·ªôi dung x√∫c ph·∫°m t√¥n gi√°o, t√¥n gi√°o, t√≠n ng∆∞·ª°ng, vƒÉn h√≥a, l·ªãch s·ª≠. - Qu·∫£ng c√°o c√≥ t√≠nh ch·∫•t k√≠ch ƒë·ªông quan h·ªá gia ƒë√¨nh, quan h·ªá ng∆∞·ªùi l·ªõn, ng∆∞·ªùi ch∆∞a th√†nh ni√™n, tr·∫ª em. - Qu·∫£ng c√°o c√≥ t√≠nh ch·∫•t k√≠ch ƒë·ªông b·∫°o l·ª±c, h√†nh hung, h√†nh vi b·∫°o l·ª±c ƒë·ªëi v·ªõi tr·∫ª em. - Qu·∫£ng c√°o c√≥ n·ªôi dung c∆∞·ª°ng ƒëo·∫°t, x√¢m ph·∫°m quy·ªÅn s·ªü h·ªØu tr√≠ tu·ªá, quy·ªÅn s·ªü h·ªØu c√¥ng nghi·ªáp, quy·ªÅn ƒë·ªëi v·ªõi quy·ªÅn t√°c gi·∫£, quy·ªÅn li√™n quan ƒë·∫øn quy·ªÅn t√°c gi·∫£, quy·ªÅn s·ªü h·ªØu c√¥ng nghi·ªáp, quy·ªÅn ƒë·ªëi v·ªõi quy·ªÅn t√°c gi·∫£, quy·ªÅn li√™n quan ƒë·∫øn quy·ªÅn t√°c gi·∫£. - Qu·∫£ng c√°o c√≥ n·ªôi dung l·ª£i d·ª•ng danh nghƒ©a c∆° quan, t·ªï ch·ª©c, c√° nh√¢n, s·∫£n ph·∫©m, h√†ng h√≥a, d·ªãch v·ª• ƒë√£ ƒë∆∞·ª£c c∆° quan c√≥ th·∫©m quy·ªÅn ch·∫•p thu·∫≠n. - Qu·∫£ng c√°o c√≥ t√≠nh ch·∫•t g√¢y thi·ªát h·∫°i cho m√¥i tr∆∞·ªùng. - Qu·∫£ng c√°o c√≥ n·ªôi dung kh√¥ng ph√π h·ª£p v·ªõi l·ªìng gh√©p tr√™n ph∆∞∆°ng ti·ªán qu·∫£ng c√°o. - Qu·∫£ng c√°o c√≥ t√≠nh ch·∫•t l√†m nh·ª•c Nh√† n∆∞·ªõc, l√£nh ƒë·∫°o ƒê·∫£ng, l√£nh ƒë·∫°o n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam, ng∆∞·ªùi ƒë·ª©ng ƒë·∫ßu, l√£nh ƒë·∫°o v√† c√°n b·ªô c·ªßa c∆° quan, t·ªï ch·ª©c, c√° nh√¢n. - Qu·∫£ng c√°o c√≥ t√≠nh ch·∫•t l√†m nh·ª•c t·ªï ch·ª©c, c√° nh√¢n, s·∫£n ph·∫©m, h√†ng h√≥a, d·ªãch v·ª• ƒë∆∞·ª£c b·∫£o h·ªô quy·ªÅn t√°c gi·∫£, quy·ªÅn s·ªü h·ªØu c√¥ng nghi·ªáp. - Qu·∫£ng c√°o g√¢y ·∫£nh h∆∞·ªüng x·∫•u ƒë·∫øn an to√†n th·ª±c ph·∫©m. - Qu·∫£ng c√°o c√≥ n·ªôi dung l·ª£i d·ª•ng thi√™n tai, d·ªãch b·ªánh, tai n·∫°n giao th√¥ng, kh·ªßng b·ªë, an ninh, tr·∫≠t t·ª±, an to√†n x√£ h·ªôi. - Qu·∫£ng c√°o c√≥ n·ªôi dung l·ª£i d·ª•ng s·ª± ki·ªán l·ªãch s·ª≠. - Qu·∫£ng c√°o c√≥ t√≠nh ch·∫•t ƒëe d·ªça an\n\n================================================================================\n\n================================================================================\nTEST SAMPLE #2\n================================================================================\n\nüìù Instruction: Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\n\n‚ùì Input: Di·ªÖn vi√™n ƒëi·ªán ·∫£nh c√≥ ƒë∆∞·ª£c h∆∞·ªüng quy·ªÅn t√°c gi·∫£ hay kh√¥ng?\n\nüéØ Expected Output:\nCƒÉn c·ª© theo kho·∫£n 1 ƒêi·ªÅu 21 Lu·∫≠t S·ªü h·ªØu tr√≠ tu·ªá 2005 ƒë∆∞·ª£c s·ª≠a ƒë·ªïi b·ªüi kho·∫£n 5 ƒêi·ªÅu 1 Lu·∫≠t S·ªü h·ªØu tr√≠ tu·ªá s·ª≠a ƒë·ªïi 2022 nh∆∞ sau: CƒÉn c·ª© theo ƒêi·ªÅu 10 Ngh·ªã ƒë·ªãnh 17/2023/Nƒê-CP quy ƒë·ªãnh v·ªÅ quy·ªÅn t√°c gi·∫£ ƒë·ªëi v·ªõi t√°c ph·∫©m ƒëi·ªán ·∫£nh V·ªÅ nguy√™n t·∫Øc, t√°c ph·∫©m ƒëi·ªán ·∫£nh l√† t√°c ph·∫©m ƒë∆∞·ª£c th·∫ø hi·ªán b·∫±ng h√¨nh ·∫£nh ƒë·ªông...\n\nü§ñ Model Response:\nTheo ƒêi·ªÅu 7 Lu·∫≠t S·ªü h·ªØu tr√≠ tu·ªá 2005 ƒë∆∞·ª£c s·ª≠a ƒë·ªïi b·ªüi Kho·∫£n 3 ƒêi·ªÅu 1 Lu·∫≠t S·ªü h·ªØu tr√≠ tu·ªá s·ª≠a ƒë·ªïi 2022 quy ƒë·ªãnh v·ªÅ quy·ªÅn t√°c gi·∫£ nh∆∞ sau: Quy·ªÅn t√°c gi·∫£ 1. Ng∆∞·ªùi s√°ng t·∫°o ra t√°c ph·∫©m ƒë∆∞·ª£c h∆∞·ªüng quy·ªÅn t√°c gi·∫£ v√† quy·ªÅn li√™n quan ƒë·∫øn quy·ªÅn t√°c gi·∫£ theo quy ƒë·ªãnh c·ªßa Lu·∫≠t n√†y. 2. Ch·ªß s·ªü h·ªØu quy·ªÅn t√°c gi·∫£ ƒë∆∞·ª£c h∆∞·ªüng quy·ªÅn sau ƒë√¢y: a) ƒê∆∞·ª£c t√°c gi·∫£ ho·∫∑c ng∆∞·ªùi ƒë∆∞·ª£c ·ªßy quy·ªÅn, trong tr∆∞·ªùng h·ª£p c√≥ th·ªèa thu·∫≠n kh√°c, c√¥ng b·ªë, tr√¨nh b√†y, bi·ªÉu di·ªÖn c√¥ng khai t√°c ph·∫©m tr∆∞·ªõc c√¥ng ch√∫ng; b) ƒê∆∞·ª£c t√°c gi·∫£ ho·∫∑c ng∆∞·ªùi ƒë∆∞·ª£c ·ªßy quy·ªÅn, trong tr∆∞·ªùng h·ª£p c√≥ th·ªèa thu·∫≠n kh√°c, ƒë∆∞a t√°c ph·∫©m v√†o qu√° tr√¨nh l∆∞u tr·ªØ, truy·ªÅn d·∫´n, thu th·∫≠p, s∆∞u t·∫≠p, ph√¢n t√≠ch, nghi√™n c·ª©u, khai th√°c v√† s·ª≠ d·ª•ng t√°c ph·∫©m b·∫±ng b·∫•t k·ª≥ ph∆∞∆°ng ti·ªán hay h√¨nh th·ª©c n√†o, bao g·ªìm c·∫£ vi·ªác cung c·∫•p d·ªãch v·ª•; c) ƒê∆∞·ª£c t√°c gi·∫£ ho·∫∑c ng∆∞·ªùi ƒë∆∞·ª£c ·ªßy quy·ªÅn, trong tr∆∞·ªùng h·ª£p c√≥ th·ªèa thu·∫≠n kh√°c, sao ch√©p tr·ª±c ti·∫øp ho·∫∑c gi√°n ti·∫øp to√†n b·ªô ho·∫∑c m·ªôt ph·∫ßn t√°c ph·∫©m b·∫±ng b·∫•t k·ª≥ ph∆∞∆°ng ti·ªán hay h√¨nh th·ª©c n√†o, tr·ª´ tr∆∞·ªùng h·ª£p quy ƒë·ªãnh t·∫°i ƒëi·ªÉm h kho·∫£n 2 ƒêi·ªÅu n√†y; d) ƒê∆∞·ª£c ch·ªß s·ªü h·ªØu quy·ªÅn t√°c gi·∫£ chuy·ªÉn giao quy·ªÅn s·ª≠ d·ª•ng m·ªôt ph·∫ßn ho·∫∑c to√†n b·ªô quy·ªÅn t√°c gi·∫£ b·∫±ng c√°ch b√°n, cho thu√™, cho m∆∞·ª£n ho·∫∑c c√°c h√¨nh th·ª©c kh√°c; ƒë) Ng∆∞·ªùi ƒë∆∞·ª£c ·ªßy quy·ªÅn th·ª±c hi·ªán m·ªôt ho·∫∑c nhi·ªÅu h√†nh ƒë·ªông quy ƒë·ªãnh t·∫°i c√°c ƒëi·ªÉm a, b, c v√† d kho·∫£n 2 ƒêi·ªÅu n√†y. 3. Ng∆∞·ªùi s√°ng t·∫°o ra t√°c ph·∫©m m√† quy·ªÅn t√°c gi·∫£ c·ªßa m√¨nh ƒë√£ chuy·ªÉn giao cho ng∆∞·ªùi kh√°c th√¨ ng∆∞·ªùi ƒë√≥ kh√¥ng ƒë∆∞·ª£c h∆∞·ªüng quy·ªÅn t√°c gi·∫£ ƒë·ªëi v·ªõi t√°c ph·∫©m ƒë√≥, tr·ª´ tr∆∞·ªùng h·ª£p c√≥ th·ªèa thu·∫≠n kh√°c. 4. Tr∆∞·ªùng h·ª£p t√°c ph·∫©m do ng∆∞·ªùi b·ªã h·∫°n ch·∫ø kh·∫£ nƒÉng ƒëi·ªÅu khi·ªÉn m√† ng∆∞·ªùi ƒë√≥ cho ph√©p th·ª±c hi·ªán th√¨ ng∆∞·ªùi b·ªã h·∫°n ch·∫ø kh·∫£ nƒÉng ƒëi·ªÅu khi·ªÉn ƒë∆∞·ª£c h∆∞·ªüng quy·ªÅn t√°c gi·∫£ ƒë·ªëi v·ªõi t√°c ph·∫©m ƒë√≥. 5. Ng∆∞·ªùi th·ª±c hi·ªán h√†nh ƒë·ªông quy ƒë·ªãnh t·∫°i ƒëi·ªÉm c kho·∫£n 2 ƒêi·ªÅu n√†y ph·∫£i ƒë∆∞·ª£c s·ª± ƒë·ªìng √Ω c·ªßa ch·ªß s·ªü h·ªØu quy·ªÅn t√°c gi·∫£, tr·ª´ tr∆∞·ªùng h·ª£p quy ƒë·ªãnh t·∫°i kho·∫£n 4 ƒêi·ªÅu n√†y. 6. Ch·ªß s·ªü h·ªØu quy·ªÅn t√°c gi·∫£ ƒë∆∞·ª£c h∆∞·ªüng quy·ªÅn li√™n quan ƒë·∫øn quy·ªÅn t√°c gi·∫£ theo quy ƒë·ªãnh c·ªßa Lu·∫≠t n√†y v√† theo th·ªèa thu·∫≠n v·ªõi ng∆∞·ªùi ƒë∆∞·ª£c ·ªßy quy·ªÅn. 7. Trong tr∆∞·ªùng h·ª£p t√°c ph·∫©m do ng∆∞·ªùi ƒë√£ ch·∫øt m√† quy·ªÅn t√°c gi·∫£ c·ªßa ng∆∞·ªùi\n\n================================================================================\n\n================================================================================\nTEST SAMPLE #3\n================================================================================\n\nüìù Instruction: Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\n\n‚ùì Input: Tr√°ch nhi·ªám c·ªßa t·ªï ch·ª©c c√° nh√¢n v·ªÅ c√°c ch·ªâ ti√™u k·ªπ thu·∫≠t, an to√†n s·ª©c kh·ªèe c·ªßa xƒÉng, nhi√™n li·ªáu ƒëi√™zen v√† nhi√™n li·ªáu sinh h·ªçc l√† g√¨?\n\nüéØ Expected Output:\nT·∫°i M·ª•c 5 Quy chu·∫©n k·ªπ thu·∫≠t qu·ªëc gia QCVN 01:2022/BKHCN tr√°ch nhi·ªám c·ªßa t·ªï ch·ª©c c√° nh√¢n v·ªÅ xƒÉng, nhi√™n li·ªáu ƒëi√™zen v√† nhi√™n li·ªáu sinh h·ªçc nh∆∞ sau: - T·ªï ch·ª©c, c√° nh√¢n s·∫£n xu·∫•t, pha ch·∫ø v√† nh·∫≠p kh·∫©u xƒÉng kh√¥ng ch√¨, xƒÉng E5, xƒÉng E10, nhi√™n li·ªáu ƒëi√™zen, nhi√™n li·ªáu ƒëi√™zen B5, etanol nhi√™n li·ªáu bi·∫øn t√≠n...\n\nü§ñ Model Response:\nCƒÉn c·ª© theo ƒêi·ªÅu 7 Ngh·ªã ƒë·ªãnh 32/2022/Nƒê-CP, tr√°ch nhi·ªám c·ªßa t·ªï ch·ª©c c√° nh√¢n v·ªÅ c√°c ch·ªâ ti√™u k·ªπ thu·∫≠t, an to√†n s·ª©c kh·ªèe c·ªßa xƒÉng, nhi√™n li·ªáu ƒëi√™zen v√† nhi√™n li·ªáu sinh h·ªçc nh∆∞ sau: - Th·ª±c hi·ªán vi·ªác c·∫•p, l∆∞u tr·ªØ, v·∫≠n chuy·ªÉn, s·ª≠ d·ª•ng xƒÉng, d·∫ßu, nhi√™n li·ªáu ƒëi√™zen, nhi√™n li·ªáu sinh h·ªçc ƒë√°p ·ª©ng c√°c y√™u c·∫ßu v·ªÅ an to√†n s·ª©c kh·ªèe, ch·∫•t l∆∞·ª£ng, ti√™u chu·∫©n k·ªπ thu·∫≠t, m√¥i tr∆∞·ªùng, an to√†n v·ªÅ ch√°y, n·ªï, an to√†n v·ªÅ m√¥i tr∆∞·ªùng theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t v√† quy ƒë·ªãnh c·ªßa c∆° quan qu·∫£n l√Ω nh√† n∆∞·ªõc c√≥ th·∫©m quy·ªÅn; - Th·ª±c hi·ªán vi·ªác v·∫≠n chuy·ªÉn, s·ª≠ d·ª•ng xƒÉng, d·∫ßu, nhi√™n li·ªáu ƒëi√™zen, nhi√™n li·ªáu sinh h·ªçc ƒë√°p ·ª©ng c√°c y√™u c·∫ßu v·ªÅ an to√†n s·ª©c kh·ªèe, ch·∫•t l∆∞·ª£ng, ti√™u chu·∫©n k·ªπ thu·∫≠t, m√¥i tr∆∞·ªùng, an to√†n v·ªÅ ch√°y, n·ªï, an to√†n v·ªÅ m√¥i tr∆∞·ªùng theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t v√† quy ƒë·ªãnh c·ªßa c∆° quan qu·∫£n l√Ω nh√† n∆∞·ªõc c√≥ th·∫©m quy·ªÅn.\n\n================================================================================\n","output_type":"stream"}],"execution_count":15},{"id":"4d2678b9","cell_type":"markdown","source":"## üíæ Save Model","metadata":{}},{"id":"8b71b81d","cell_type":"code","source":"# Save LoRA adapters (only ~100-200MB!)\n#model.save_pretrained(\"vietnamese_legal_lora\")\n#tokenizer.save_pretrained(\"vietnamese_legal_lora\")\n\n#rint(\"‚úÖ LoRA adapters saved to: vietnamese_legal_lora/\")\n#print(\"üì¶ Size: ~100-200MB (adapters only)\")\n\n# Optional: Save merged model (full size ~6GB)\nmodel.save_pretrained_merged(\"vietnamese_legal_merged\", tokenizer, save_method=\"merged_16bit\")\nprint(\"‚úÖ Merged model saved to: vietnamese_legal_merged/\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T13:28:56.760754Z","iopub.execute_input":"2025-11-28T13:28:56.761014Z","iopub.status.idle":"2025-11-28T13:30:03.772826Z","shell.execute_reply.started":"2025-11-28T13:28:56.760998Z","shell.execute_reply":"2025-11-28T13:30:03.772051Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d386c67f69a24f58951de4053b77a1f8"}},"metadata":{}},{"name":"stderr","text":"Configuration saved in vietnamese_legal_merged/config.json\n","output_type":"stream"},{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f28f16e425e043efaa9cdb3def18b428"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd3fd9b5f5846b79dd9672e291b6722"}},"metadata":{}},{"name":"stdout","text":"Checking cache directory for required files...\nCache check failed: model-00001-of-00002.safetensors not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\nChecking cache directory for required files...\nCache check failed: tokenizer.model not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f9afa0acd445f2a87017fb9fcaf6e0"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:13<00:13, 13.25s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"562e3fb6fa214865a708d3583276d4a0"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:22<00:00, 11.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:41<00:00, 20.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merge process complete. Saved to `/kaggle/working/vietnamese_legal_merged`\n‚úÖ Merged model saved to: vietnamese_legal_merged/\n","output_type":"stream"}],"execution_count":16},{"id":"429916aa-a0c2-47e4-8cb1-4bf134cf70b9","cell_type":"code","source":"import os\nimport shutil\nfrom google.colab import files\n\n# Detect environment\nis_colab = \"COLAB_\" in \"\".join(os.environ.keys())\nis_kaggle = os.path.exists(\"/kaggle/working\")\n\nif is_colab:\n    print(\"üîç Detected: Google Colab\")\n    print(\"üì¶ Compressing models for download...\\n\")\n    \n    from google.colab import files\n    \n    # Zip LoRA adapters\n    if os.path.exists(\"vietnamese_legal_lora\"):\n        shutil.make_archive('vietnamese_legal_lora', 'zip', 'vietnamese_legal_lora')\n        print(\"‚úÖ LoRA adapters zipped: vietnamese_legal_lora.zip (~100-200MB)\")\n        files.download('vietnamese_legal_lora.zip')\n    \n    # Zip merged model (if exists)\nif os.path.exists(\"vietnamese_legal_merged\"):\n        print(\"\\n‚ö†Ô∏è Merged model is ~6GB, this may take a while...\")\n        zip_path = shutil.make_archive('vietnamese_legal_merged', 'zip', 'vietnamese_legal_merged')\n        print(f\"‚úÖ Merged model zipped: {zip_path} (~6GB)\")\n        if files:\n            files.download(zip_path)\n        else:\n            print(f\"üìÇ File saved at: {os.path.abspath(zip_path)}\")\n    \n\nelif is_kaggle:\n    print(\"üîç Detected: Kaggle\")\n    print(\"üìÇ Models are saved in the output directory.\")\n    print(\"‚ÑπÔ∏è After notebook execution completes:\")\n    print(\"  1. Go to 'Output' tab on the right\")\n    print(\"  2. Download the following folders:\")\n    print(\"     - vietnamese_legal_lora/ (~100-200MB)\")\n    print(\"     - vietnamese_legal_merged/ (~6GB, if saved)\")\n    print(\"     - *.gguf files (if exported)\")\n    print(\"\\nüí° Tip: Models in the output directory will be available for 7 days.\")\n\nelse:\n    print(\"üîç Detected: Local environment\")\n    print(\"üìÇ Models saved at:\")\n    \n    if os.path.exists(\"vietnamese_legal_lora\"):\n        lora_path = os.path.abspath(\"vietnamese_legal_lora\")\n        print(f\"  ‚úÖ LoRA adapters: {lora_path}\")\n    \n    if os.path.exists(\"vietnamese_legal_merged\"):\n        merged_path = os.path.abspath(\"vietnamese_legal_merged\")\n        print(f\"  ‚úÖ Merged model: {merged_path}\")\n    \n    gguf_files = [f for f in os.listdir('.') if f.endswith('.gguf')]\n    if gguf_files:\n        print(f\"  ‚úÖ GGUF models: {os.path.abspath('.')}\")\n        for gguf_file in gguf_files:\n            print(f\"     - {gguf_file}\")\n    \n    print(\"\\n‚úÖ Models are already on your local machine!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:31:37.257987Z","iopub.status.idle":"2025-11-28T08:31:37.258582Z","shell.execute_reply.started":"2025-11-28T08:31:37.258416Z","shell.execute_reply":"2025-11-28T08:31:37.258431Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"4a85c2fe-51ad-4093-912b-196f4d0dc670","cell_type":"code","source":"# Upload merged model to HuggingFace Hub (gi·∫£i ph√°p cho file l·ªõn!)\n# B∆∞·ªõc 1: T·∫°o HuggingFace account t·∫°i https://huggingface.co/join\n# B∆∞·ªõc 2: T·∫°o token t·∫°i https://huggingface.co/settings/tokens (ch·ªçn \"Write\" permission)\n# B∆∞·ªõc 3: Th√™m token v√†o Kaggle Secrets v·ªõi key \"HF_TOKEN\"\n\nimport os\n\nif os.path.exists(\"/kaggle/working\"):\n    print(\"üöÄ Uploading model to HuggingFace Hub...\")\n    print(\"=\"*70)\n    \n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n        \n        from huggingface_hub import HfApi, login\n        \n        # Login to HuggingFace\n        login(token=hf_token)\n        print(\"‚úÖ Logged in to HuggingFace\")\n        \n        # Thay YOUR_USERNAME b·∫±ng username HuggingFace c·ªßa b·∫°n\n        YOUR_HF_USERNAME = \"mikeethanh\"  # ‚ö†Ô∏è S·ª¨A D√íNG N√ÄY!\n        repo_name = f\"{YOUR_HF_USERNAME}/vietnamese-legal-llama3.2-3b-merged-sft-v1\"\n        \n        print(f\"\\nüì§ Uploading to: {repo_name}\")\n        print(\"‚è≥ ƒêang upload ~6GB, c√≥ th·ªÉ m·∫•t 10-15 ph√∫t...\\n\")\n        \n        # Upload merged model\n        if os.path.exists(\"vietnamese_legal_merged\"):\n            from huggingface_hub import create_repo, upload_folder\n            \n            # Create repo (public)\n            try:\n                create_repo(repo_name, repo_type=\"model\", exist_ok=True)\n                print(f\"‚úÖ Repository created: https://huggingface.co/{repo_name}\")\n            except:\n                print(f\"‚ÑπÔ∏è Repository already exists: https://huggingface.co/{repo_name}\")\n            \n            # Upload folder\n            upload_folder(\n                folder_path=\"vietnamese_legal_merged\",\n                repo_id=repo_name,\n                commit_message=\"Vietnamese Legal AI - Llama 3.2 3B Merged Model\",\n            )\n            \n            print(\"\\n\" + \"=\"*70)\n            print(\"‚úÖ UPLOAD TH√ÄNH C√îNG!\")\n            print(\"=\"*70)\n            print(f\"\\nüì• Download model v·ªÅ m√°y b·∫±ng c√°ch:\")\n            print(f\"   git clone https://huggingface.co/{repo_name}\")\n            print(f\"\\nüåê Ho·∫∑c xem tr√™n web:\")\n            print(f\"   https://huggingface.co/{repo_name}\")\n            print(\"\\nüí° Model ƒë√£ public, ai c≈©ng c√≥ th·ªÉ download!\")\n        else:\n            print(\"‚ö†Ô∏è Folder 'vietnamese_legal_merged' not found!\")\n            \n    except Exception as e:\n        print(f\"‚ùå Error: {e}\")\n        print(\"\\nüìù H∆∞·ªõng d·∫´n fix:\")\n        print(\"  1. T·∫°o account t·∫°i: https://huggingface.co/join\")\n        print(\"  2. T·∫°o token t·∫°i: https://huggingface.co/settings/tokens\")\n        print(\"  3. Kaggle: Add-ons ‚Üí Secrets ‚Üí Add 'HF_TOKEN'\")\n        print(\"  4. S·ª≠a YOUR_USERNAME trong code\")\n        \nelse:\n    print(\"‚ÑπÔ∏è This cell only works on Kaggle\")\n    print(\"üí° For local, use: model.push_to_hub() directly\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T13:32:06.618323Z","iopub.execute_input":"2025-11-28T13:32:06.618652Z","iopub.status.idle":"2025-11-28T13:33:10.470517Z","shell.execute_reply.started":"2025-11-28T13:32:06.618626Z","shell.execute_reply":"2025-11-28T13:33:10.469892Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üöÄ Uploading model to HuggingFace Hub...\n======================================================================\n‚úÖ Logged in to HuggingFace\n\nüì§ Uploading to: mikeethanh/vietnamese-legal-llama3.2-3b-merged-sft-v1\n‚è≥ ƒêang upload ~6GB, c√≥ th·ªÉ m·∫•t 10-15 ph√∫t...\n\n‚úÖ Repository created: https://huggingface.co/mikeethanh/vietnamese-legal-llama3.2-3b-merged-sft-v1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f913391560134eec8cd10ad3f622e5ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb570986ba6f49519acac474da787beb"}},"metadata":{}},{"name":"stdout","text":"\n======================================================================\n‚úÖ UPLOAD TH√ÄNH C√îNG!\n======================================================================\n\nüì• Download model v·ªÅ m√°y b·∫±ng c√°ch:\n   git clone https://huggingface.co/mikeethanh/vietnamese-legal-llama3.2-3b-merged-sft-v1\n\nüåê Ho·∫∑c xem tr√™n web:\n   https://huggingface.co/mikeethanh/vietnamese-legal-llama3.2-3b-merged-sft-v1\n\nüí° Model ƒë√£ public, ai c≈©ng c√≥ th·ªÉ download!\n","output_type":"stream"}],"execution_count":19},{"id":"10556004","cell_type":"markdown","source":"## üì§ Push to HuggingFace Hub (Optional)","metadata":{}},{"id":"4c6cb67f","cell_type":"code","source":"# Uncomment to push to HuggingFace Hub\n# You need to login first: huggingface-cli login\n\n# model.push_to_hub(\n#     \"your-username/vietnamese-legal-llama3.2-3b-lora\",\n#     token=\"your_hf_token\",\n#     commit_message=\"Vietnamese Legal AI - Traffic Law QA\"\n# )\n# tokenizer.push_to_hub(\n#     \"your-username/vietnamese-legal-llama3.2-3b-lora\",\n#     token=\"your_hf_token\"\n# )\n\n# print(\"‚úÖ Model pushed to HuggingFace Hub!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:31:37.260965Z","iopub.status.idle":"2025-11-28T08:31:37.261274Z","shell.execute_reply.started":"2025-11-28T08:31:37.261129Z","shell.execute_reply":"2025-11-28T08:31:37.261147Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"5aa8e8a4","cell_type":"markdown","source":"## üìä Quantization Export (for deployment)","metadata":{}},{"id":"48c7b379","cell_type":"code","source":"# Export to GGUF for llama.cpp / Ollama deployment\n# Uncomment the quantization method you want\n\nquantization_methods = [\n    \"q8_0\",    # Fast inference, good quality (recommended)\n    # \"q4_k_m\",  # Smaller size, still good quality\n    # \"q5_k_m\",  # Balance between size and quality\n]\n\nfor method in quantization_methods:\n    print(f\"\\nüì¶ Exporting to {method.upper()}...\")\n    model.save_pretrained_gguf(\n        \"vietnamese_legal_model\",\n        tokenizer,\n        quantization_method=method,\n    )\n    print(f\"‚úÖ Exported: vietnamese_legal_model-{method.upper()}.gguf\")\n\nprint(\"\\n‚úÖ All quantization exports completed!\")\nprint(\"üìù You can now use these with Ollama or llama.cpp\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:31:37.261638Z","iopub.status.idle":"2025-11-28T08:31:37.261853Z","shell.execute_reply.started":"2025-11-28T08:31:37.261731Z","shell.execute_reply":"2025-11-28T08:31:37.261739Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"c3ed1db1","cell_type":"markdown","source":"## üéâ Finish & Cleanup","metadata":{}},{"id":"a9f96c85","cell_type":"code","source":"# Finish WandB run\nwandb.finish()\n\n# Clear GPU memory\ndel model\ndel trainer\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"‚úÖ Training completed successfully!\")\nprint(\"\\nüìä Summary:\")\nprint(f\"  - Model: Llama-3.2-3B-Instruct\")\nprint(f\"  - Training samples: {len(train_data):,}\")\nprint(f\"  - Validation samples: {len(val_data):,}\")\nprint(f\"  - Test samples: {len(test_data):,}\")\nprint(f\"  - Training time: ~{trainer_stats.metrics['train_runtime']/3600:.2f} hours\")\nprint(f\"  - Final eval loss: {eval_results['eval_loss']:.4f}\")\nprint(\"\\nüìÇ Saved outputs:\")\nprint(\"  - LoRA adapters: vietnamese_legal_lora/\")\nprint(\"  - GGUF models: vietnamese_legal_model-*.gguf\")\nprint(\"\\nüéØ Next steps:\")\nprint(\"  1. Test model on more samples\")\nprint(\"  2. Deploy with Ollama or llama.cpp\")\nprint(\"  3. Collect feedback and iterate\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T08:31:37.262920Z","iopub.status.idle":"2025-11-28T08:31:37.263724Z","shell.execute_reply.started":"2025-11-28T08:31:37.263588Z","shell.execute_reply":"2025-11-28T08:31:37.263605Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"9f72173f","cell_type":"markdown","source":"---\n\n## üìö References & Resources\n\n- **Unsloth**: https://github.com/unslothai/unsloth\n- **Unsloth Docs**: https://docs.unsloth.ai\n- **WandB**: https://wandb.ai\n- **Llama 3.2**: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n\n## üí° Tips for Better Results\n\n1. **More data**: Collect more Vietnamese legal Q&A pairs\n2. **Data quality**: Clean and verify answers\n3. **Hyperparameter tuning**: Try different learning rates (1e-4, 5e-5)\n4. **Longer training**: Try 4-5 epochs if not overfitting\n5. **Larger model**: Try Llama-3.2-11B if you have more GPU\n6. **Domain adaptation**: Continue pretraining on legal documents first\n\n## üêõ Troubleshooting\n\n- **OOM (Out of Memory)**: Reduce batch size or max_seq_length\n- **Slow training**: Enable packing=True for short sequences\n- **Poor results**: Increase LoRA rank or training epochs\n- **Overfitting**: Reduce epochs or add more data augmentation","metadata":{}}]}